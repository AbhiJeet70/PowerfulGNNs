{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/PowerfulGNNs/blob/main/SUN_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@inproceedings{frasca2022understanding,\n",
        "#title={Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries},\n",
        "#author={Frasca, Fabrizio and Bevilacqua, Beatrice and Bronstein, Michael M and Maron, Haggai},\n",
        "#booktitle={Advances in Neural Information Processing Systems},\n",
        "#year={2022},\n",
        "#}\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install torch torch-geometric\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import to_dense_adj, k_hop_subgraph\n",
        "from torch_geometric.nn import MessagePassing\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SUNLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SUNLayer, self).__init__()\n",
        "        # Separate transformations for root and non-root nodes\n",
        "        self.root_mlp = nn.Linear(in_channels, out_channels)\n",
        "        self.non_root_mlp = nn.Linear(in_channels, out_channels)\n",
        "        self.global_mlp = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, subgraph_masks):\n",
        "        # Convert edge_index to dense adjacency matrix\n",
        "        adjacency_matrix = to_dense_adj(edge_index, max_num_nodes=x.size(0))[0]\n",
        "\n",
        "        # Local message passing within subgraphs\n",
        "        local_features = torch.matmul(adjacency_matrix, x)\n",
        "\n",
        "        # Global aggregation across subgraphs\n",
        "        global_features = self.global_mlp(torch.mean(x, dim=0, keepdim=True))\n",
        "        global_features = global_features.expand(x.size(0), global_features.size(1))  # Broadcast to match x's shape\n",
        "\n",
        "        # Initialize root and non-root features with correct shape\n",
        "        root_features = torch.zeros((x.size(0), global_features.size(1)), device=x.device)\n",
        "        non_root_features = torch.zeros((x.size(0), global_features.size(1)), device=x.device)\n",
        "\n",
        "        # Apply transformations to root nodes\n",
        "        root_features[subgraph_masks] = self.root_mlp(x[subgraph_masks])\n",
        "\n",
        "        # Apply transformations to non-root nodes\n",
        "        non_root_features = self.non_root_mlp(local_features)\n",
        "\n",
        "        # Combine root, non-root, and global updates\n",
        "        updated_features = root_features + non_root_features + global_features\n",
        "\n",
        "        return updated_features\n",
        "\n",
        "\n",
        "# Define the SUN model\n",
        "class SUN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, hidden_channels):\n",
        "        super(SUN, self).__init__()\n",
        "        self.layer1 = SUNLayer(num_features, hidden_channels)\n",
        "        self.layer2 = SUNLayer(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Create subgraph masks for root and non-root nodes using a node-based policy (e.g., 1-hop ego nets)\n",
        "        num_nodes = x.size(0)\n",
        "        subgraph_masks = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "        # Example subgraph extraction: mark every node as root for simplicity\n",
        "        for i in range(num_nodes):\n",
        "            _, _, _, node_mask = k_hop_subgraph(i, 1, edge_index, relabel_nodes=False, num_nodes=num_nodes)\n",
        "            subgraph_masks[node_mask[:num_nodes]] = True  # Ensure alignment with graph size\n",
        "\n",
        "        # Pass through SUN layers\n",
        "        x = self.layer1(x, edge_index, subgraph_masks)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x, edge_index, subgraph_masks)\n",
        "        return x\n",
        "\n",
        "# Training and Evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Planetoid datasets\n",
        "    datasets = ['Cora', 'Pubmed', 'CiteSeer']\n",
        "    results = {}\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset = Planetoid(root=f'./data/{dataset_name}', name=dataset_name)\n",
        "        data = dataset[0]\n",
        "\n",
        "        # Initialize model, optimizer, and loss function\n",
        "        model = SUN(dataset.num_features, dataset.num_classes, hidden_channels=32)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train the model\n",
        "        def train():\n",
        "            for epoch in range(200):\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                out = model(data)\n",
        "                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Test the model\n",
        "        def test():\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                pred = model(data).argmax(dim=1)\n",
        "                acc = (pred[data.test_mask] == data.y[data.test_mask]).sum() / data.test_mask.sum()\n",
        "                return acc.item()\n",
        "\n",
        "        train()\n",
        "        accuracy = test()\n",
        "        results[dataset_name] = accuracy\n",
        "\n",
        "    # Print results\n",
        "    for dataset_name, accuracy in results.items():\n",
        "        print(f\"Dataset: {dataset_name}, Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T09:29:43.164289Z",
          "iopub.execute_input": "2024-12-12T09:29:43.16472Z",
          "iopub.status.idle": "2024-12-12T11:20:23.657817Z",
          "shell.execute_reply.started": "2024-12-12T09:29:43.164683Z",
          "shell.execute_reply": "2024-12-12T11:20:23.656358Z"
        },
        "id": "IKpnyHQiTGz0",
        "outputId": "7e15e930-103a-4245-c01d-64bedbbc0a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: torch-geometric in /opt/conda/lib/python3.10/site-packages (2.6.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nEpoch 1, Loss: 2.3401\nEpoch 2, Loss: 1.5984\nEpoch 3, Loss: 1.3868\nEpoch 4, Loss: 0.8538\nEpoch 5, Loss: 0.7661\nEpoch 6, Loss: 0.5640\nEpoch 7, Loss: 0.4513\nEpoch 8, Loss: 0.3978\nEpoch 9, Loss: 0.2903\nEpoch 10, Loss: 0.1773\nEpoch 11, Loss: 0.1312\nEpoch 12, Loss: 0.1427\nEpoch 13, Loss: 0.0696\nEpoch 14, Loss: 0.0513\nEpoch 15, Loss: 0.0389\nEpoch 16, Loss: 0.0295\nEpoch 17, Loss: 0.0224\nEpoch 18, Loss: 0.0181\nEpoch 19, Loss: 0.0145\nEpoch 20, Loss: 0.0093\nEpoch 21, Loss: 0.0062\nEpoch 22, Loss: 0.0045\nEpoch 23, Loss: 0.0034\nEpoch 24, Loss: 0.0027\nEpoch 25, Loss: 0.0021\nEpoch 26, Loss: 0.0018\nEpoch 27, Loss: 0.0015\nEpoch 28, Loss: 0.0013\nEpoch 29, Loss: 0.0012\nEpoch 30, Loss: 0.0010\nEpoch 31, Loss: 0.0009\nEpoch 32, Loss: 0.0008\nEpoch 33, Loss: 0.0007\nEpoch 34, Loss: 0.0006\nEpoch 35, Loss: 0.0005\nEpoch 36, Loss: 0.0005\nEpoch 37, Loss: 0.0004\nEpoch 38, Loss: 0.0004\nEpoch 39, Loss: 0.0003\nEpoch 40, Loss: 0.0003\nEpoch 41, Loss: 0.0003\nEpoch 42, Loss: 0.0002\nEpoch 43, Loss: 0.0002\nEpoch 44, Loss: 0.0002\nEpoch 45, Loss: 0.0002\nEpoch 46, Loss: 0.0002\nEpoch 47, Loss: 0.0002\nEpoch 48, Loss: 0.0002\nEpoch 49, Loss: 0.0002\nEpoch 50, Loss: 0.0002\nEpoch 51, Loss: 0.0001\nEpoch 52, Loss: 0.0001\nEpoch 53, Loss: 0.0001\nEpoch 54, Loss: 0.0001\nEpoch 55, Loss: 0.0001\nEpoch 56, Loss: 0.0001\nEpoch 57, Loss: 0.0001\nEpoch 58, Loss: 0.0001\nEpoch 59, Loss: 0.0001\nEpoch 60, Loss: 0.0001\nEpoch 61, Loss: 0.0001\nEpoch 62, Loss: 0.0001\nEpoch 63, Loss: 0.0001\nEpoch 64, Loss: 0.0001\nEpoch 65, Loss: 0.0001\nEpoch 66, Loss: 0.0001\nEpoch 67, Loss: 0.0001\nEpoch 68, Loss: 0.0001\nEpoch 69, Loss: 0.0001\nEpoch 70, Loss: 0.0001\nEpoch 71, Loss: 0.0001\nEpoch 72, Loss: 0.0001\nEpoch 73, Loss: 0.0001\nEpoch 74, Loss: 0.0001\nEpoch 75, Loss: 0.0001\nEpoch 76, Loss: 0.0001\nEpoch 77, Loss: 0.0001\nEpoch 78, Loss: 0.0001\nEpoch 79, Loss: 0.0001\nEpoch 80, Loss: 0.0001\nEpoch 81, Loss: 0.0001\nEpoch 82, Loss: 0.0001\nEpoch 83, Loss: 0.0001\nEpoch 84, Loss: 0.0001\nEpoch 85, Loss: 0.0001\nEpoch 86, Loss: 0.0001\nEpoch 87, Loss: 0.0001\nEpoch 88, Loss: 0.0001\nEpoch 89, Loss: 0.0001\nEpoch 90, Loss: 0.0001\nEpoch 91, Loss: 0.0001\nEpoch 92, Loss: 0.0001\nEpoch 93, Loss: 0.0001\nEpoch 94, Loss: 0.0001\nEpoch 95, Loss: 0.0001\nEpoch 96, Loss: 0.0001\nEpoch 97, Loss: 0.0001\nEpoch 98, Loss: 0.0001\nEpoch 99, Loss: 0.0001\nEpoch 100, Loss: 0.0001\nEpoch 101, Loss: 0.0001\nEpoch 102, Loss: 0.0001\nEpoch 103, Loss: 0.0001\nEpoch 104, Loss: 0.0001\nEpoch 105, Loss: 0.0001\nEpoch 106, Loss: 0.0001\nEpoch 107, Loss: 0.0001\nEpoch 108, Loss: 0.0001\nEpoch 109, Loss: 0.0001\nEpoch 110, Loss: 0.0001\nEpoch 111, Loss: 0.0001\nEpoch 112, Loss: 0.0001\nEpoch 113, Loss: 0.0001\nEpoch 114, Loss: 0.0001\nEpoch 115, Loss: 0.0001\nEpoch 116, Loss: 0.0001\nEpoch 117, Loss: 0.0001\nEpoch 118, Loss: 0.0001\nEpoch 119, Loss: 0.0001\nEpoch 120, Loss: 0.0001\nEpoch 121, Loss: 0.0001\nEpoch 122, Loss: 0.0001\nEpoch 123, Loss: 0.0001\nEpoch 124, Loss: 0.0001\nEpoch 125, Loss: 0.0001\nEpoch 126, Loss: 0.0001\nEpoch 127, Loss: 0.0001\nEpoch 128, Loss: 0.0001\nEpoch 129, Loss: 0.0001\nEpoch 130, Loss: 0.0001\nEpoch 131, Loss: 0.0001\nEpoch 132, Loss: 0.0001\nEpoch 133, Loss: 0.0001\nEpoch 134, Loss: 0.0001\nEpoch 135, Loss: 0.0001\nEpoch 136, Loss: 0.0001\nEpoch 137, Loss: 0.0001\nEpoch 138, Loss: 0.0001\nEpoch 139, Loss: 0.0001\nEpoch 140, Loss: 0.0001\nEpoch 141, Loss: 0.0001\nEpoch 142, Loss: 0.0001\nEpoch 143, Loss: 0.0001\nEpoch 144, Loss: 0.0001\nEpoch 145, Loss: 0.0001\nEpoch 146, Loss: 0.0001\nEpoch 147, Loss: 0.0001\nEpoch 148, Loss: 0.0001\nEpoch 149, Loss: 0.0001\nEpoch 150, Loss: 0.0001\nEpoch 151, Loss: 0.0001\nEpoch 152, Loss: 0.0001\nEpoch 153, Loss: 0.0001\nEpoch 154, Loss: 0.0001\nEpoch 155, Loss: 0.0001\nEpoch 156, Loss: 0.0001\nEpoch 157, Loss: 0.0001\nEpoch 158, Loss: 0.0001\nEpoch 159, Loss: 0.0001\nEpoch 160, Loss: 0.0001\nEpoch 161, Loss: 0.0001\nEpoch 162, Loss: 0.0001\nEpoch 163, Loss: 0.0001\nEpoch 164, Loss: 0.0001\nEpoch 165, Loss: 0.0001\nEpoch 166, Loss: 0.0001\nEpoch 167, Loss: 0.0001\nEpoch 168, Loss: 0.0001\nEpoch 169, Loss: 0.0001\nEpoch 170, Loss: 0.0001\nEpoch 171, Loss: 0.0001\nEpoch 172, Loss: 0.0001\nEpoch 173, Loss: 0.0001\nEpoch 174, Loss: 0.0001\nEpoch 175, Loss: 0.0001\nEpoch 176, Loss: 0.0001\nEpoch 177, Loss: 0.0001\nEpoch 178, Loss: 0.0001\nEpoch 179, Loss: 0.0001\nEpoch 180, Loss: 0.0001\nEpoch 181, Loss: 0.0001\nEpoch 182, Loss: 0.0001\nEpoch 183, Loss: 0.0001\nEpoch 184, Loss: 0.0001\nEpoch 185, Loss: 0.0001\nEpoch 186, Loss: 0.0000\nEpoch 187, Loss: 0.0000\nEpoch 188, Loss: 0.0000\nEpoch 189, Loss: 0.0000\nEpoch 190, Loss: 0.0000\nEpoch 191, Loss: 0.0000\nEpoch 192, Loss: 0.0000\nEpoch 193, Loss: 0.0000\nEpoch 194, Loss: 0.0000\nEpoch 195, Loss: 0.0000\nEpoch 196, Loss: 0.0000\nEpoch 197, Loss: 0.0000\nEpoch 198, Loss: 0.0000\nEpoch 199, Loss: 0.0000\nEpoch 200, Loss: 0.0000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1, Loss: 1.1357\nEpoch 2, Loss: 0.9954\nEpoch 3, Loss: 0.8519\nEpoch 4, Loss: 0.7326\nEpoch 5, Loss: 0.6005\nEpoch 6, Loss: 0.5465\nEpoch 7, Loss: 0.4643\nEpoch 8, Loss: 0.3931\nEpoch 9, Loss: 0.3455\nEpoch 10, Loss: 0.2991\nEpoch 11, Loss: 0.2557\nEpoch 12, Loss: 0.2160\nEpoch 13, Loss: 0.1810\nEpoch 14, Loss: 0.1509\nEpoch 15, Loss: 0.1247\nEpoch 16, Loss: 0.1011\nEpoch 17, Loss: 0.0801\nEpoch 18, Loss: 0.0628\nEpoch 19, Loss: 0.0505\nEpoch 20, Loss: 0.0397\nEpoch 21, Loss: 0.0322\nEpoch 22, Loss: 0.0262\nEpoch 23, Loss: 0.0211\nEpoch 24, Loss: 0.0168\nEpoch 25, Loss: 0.0134\nEpoch 26, Loss: 0.0108\nEpoch 27, Loss: 0.0087\nEpoch 28, Loss: 0.0072\nEpoch 29, Loss: 0.0060\nEpoch 30, Loss: 0.0051\nEpoch 31, Loss: 0.0043\nEpoch 32, Loss: 0.0037\nEpoch 33, Loss: 0.0032\nEpoch 34, Loss: 0.0028\nEpoch 35, Loss: 0.0024\nEpoch 36, Loss: 0.0021\nEpoch 37, Loss: 0.0019\nEpoch 38, Loss: 0.0017\nEpoch 39, Loss: 0.0015\nEpoch 40, Loss: 0.0014\nEpoch 41, Loss: 0.0012\nEpoch 42, Loss: 0.0011\nEpoch 43, Loss: 0.0010\nEpoch 44, Loss: 0.0010\nEpoch 45, Loss: 0.0009\nEpoch 46, Loss: 0.0008\nEpoch 47, Loss: 0.0008\nEpoch 48, Loss: 0.0007\nEpoch 49, Loss: 0.0007\nEpoch 50, Loss: 0.0007\nEpoch 51, Loss: 0.0006\nEpoch 52, Loss: 0.0006\nEpoch 53, Loss: 0.0006\nEpoch 54, Loss: 0.0006\nEpoch 55, Loss: 0.0005\nEpoch 56, Loss: 0.0005\nEpoch 57, Loss: 0.0005\nEpoch 58, Loss: 0.0005\nEpoch 59, Loss: 0.0005\nEpoch 60, Loss: 0.0005\nEpoch 61, Loss: 0.0005\nEpoch 62, Loss: 0.0004\nEpoch 63, Loss: 0.0004\nEpoch 64, Loss: 0.0004\nEpoch 65, Loss: 0.0004\nEpoch 66, Loss: 0.0004\nEpoch 67, Loss: 0.0004\nEpoch 68, Loss: 0.0004\nEpoch 69, Loss: 0.0004\nEpoch 70, Loss: 0.0004\nEpoch 71, Loss: 0.0004\nEpoch 72, Loss: 0.0004\nEpoch 73, Loss: 0.0004\nEpoch 74, Loss: 0.0004\nEpoch 75, Loss: 0.0003\nEpoch 76, Loss: 0.0003\nEpoch 77, Loss: 0.0003\nEpoch 78, Loss: 0.0003\nEpoch 79, Loss: 0.0003\nEpoch 80, Loss: 0.0003\nEpoch 81, Loss: 0.0003\nEpoch 82, Loss: 0.0003\nEpoch 83, Loss: 0.0003\nEpoch 84, Loss: 0.0003\nEpoch 85, Loss: 0.0003\nEpoch 86, Loss: 0.0003\nEpoch 87, Loss: 0.0003\nEpoch 88, Loss: 0.0003\nEpoch 89, Loss: 0.0003\nEpoch 90, Loss: 0.0003\nEpoch 91, Loss: 0.0003\nEpoch 92, Loss: 0.0003\nEpoch 93, Loss: 0.0003\nEpoch 94, Loss: 0.0003\nEpoch 95, Loss: 0.0003\nEpoch 96, Loss: 0.0003\nEpoch 97, Loss: 0.0003\nEpoch 98, Loss: 0.0003\nEpoch 99, Loss: 0.0003\nEpoch 100, Loss: 0.0003\nEpoch 101, Loss: 0.0003\nEpoch 102, Loss: 0.0003\nEpoch 103, Loss: 0.0003\nEpoch 104, Loss: 0.0003\nEpoch 105, Loss: 0.0003\nEpoch 106, Loss: 0.0002\nEpoch 107, Loss: 0.0002\nEpoch 108, Loss: 0.0002\nEpoch 109, Loss: 0.0002\nEpoch 110, Loss: 0.0002\nEpoch 111, Loss: 0.0002\nEpoch 112, Loss: 0.0002\nEpoch 113, Loss: 0.0002\nEpoch 114, Loss: 0.0002\nEpoch 115, Loss: 0.0002\nEpoch 116, Loss: 0.0002\nEpoch 117, Loss: 0.0002\nEpoch 118, Loss: 0.0002\nEpoch 119, Loss: 0.0002\nEpoch 120, Loss: 0.0002\nEpoch 121, Loss: 0.0002\nEpoch 122, Loss: 0.0002\nEpoch 123, Loss: 0.0002\nEpoch 124, Loss: 0.0002\nEpoch 125, Loss: 0.0002\nEpoch 126, Loss: 0.0002\nEpoch 127, Loss: 0.0002\nEpoch 128, Loss: 0.0002\nEpoch 129, Loss: 0.0002\nEpoch 130, Loss: 0.0002\nEpoch 131, Loss: 0.0002\nEpoch 132, Loss: 0.0002\nEpoch 133, Loss: 0.0002\nEpoch 134, Loss: 0.0002\nEpoch 135, Loss: 0.0002\nEpoch 136, Loss: 0.0002\nEpoch 137, Loss: 0.0002\nEpoch 138, Loss: 0.0002\nEpoch 139, Loss: 0.0002\nEpoch 140, Loss: 0.0002\nEpoch 141, Loss: 0.0002\nEpoch 142, Loss: 0.0002\nEpoch 143, Loss: 0.0002\nEpoch 144, Loss: 0.0002\nEpoch 145, Loss: 0.0002\nEpoch 146, Loss: 0.0002\nEpoch 147, Loss: 0.0002\nEpoch 148, Loss: 0.0002\nEpoch 149, Loss: 0.0002\nEpoch 150, Loss: 0.0002\nEpoch 151, Loss: 0.0002\nEpoch 152, Loss: 0.0002\nEpoch 153, Loss: 0.0002\nEpoch 154, Loss: 0.0002\nEpoch 155, Loss: 0.0002\nEpoch 156, Loss: 0.0002\nEpoch 157, Loss: 0.0002\nEpoch 158, Loss: 0.0002\nEpoch 159, Loss: 0.0002\nEpoch 160, Loss: 0.0002\nEpoch 161, Loss: 0.0002\nEpoch 162, Loss: 0.0002\nEpoch 163, Loss: 0.0002\nEpoch 164, Loss: 0.0002\nEpoch 165, Loss: 0.0002\nEpoch 166, Loss: 0.0002\nEpoch 167, Loss: 0.0002\nEpoch 168, Loss: 0.0002\nEpoch 169, Loss: 0.0002\nEpoch 170, Loss: 0.0002\nEpoch 171, Loss: 0.0002\nEpoch 172, Loss: 0.0002\nEpoch 173, Loss: 0.0002\nEpoch 174, Loss: 0.0001\nEpoch 175, Loss: 0.0001\nEpoch 176, Loss: 0.0001\nEpoch 177, Loss: 0.0001\nEpoch 178, Loss: 0.0001\nEpoch 179, Loss: 0.0001\nEpoch 180, Loss: 0.0001\nEpoch 181, Loss: 0.0001\nEpoch 182, Loss: 0.0001\nEpoch 183, Loss: 0.0001\nEpoch 184, Loss: 0.0001\nEpoch 185, Loss: 0.0001\nEpoch 186, Loss: 0.0001\nEpoch 187, Loss: 0.0001\nEpoch 188, Loss: 0.0001\nEpoch 189, Loss: 0.0001\nEpoch 190, Loss: 0.0001\nEpoch 191, Loss: 0.0001\nEpoch 192, Loss: 0.0001\nEpoch 193, Loss: 0.0001\nEpoch 194, Loss: 0.0001\nEpoch 195, Loss: 0.0001\nEpoch 196, Loss: 0.0001\nEpoch 197, Loss: 0.0001\nEpoch 198, Loss: 0.0001\nEpoch 199, Loss: 0.0001\nEpoch 200, Loss: 0.0001\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1, Loss: 1.9279\nEpoch 2, Loss: 1.8588\nEpoch 3, Loss: 0.9494\nEpoch 4, Loss: 0.5665\nEpoch 5, Loss: 0.4185\nEpoch 6, Loss: 0.2994\nEpoch 7, Loss: 0.1989\nEpoch 8, Loss: 0.1201\nEpoch 9, Loss: 0.0656\nEpoch 10, Loss: 0.0364\nEpoch 11, Loss: 0.0222\nEpoch 12, Loss: 0.0163\nEpoch 13, Loss: 0.0118\nEpoch 14, Loss: 0.0060\nEpoch 15, Loss: 0.0033\nEpoch 16, Loss: 0.0019\nEpoch 17, Loss: 0.0012\nEpoch 18, Loss: 0.0008\nEpoch 19, Loss: 0.0005\nEpoch 20, Loss: 0.0004\nEpoch 21, Loss: 0.0003\nEpoch 22, Loss: 0.0002\nEpoch 23, Loss: 0.0001\nEpoch 24, Loss: 0.0001\nEpoch 25, Loss: 0.0001\nEpoch 26, Loss: 0.0001\nEpoch 27, Loss: 0.0001\nEpoch 28, Loss: 0.0001\nEpoch 29, Loss: 0.0000\nEpoch 30, Loss: 0.0000\nEpoch 31, Loss: 0.0000\nEpoch 32, Loss: 0.0000\nEpoch 33, Loss: 0.0000\nEpoch 34, Loss: 0.0000\nEpoch 35, Loss: 0.0000\nEpoch 36, Loss: 0.0000\nEpoch 37, Loss: 0.0000\nEpoch 38, Loss: 0.0000\nEpoch 39, Loss: 0.0000\nEpoch 40, Loss: 0.0000\nEpoch 41, Loss: 0.0000\nEpoch 42, Loss: 0.0000\nEpoch 43, Loss: 0.0000\nEpoch 44, Loss: 0.0000\nEpoch 45, Loss: 0.0000\nEpoch 46, Loss: 0.0000\nEpoch 47, Loss: 0.0000\nEpoch 48, Loss: 0.0000\nEpoch 49, Loss: 0.0000\nEpoch 50, Loss: 0.0000\nEpoch 51, Loss: 0.0000\nEpoch 52, Loss: 0.0000\nEpoch 53, Loss: 0.0000\nEpoch 54, Loss: 0.0000\nEpoch 55, Loss: 0.0000\nEpoch 56, Loss: 0.0000\nEpoch 57, Loss: 0.0000\nEpoch 58, Loss: 0.0000\nEpoch 59, Loss: 0.0000\nEpoch 60, Loss: 0.0000\nEpoch 61, Loss: 0.0000\nEpoch 62, Loss: 0.0000\nEpoch 63, Loss: 0.0000\nEpoch 64, Loss: 0.0000\nEpoch 65, Loss: 0.0000\nEpoch 66, Loss: 0.0000\nEpoch 67, Loss: 0.0000\nEpoch 68, Loss: 0.0000\nEpoch 69, Loss: 0.0000\nEpoch 70, Loss: 0.0000\nEpoch 71, Loss: 0.0000\nEpoch 72, Loss: 0.0000\nEpoch 73, Loss: 0.0000\nEpoch 74, Loss: 0.0000\nEpoch 75, Loss: 0.0000\nEpoch 76, Loss: 0.0000\nEpoch 77, Loss: 0.0000\nEpoch 78, Loss: 0.0000\nEpoch 79, Loss: 0.0000\nEpoch 80, Loss: 0.0000\nEpoch 81, Loss: 0.0000\nEpoch 82, Loss: 0.0000\nEpoch 83, Loss: 0.0000\nEpoch 84, Loss: 0.0000\nEpoch 85, Loss: 0.0000\nEpoch 86, Loss: 0.0000\nEpoch 87, Loss: 0.0000\nEpoch 88, Loss: 0.0000\nEpoch 89, Loss: 0.0000\nEpoch 90, Loss: 0.0000\nEpoch 91, Loss: 0.0000\nEpoch 92, Loss: 0.0000\nEpoch 93, Loss: 0.0000\nEpoch 94, Loss: 0.0000\nEpoch 95, Loss: 0.0000\nEpoch 96, Loss: 0.0000\nEpoch 97, Loss: 0.0000\nEpoch 98, Loss: 0.0000\nEpoch 99, Loss: 0.0000\nEpoch 100, Loss: 0.0000\nEpoch 101, Loss: 0.0000\nEpoch 102, Loss: 0.0000\nEpoch 103, Loss: 0.0000\nEpoch 104, Loss: 0.0000\nEpoch 105, Loss: 0.0000\nEpoch 106, Loss: 0.0000\nEpoch 107, Loss: 0.0000\nEpoch 108, Loss: 0.0000\nEpoch 109, Loss: 0.0000\nEpoch 110, Loss: 0.0000\nEpoch 111, Loss: 0.0000\nEpoch 112, Loss: 0.0000\nEpoch 113, Loss: 0.0000\nEpoch 114, Loss: 0.0000\nEpoch 115, Loss: 0.0000\nEpoch 116, Loss: 0.0000\nEpoch 117, Loss: 0.0000\nEpoch 118, Loss: 0.0000\nEpoch 119, Loss: 0.0000\nEpoch 120, Loss: 0.0000\nEpoch 121, Loss: 0.0000\nEpoch 122, Loss: 0.0000\nEpoch 123, Loss: 0.0000\nEpoch 124, Loss: 0.0000\nEpoch 125, Loss: 0.0000\nEpoch 126, Loss: 0.0000\nEpoch 127, Loss: 0.0000\nEpoch 128, Loss: 0.0000\nEpoch 129, Loss: 0.0000\nEpoch 130, Loss: 0.0000\nEpoch 131, Loss: 0.0000\nEpoch 132, Loss: 0.0000\nEpoch 133, Loss: 0.0000\nEpoch 134, Loss: 0.0000\nEpoch 135, Loss: 0.0000\nEpoch 136, Loss: 0.0000\nEpoch 137, Loss: 0.0000\nEpoch 138, Loss: 0.0000\nEpoch 139, Loss: 0.0000\nEpoch 140, Loss: 0.0000\nEpoch 141, Loss: 0.0000\nEpoch 142, Loss: 0.0000\nEpoch 143, Loss: 0.0000\nEpoch 144, Loss: 0.0000\nEpoch 145, Loss: 0.0000\nEpoch 146, Loss: 0.0000\nEpoch 147, Loss: 0.0000\nEpoch 148, Loss: 0.0000\nEpoch 149, Loss: 0.0000\nEpoch 150, Loss: 0.0000\nEpoch 151, Loss: 0.0000\nEpoch 152, Loss: 0.0000\nEpoch 153, Loss: 0.0000\nEpoch 154, Loss: 0.0000\nEpoch 155, Loss: 0.0000\nEpoch 156, Loss: 0.0000\nEpoch 157, Loss: 0.0000\nEpoch 158, Loss: 0.0000\nEpoch 159, Loss: 0.0000\nEpoch 160, Loss: 0.0000\nEpoch 161, Loss: 0.0000\nEpoch 162, Loss: 0.0000\nEpoch 163, Loss: 0.0000\nEpoch 164, Loss: 0.0000\nEpoch 165, Loss: 0.0000\nEpoch 166, Loss: 0.0000\nEpoch 167, Loss: 0.0000\nEpoch 168, Loss: 0.0000\nEpoch 169, Loss: 0.0000\nEpoch 170, Loss: 0.0000\nEpoch 171, Loss: 0.0000\nEpoch 172, Loss: 0.0000\nEpoch 173, Loss: 0.0000\nEpoch 174, Loss: 0.0000\nEpoch 175, Loss: 0.0000\nEpoch 176, Loss: 0.0000\nEpoch 177, Loss: 0.0000\nEpoch 178, Loss: 0.0000\nEpoch 179, Loss: 0.0000\nEpoch 180, Loss: 0.0000\nEpoch 181, Loss: 0.0000\nEpoch 182, Loss: 0.0000\nEpoch 183, Loss: 0.0000\nEpoch 184, Loss: 0.0000\nEpoch 185, Loss: 0.0000\nEpoch 186, Loss: 0.0000\nEpoch 187, Loss: 0.0000\nEpoch 188, Loss: 0.0000\nEpoch 189, Loss: 0.0000\nEpoch 190, Loss: 0.0000\nEpoch 191, Loss: 0.0000\nEpoch 192, Loss: 0.0000\nEpoch 193, Loss: 0.0000\nEpoch 194, Loss: 0.0000\nEpoch 195, Loss: 0.0000\nEpoch 196, Loss: 0.0000\nEpoch 197, Loss: 0.0000\nEpoch 198, Loss: 0.0000\nEpoch 199, Loss: 0.0000\nEpoch 200, Loss: 0.0000\nDataset: Cora, Test Accuracy: 0.7580\nDataset: Pubmed, Test Accuracy: 0.7570\nDataset: CiteSeer, Test Accuracy: 0.5500\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}