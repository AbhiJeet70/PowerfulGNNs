{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/PowerfulGNNs/blob/main/Attacks_On_Everything.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import subgraph\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, Flickr\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import networkx as nx\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.nn.models.autoencoder import GAE\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gc\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name in [\"Cora\", \"PubMed\", \"CiteSeer\"]:\n",
        "        dataset = Planetoid(root=f\"./data/{dataset_name}\", name=dataset_name)\n",
        "    elif dataset_name == \"Flickr\":\n",
        "        dataset = Flickr(root=\"./data/Flickr\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Split dataset into train/validation/test\n",
        "# Updated to randomly mask out 20% of nodes, use 10% for labeled nodes, and 10% for validation\n",
        "def split_dataset(data, test_size=0.2, val_size=0.1):\n",
        "    num_nodes = data.num_nodes\n",
        "    indices = np.arange(num_nodes)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    num_test = int(test_size * num_nodes)\n",
        "    num_val = int(val_size * num_nodes)\n",
        "    num_train = num_nodes - num_test - num_val\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    val_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train + num_val]] = True\n",
        "    test_mask[indices[num_train + num_val:]] = True\n",
        "\n",
        "    data.train_mask = train_mask\n",
        "    data.val_mask = val_mask\n",
        "    data.test_mask = test_mask\n",
        "\n",
        "    # Mask out 20% nodes for attack performance evaluation (half target, half clean test)\n",
        "    num_target = int(0.1 * num_nodes)  # Half of 20%\n",
        "    target_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    clean_test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    target_mask[indices[num_train + num_val:num_train + num_val + num_target]] = True\n",
        "    clean_test_mask[indices[num_train + num_val + num_target:]] = True\n",
        "\n",
        "    data.target_mask = target_mask\n",
        "    data.clean_test_mask = clean_test_mask\n",
        "\n",
        "    return data\n",
        "\n",
        "# Select nodes to poison based on high-centrality (degree centrality) for a stronger impact\n",
        "def select_high_centrality_nodes(data, num_nodes_to_select):\n",
        "    graph = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    graph.add_edges_from(edge_index.T)\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    sorted_nodes = sorted(centrality, key=centrality.get, reverse=True)\n",
        "    return torch.tensor(sorted_nodes[:num_nodes_to_select], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "class TriggerGenerator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(TriggerGenerator, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),  # Reduce dimensions\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, input_dim)  # Restore to input_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class GAE(torch.nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        return self.encoder(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "class OODDetector(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(OODDetector, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.gae = GAE(self.encoder, self.decoder)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        return z\n",
        "\n",
        "    def reconstruct(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "    def reconstruction_loss(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        reconstructed = self.reconstruct(z, edge_index)\n",
        "        return F.mse_loss(reconstructed, x)\n",
        "\n",
        "    def detect_ood(self, x, edge_index, threshold):\n",
        "        loss = self.reconstruction_loss(x, edge_index)\n",
        "        return loss > threshold\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = F.relu(self.conv1(x, edge_index))\n",
        "        z = self.conv2(z, edge_index)\n",
        "        return z\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = GCNConv(latent_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        x = F.relu(self.conv1(z, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def train_ood_detector(ood_detector, data, optimizer, epochs=50):\n",
        "    ood_detector.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        z = ood_detector(data.x, data.edge_index)\n",
        "\n",
        "        # Reconstruct data using the latent embedding\n",
        "        reconstructed_x = ood_detector.reconstruct(z, data.edge_index)\n",
        "\n",
        "        # Use only the training mask to compute reconstruction loss\n",
        "        loss = F.mse_loss(reconstructed_x[data.train_mask], data.x[data.train_mask])\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Print loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Reconstruction Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_with_poisoned_data(model, data, optimizer, poisoned_nodes, trigger_gen, attack, ood_detector=None, alpha=0.7, early_stopping=False):\n",
        "    # Apply trigger injection\n",
        "    data_poisoned = inject_trigger(data, poisoned_nodes, attack, trigger_gen, ood_detector, alpha)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data_poisoned.x, data_poisoned.edge_index)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.cross_entropy(out[data_poisoned.train_mask], data_poisoned.y[data_poisoned.train_mask])\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()  # No retain_graph=True unless explicitly required\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional: Print loss during training for insight\n",
        "        if early_stopping and epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, data_poisoned\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def select_diverse_nodes(data, num_nodes_to_select, num_clusters=None):\n",
        "    if num_clusters is None:\n",
        "        num_clusters = len(torch.unique(data.y))\n",
        "\n",
        "    # Use GCN encoder to get node embeddings\n",
        "    encoder = GCNEncoder(data.num_features, out_channels=16).to(data.x.device)  # Move encoder to the correct device\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = encoder(data.x, data.edge_index).to(data.x.device)  # Ensure embeddings are on the correct device\n",
        "\n",
        "    # Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(embeddings.cpu().numpy())  # Clustering runs on CPU\n",
        "    labels = kmeans.labels_\n",
        "    cluster_centers = torch.tensor(kmeans.cluster_centers_, device=data.x.device)  # Move cluster centers to device\n",
        "\n",
        "    # Select nodes closest to the cluster centers\n",
        "    selected_nodes = []\n",
        "    for i in range(num_clusters):\n",
        "        cluster_indices = torch.where(torch.tensor(labels, device=data.x.device) == i)[0]\n",
        "        center = cluster_centers[i]\n",
        "        distances = torch.norm(embeddings[cluster_indices] - center, dim=1)\n",
        "        closest_node = cluster_indices[torch.argmin(distances)]\n",
        "        selected_nodes.append(closest_node)\n",
        "\n",
        "    # Calculate node degrees\n",
        "    degree = torch.bincount(data.edge_index[0], minlength=data.num_nodes).to(data.x.device)\n",
        "    high_degree_nodes = torch.topk(degree, len(selected_nodes) // 2).indices\n",
        "\n",
        "    # Combine diverse nodes and high-degree nodes\n",
        "    combined_nodes = torch.cat([torch.tensor(selected_nodes, device=data.x.device), high_degree_nodes])\n",
        "    unique_nodes = torch.unique(combined_nodes)[:num_nodes_to_select]\n",
        "\n",
        "    return unique_nodes.to(data.x.device)  # Ensure the selected nodes are on the correct device\n",
        "\n",
        "def inject_trigger(data, poisoned_nodes, attack_type, trigger_gen=None, ood_detector=None, alpha=0.7, trigger_size=5, trigger_density=0.5, input_dim=None):\n",
        "    # Clone data to avoid overwriting the original graph\n",
        "    data_poisoned = data.clone()\n",
        "    device = data_poisoned.x.device\n",
        "\n",
        "    if len(poisoned_nodes) == 0:\n",
        "        raise ValueError(\"No poisoned nodes selected. Ensure 'poisoned_nodes' is populated and non-empty.\")\n",
        "\n",
        "    # Adjust trigger_size if it exceeds the number of poisoned nodes\n",
        "    trigger_size = min(trigger_size, len(poisoned_nodes))\n",
        "\n",
        "    if attack_type == 'SBA-Samp':\n",
        "        # Subgraph-Based Attack - Random Sampling\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes[:trigger_size]]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ]).to(device)\n",
        "        natural_features = avg_features + torch.randn_like(avg_features) * 0.02  # Small randomness\n",
        "\n",
        "        # Generate subgraph with realistic density\n",
        "        G = nx.erdos_renyi_graph(trigger_size, trigger_density)\n",
        "        trigger_edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous().to(device)\n",
        "\n",
        "        # Connect poisoned nodes to the subgraph\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        # Update graph structure and features\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index, poisoned_edges], dim=1)\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = natural_features[:trigger_size]\n",
        "\n",
        "    elif attack_type == 'SBA-Gen':\n",
        "        # Subgraph-Based Attack - Gaussian\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes[:trigger_size]]\n",
        "        feature_mean = data.x.mean(dim=0)\n",
        "        feature_std = data.x.std(dim=0)\n",
        "\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else feature_mean for nodes in connected_nodes\n",
        "        ]).to(device)\n",
        "        natural_features = avg_features + torch.normal(mean=0.0, std=0.03, size=avg_features.shape).to(device)\n",
        "\n",
        "        trigger_edge_index = []\n",
        "        for i in range(trigger_size):\n",
        "            for j in range(i + 1, trigger_size):\n",
        "                similarity = torch.exp(-torch.norm((natural_features[i] - natural_features[j]) / feature_std)**2)\n",
        "                if similarity > torch.rand(1).item():\n",
        "                    trigger_edge_index.append([i, j])\n",
        "\n",
        "        trigger_edge_index = torch.tensor(trigger_edge_index, dtype=torch.long).t().contiguous().to(device)\n",
        "        if trigger_edge_index.numel() > 0:\n",
        "            trigger_edge_index += poisoned_nodes[:trigger_size].unsqueeze(0)\n",
        "\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index, poisoned_edges], dim=1)\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = natural_features[:trigger_size]\n",
        "\n",
        "    elif attack_type == 'DPGBA':\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ]).to(device)\n",
        "\n",
        "        if trigger_gen is None:\n",
        "            raise ValueError(\"Trigger generator is required for the DPGBA attack.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            trigger_features = trigger_gen(avg_features)\n",
        "\n",
        "        if trigger_features.shape[1] != data.x.shape[1]:\n",
        "            raise ValueError(f\"Trigger feature dimension mismatch: {trigger_features.shape[1]} vs {data.x.shape[1]}\")\n",
        "\n",
        "        node_alphas = torch.rand(len(poisoned_nodes), device=device) * 0.3 + 0.5\n",
        "        distribution_preserved_features = (\n",
        "            node_alphas.unsqueeze(1) * data.x[poisoned_nodes]\n",
        "            + (1 - node_alphas.unsqueeze(1)) * trigger_features\n",
        "        )\n",
        "\n",
        "        data_poisoned.x[poisoned_nodes] = distribution_preserved_features\n",
        "\n",
        "    elif attack_type == 'GTA':\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ]).to(device)\n",
        "        trigger_features = avg_features + torch.randn_like(avg_features) * 0.05\n",
        "        data_poisoned.x[poisoned_nodes] = trigger_features\n",
        "\n",
        "    elif attack_type == 'UGBA':\n",
        "        num_clusters = len(torch.unique(data_poisoned.y))\n",
        "        diverse_nodes = select_diverse_nodes(data_poisoned, len(poisoned_nodes)).to(device)\n",
        "        connected_nodes = [\n",
        "            data_poisoned.edge_index[0][data_poisoned.edge_index[1] == node].to(device)\n",
        "            for node in diverse_nodes\n",
        "        ]\n",
        "\n",
        "        avg_features = torch.stack([\n",
        "            data_poisoned.x[nodes].mean(dim=0) if len(nodes) > 0 else data_poisoned.x.mean(dim=0)\n",
        "            for nodes in connected_nodes\n",
        "        ]).to(device)\n",
        "        refined_trigger_features = avg_features + torch.normal(mean=2.0, std=0.5, size=avg_features.shape).to(device)\n",
        "        data_poisoned.x[diverse_nodes] = refined_trigger_features\n",
        "\n",
        "        new_edges = []\n",
        "        for i in range(len(diverse_nodes)):\n",
        "            node = diverse_nodes[i]\n",
        "            neighbor = connected_nodes[i][0] if len(connected_nodes[i]) > 0 else diverse_nodes[(i + 1) % len(diverse_nodes)]\n",
        "            new_edges.append([node, neighbor])\n",
        "\n",
        "        new_edges = torch.tensor(new_edges, dtype=torch.long).t().contiguous().to(device)\n",
        "        data_poisoned.edge_index = torch.cat([data_poisoned.edge_index, new_edges], dim=1)\n",
        "\n",
        "    return data_poisoned\n",
        "\n",
        "\n",
        "def dominant_set_clustering(data, threshold=0.7, use_pca=True, pca_components=10):\n",
        "    \"\"\"\n",
        "    Applies a simplified outlier detection framework using a combination of K-Means clustering and distance-based heuristics.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - threshold: Quantile threshold for identifying outliers based on cluster distances.\n",
        "    - use_pca: Whether to use PCA for dimensionality reduction.\n",
        "    - pca_components: Number of PCA components to use if PCA is applied.\n",
        "\n",
        "    Returns:\n",
        "    - pruned_nodes: Set of nodes identified as outliers.\n",
        "    - data: Updated PyG data object with modified features and labels for outliers.\n",
        "    \"\"\"\n",
        "    # Step 1: Determine the number of clusters based on the number of classes\n",
        "    n_clusters = len(data.y.unique())  # Number of unique classes in the dataset\n",
        "\n",
        "    # Step 2: Dimensionality reduction using PCA (optional)\n",
        "    node_features = data.x.detach().cpu().numpy()\n",
        "    if use_pca and node_features.shape[1] > pca_components:\n",
        "        pca = PCA(n_components=pca_components)\n",
        "        node_features = pca.fit_transform(node_features)\n",
        "\n",
        "    # Step 3: K-Means Clustering to identify clusters and potential outliers\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(node_features)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculate distances to cluster centers\n",
        "    distances = np.linalg.norm(node_features - cluster_centers[cluster_labels], axis=1)\n",
        "\n",
        "    # Identify outlier candidates based on distance threshold\n",
        "    distance_threshold = np.percentile(distances, 100 * threshold)\n",
        "    outlier_candidates = np.where(distances > distance_threshold)[0]\n",
        "\n",
        "    # Step 4: Update data to reflect removal of outlier influence\n",
        "    pruned_nodes = set(outlier_candidates)\n",
        "    if len(pruned_nodes) > 0:\n",
        "        outliers = torch.tensor(list(pruned_nodes), dtype=torch.long, device=data.x.device)\n",
        "\n",
        "        # Assign an invalid label (-1) to outlier nodes to discard them during training\n",
        "        data.y[outliers] = -1\n",
        "\n",
        "        # Replace the features of outliers with the average feature value to reduce their impact\n",
        "        data.x[outliers] = data.x.mean(dim=0).to(data.x.device)\n",
        "\n",
        "    return pruned_nodes, data\n",
        "\n",
        "def defense_prune_edges(data, quantile_threshold=0.9):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity between node features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile to determine pruning threshold (e.g., 0.9 means pruning edges in the top 10% dissimilar).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Adaptive threshold based on quantile of similarity distribution\n",
        "    similarity_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Keep edges with cosine similarity above the threshold\n",
        "    pruned_mask = cosine_similarities >= similarity_threshold\n",
        "    pruned_edges = edge_index[:, pruned_mask]\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def defense_prune_and_discard_labels(data, quantile_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity and discards labels of nodes connected by pruned edges selectively.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile threshold for cosine similarity pruning (e.g., 0.2 means pruning edges in the bottom 20%).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges and selectively discarded labels.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features using PyTorch\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Use quantile to determine adaptive threshold for pruning\n",
        "    adaptive_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Mask edges with similarity below the adaptive threshold\n",
        "    pruned_mask = cosine_similarities < adaptive_threshold\n",
        "    pruned_edges = edge_index[:, ~pruned_mask]  # Retain edges that are above the threshold\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    # Selectively discard labels of nodes connected by many pruned edges\n",
        "    pruned_src, pruned_dst = edge_index[:, pruned_mask]\n",
        "    pruned_nodes_count = torch.bincount(torch.cat([pruned_src, pruned_dst]), minlength=data.num_nodes)\n",
        "\n",
        "    # Only discard labels if the node has a high count of pruned edges\n",
        "    threshold_count = int(torch.median(pruned_nodes_count).item())  # Use median count as a threshold\n",
        "    nodes_to_discard = torch.where(pruned_nodes_count > threshold_count)[0]\n",
        "\n",
        "    data.y[nodes_to_discard] = -1  # Use -1 to represent discarded labels\n",
        "\n",
        "    return data\n",
        "\n",
        "# Compute ASR and Clean Accuracy (using .detach() to avoid retaining computation graph)\n",
        "def compute_metrics(model, data, poisoned_nodes):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index).detach()\n",
        "        _, pred = out.max(dim=1)\n",
        "        asr = (pred[poisoned_nodes] == data.y[poisoned_nodes]).sum().item() / len(poisoned_nodes) * 100\n",
        "        clean_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu()) * 100\n",
        "    return asr, clean_acc\n",
        "\n",
        "\n",
        "\n",
        "# Visualization Function\n",
        "# Visualize PCA for Attacks\n",
        "# Added function to visualize PCA projections of node embeddings for different attacks\n",
        "def visualize_pca_for_attacks(attack_embeddings_dict):\n",
        "    pca = PCA(n_components=2)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    for i, (attack, attack_data) in enumerate(attack_embeddings_dict.items(), 1):\n",
        "        embeddings = attack_data['data'].detach().cpu().numpy()\n",
        "        poisoned_nodes = attack_data['poisoned_nodes'].detach().cpu().numpy()\n",
        "\n",
        "        # Apply PCA to the node embeddings\n",
        "        pca_result = pca.fit_transform(embeddings)\n",
        "\n",
        "        # Create masks for clean and poisoned nodes\n",
        "        clean_mask = np.ones(embeddings.shape[0], dtype=bool)\n",
        "        clean_mask[poisoned_nodes] = False\n",
        "\n",
        "        # Extract clean and poisoned node embeddings after PCA\n",
        "        clean_embeddings = pca_result[clean_mask]\n",
        "        poisoned_embeddings = pca_result[~clean_mask]\n",
        "\n",
        "        # Plotting clean and poisoned nodes\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.scatter(clean_embeddings[:, 0], clean_embeddings[:, 1], s=10, alpha=0.5, label='Clean Nodes', c='b')\n",
        "        plt.scatter(poisoned_embeddings[:, 0], poisoned_embeddings[:, 1], s=10, alpha=0.8, label='Poisoned Nodes', c='r')\n",
        "        plt.title(f'PCA Visualization for {attack}')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "class SubstructureAwareGNN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(SubstructureAwareGNN, self).__init__()\n",
        "        self.ego_gnn = MessagePassingLayer(in_channels, hidden_channels)\n",
        "        self.cut_gnn = MessagePassingLayer(in_channels, hidden_channels)\n",
        "        self.global_encoder = nn.Linear(in_channels, hidden_channels)\n",
        "        self.final_fc = nn.Linear(3 * hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Extract subgraphs\n",
        "        ego_features = self.extract_ego_subgraph(x, edge_index)\n",
        "        cut_features = self.extract_cut_subgraph(x, edge_index)\n",
        "\n",
        "        # Apply GNN layers\n",
        "        ego_encoded = self.ego_gnn(ego_features, edge_index)\n",
        "        cut_encoded = self.cut_gnn(cut_features, edge_index)\n",
        "        global_encoded = self.global_encoder(x)\n",
        "\n",
        "        # Concatenate and pass through the final layer\n",
        "        combined_features = torch.cat([ego_encoded, cut_encoded, global_encoded], dim=-1)\n",
        "        output = self.final_fc(combined_features)\n",
        "        return F.log_softmax(output, dim=1)\n",
        "\n",
        "    def extract_ego_subgraph(self, x, edge_index):\n",
        "        # Optimize k-hop ego networks by leveraging PyTorch Geometric utilities\n",
        "        from torch_geometric.utils import k_hop_subgraph\n",
        "        k = 2\n",
        "        batch_size, num_features = x.size()\n",
        "        ego_features = torch.zeros_like(x, device=x.device)\n",
        "\n",
        "        for node_idx in range(batch_size):\n",
        "            neighbors, _, _, _ = k_hop_subgraph(node_idx, k, edge_index, num_nodes=batch_size, relabel_nodes=False)\n",
        "            if neighbors.numel() > 0:\n",
        "                ego_features[node_idx] = x[neighbors].mean(dim=0)  # Aggregate neighbor features\n",
        "            else:\n",
        "                ego_features[node_idx] = x[node_idx]  # Use the node's own features if no neighbors\n",
        "\n",
        "        return ego_features\n",
        "\n",
        "    def extract_cut_subgraph(self, x, edge_index):\n",
        "        # Optimize Cut subgraph computation using sparse operations\n",
        "        G = nx.Graph()\n",
        "        G.add_edges_from(edge_index.t().tolist())\n",
        "        edge_betweenness = nx.edge_betweenness_centrality(G)\n",
        "        sorted_edges = sorted(edge_betweenness.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        # Efficiently remove top edges\n",
        "        num_edges_to_remove = len(sorted_edges) // 2\n",
        "        G.remove_edges_from([edge for edge, _ in sorted_edges[:num_edges_to_remove]])\n",
        "\n",
        "        # Extract cut subgraph features\n",
        "        cut_features = torch.zeros_like(x, device=x.device)\n",
        "        for node in G.nodes():\n",
        "            cut_features[node] = x[node]\n",
        "\n",
        "        return cut_features\n",
        "\n",
        "# Define MessagePassingLayer\n",
        "class MessagePassingLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(MessagePassingLayer, self).__init__(aggr=\"add\")\n",
        "        self.linear = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        return self.propagate(edge_index, x=self.linear(x))\n",
        "\n",
        "    def message(self, x_j):\n",
        "        return x_j  # Message is the transformed neighbor feature\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return F.relu(aggr_out)  # Apply ReLU to the aggregated output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import to_dense_adj, k_hop_subgraph\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SUNLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SUNLayer, self).__init__()\n",
        "        # Separate transformations for root and non-root nodes\n",
        "        self.root_mlp = nn.Linear(in_channels, out_channels)\n",
        "        self.non_root_mlp = nn.Linear(in_channels, out_channels)\n",
        "        self.global_mlp = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, subgraph_masks):\n",
        "        # Convert edge_index to dense adjacency matrix\n",
        "        adjacency_matrix = to_dense_adj(edge_index, max_num_nodes=x.size(0))[0]\n",
        "\n",
        "        # Local message passing within subgraphs\n",
        "        local_features = torch.matmul(adjacency_matrix, x)\n",
        "\n",
        "        # Global aggregation across subgraphs\n",
        "        global_features = self.global_mlp(torch.mean(x, dim=0, keepdim=True))\n",
        "        global_features = global_features.expand(x.size(0), global_features.size(1))  # Broadcast to match x's shape\n",
        "\n",
        "        # Initialize root and non-root features with correct shape\n",
        "        root_features = torch.zeros((x.size(0), global_features.size(1)), device=x.device)\n",
        "        non_root_features = torch.zeros((x.size(0), global_features.size(1)), device=x.device)\n",
        "\n",
        "        # Apply transformations to root nodes\n",
        "        root_features[subgraph_masks] = self.root_mlp(x[subgraph_masks])\n",
        "\n",
        "        # Apply transformations to non-root nodes\n",
        "        non_root_features = self.non_root_mlp(local_features)\n",
        "\n",
        "        # Combine root, non-root, and global updates\n",
        "        updated_features = root_features + non_root_features + global_features\n",
        "\n",
        "        return updated_features\n",
        "\n",
        "class SUN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, hidden_channels):\n",
        "        super(SUN, self).__init__()\n",
        "        self.layer1 = SUNLayer(num_features, hidden_channels)\n",
        "        self.layer2 = SUNLayer(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        num_nodes = x.size(0)\n",
        "        subgraph_masks = torch.zeros(num_nodes, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        # Example subgraph extraction: mark every node as root for simplicity\n",
        "        for i in range(num_nodes):\n",
        "            _, _, _, node_mask = k_hop_subgraph(i, 1, edge_index, relabel_nodes=False, num_nodes=num_nodes)\n",
        "            subgraph_masks[node_mask[:num_nodes]] = True\n",
        "\n",
        "        # Pass through SUN layers\n",
        "        x = self.layer1(x, edge_index, subgraph_masks)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x, edge_index, subgraph_masks)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class ESAN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ESAN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.shared_aggregator = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, subgraphs, num_nodes, batch_size=50):\n",
        "        device = next(self.parameters()).device\n",
        "        # Correctly initialize node_predictions and node_counts\n",
        "        node_predictions = torch.zeros((num_nodes, self.shared_aggregator.out_features),\n",
        "                                       dtype=torch.float, device=device)\n",
        "        node_counts = torch.zeros(num_nodes, dtype=torch.float, device=device)\n",
        "\n",
        "        for i in range(0, len(subgraphs), batch_size):\n",
        "            batch = subgraphs[i:i + batch_size]\n",
        "            for subgraph in batch:\n",
        "                x, edge_index = subgraph.x.to(device), subgraph.edge_index.to(device)\n",
        "\n",
        "                # Debugging: Ensure input dimensions match the layer's expected dimensions\n",
        "                if x.shape[1] != self.conv1.in_channels:\n",
        "                    raise ValueError(f\"Shape mismatch! x.shape[1] = {x.shape[1]}, expected = {self.conv1.in_channels}\")\n",
        "\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "                x = self.conv2(x, edge_index)\n",
        "                x = self.shared_aggregator(x)\n",
        "                node_predictions[subgraph.n_id] += x\n",
        "                node_counts[subgraph.n_id] += 1\n",
        "\n",
        "        node_predictions = node_predictions / node_counts.unsqueeze(1).clamp(min=1)\n",
        "        return F.log_softmax(node_predictions, dim=1)\n",
        "\n",
        "\n",
        "def generate_subgraphs(data, policy=\"edge_deleted\", max_subgraphs=300):\n",
        "    \"\"\"\n",
        "    Generate subgraphs for ESAN based on different policies.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object.\n",
        "    - policy: Subgraph generation policy ('ego', 'edge_deleted', 'node_deleted').\n",
        "    - max_subgraphs: Maximum number of subgraphs to generate.\n",
        "\n",
        "    Returns:\n",
        "    - List of subgraphs in PyG data format.\n",
        "    \"\"\"\n",
        "    G = to_networkx(data, to_undirected=True)  # Convert PyG data to NetworkX graph\n",
        "    subgraphs = []\n",
        "\n",
        "    if policy == \"ego\":\n",
        "        radius = 2  # Example radius for ego networks\n",
        "        for node in range(data.num_nodes):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = nx.ego_graph(G, node, radius=radius)\n",
        "            pyg_subgraph = from_networkx(subgraph)  # Convert subgraph to PyG format\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes), dtype=torch.long)\n",
        "            pyg_subgraph.x = data.x[pyg_subgraph.n_id]  # Assign features from original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"edge_deleted\":\n",
        "        for edge in G.edges:\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = G.copy()\n",
        "            subgraph.remove_edge(*edge)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes), dtype=torch.long)\n",
        "            pyg_subgraph.x = data.x[pyg_subgraph.n_id]\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"node_deleted\":\n",
        "        for node in G.nodes:\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = G.copy()\n",
        "            subgraph.remove_node(node)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes), dtype=torch.long)\n",
        "            pyg_subgraph.x = data.x[pyg_subgraph.n_id]\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    return subgraphs\n",
        "\n",
        "def train_model(model, subgraphs, data, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(subgraphs, data.num_nodes)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_model(model, subgraphs, data):\n",
        "    model.eval()\n",
        "    logits = model(subgraphs, data.num_nodes)\n",
        "    accs = {}\n",
        "    for mask_name, mask in zip([\"Train\", \"Validation\", \"Test\"], [data.train_mask, data.val_mask, data.test_mask]):\n",
        "        pred = logits[mask].argmax(dim=1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs[mask_name] = acc\n",
        "    return accs\n",
        "\n",
        "\n",
        "def run_all_attacks():\n",
        "    datasets = [\"Cora\", \"PubMed\", \"CiteSeer\"]\n",
        "    results_summary = []\n",
        "    policies = [\"ego\", \"edge_deleted\", \"node_deleted\"]  # For ESAN\n",
        "    dataset_budgets = {'Cora': 10, 'PubMed': 40, 'CiteSeer': 30}  # Poisoning budgets\n",
        "\n",
        "    # Define models\n",
        "    model_types = ['SUN', 'ESAN', 'SAGN']\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        try:\n",
        "            dataset = load_dataset(dataset_name)\n",
        "            data = dataset[0].to(device)\n",
        "            input_dim = data.num_features\n",
        "            output_dim = dataset.num_classes if isinstance(dataset.num_classes, int) else dataset.num_classes[0]\n",
        "            data = split_dataset(data)\n",
        "            poisoned_node_budget = dataset_budgets.get(dataset_name, 10)\n",
        "\n",
        "            for model_type in model_types:\n",
        "                try:\n",
        "                    if model_type == \"SAGN\":\n",
        "                        model = SubstructureAwareGNN(in_channels=input_dim, hidden_channels=64, out_channels=output_dim).to(device)\n",
        "                        optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "                    elif model_type == \"SUN\":\n",
        "                        model = SUN(num_features=input_dim, num_classes=output_dim, hidden_channels=64).to(device)\n",
        "                        optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "                    elif model_type == \"ESAN\":\n",
        "                        optimizer = None  # Placeholder, as ESAN requires subgraph handling\n",
        "\n",
        "                    # Baseline training and accuracy evaluation\n",
        "                    if model_type != \"ESAN\":\n",
        "                        print(f\"Training baseline model ({model_type}) for dataset {dataset_name}.\")\n",
        "                        model.train()\n",
        "                        for epoch in range(200):\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(data.x, data.edge_index)\n",
        "                            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                        # Evaluate baseline accuracy\n",
        "                        model.eval()\n",
        "                        with torch.no_grad():\n",
        "                            out = model(data.x, data.edge_index)\n",
        "                            predictions = out.argmax(dim=1)\n",
        "                            baseline_acc = (predictions[data.test_mask] == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
        "                        print(f\"Dataset: {dataset_name}, Model: {model_type}, Baseline Accuracy: {baseline_acc * 100:.2f}%\")\n",
        "\n",
        "                        results_summary.append(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: None, Defense: None - ASR: N/A, Clean Accuracy: {baseline_acc * 100:.2f}%\")\n",
        "\n",
        "                        # Initialize defenses\n",
        "                        trigger_gen = TriggerGenerator(input_dim=data.num_features, hidden_dim=64).to(device)\n",
        "                        ood_detector = OODDetector(input_dim=input_dim, hidden_dim=64, latent_dim=16).to(device)\n",
        "                        ood_optimizer = torch.optim.Adam(ood_detector.parameters(), lr=0.001)\n",
        "                        train_ood_detector(ood_detector, data, ood_optimizer, epochs=10)\n",
        "\n",
        "                        poisoned_nodes = select_high_centrality_nodes(data, poisoned_node_budget)\n",
        "\n",
        "                        # Attack evaluation\n",
        "                        attack_methods = ['SBA-Samp', 'SBA-Gen', 'GTA', 'UGBA', 'DPGBA']\n",
        "                        for attack in attack_methods:\n",
        "                            try:\n",
        "                                trained_model, data_poisoned = train_with_poisoned_data(\n",
        "                                    model=model,\n",
        "                                    data=data,\n",
        "                                    optimizer=optimizer,\n",
        "                                    poisoned_nodes=poisoned_nodes,\n",
        "                                    trigger_gen=trigger_gen,\n",
        "                                    attack=attack,\n",
        "                                    ood_detector=ood_detector,\n",
        "                                    alpha=0.7,\n",
        "                                    early_stopping=True\n",
        "                                )\n",
        "\n",
        "                                asr, clean_acc = compute_metrics(trained_model, data_poisoned, poisoned_nodes)\n",
        "                                results_summary.append(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: None - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "                                print(f\"Dataset: {dataset_name}, Model: {model_type}, Attack: {attack}, Defense: None - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error during attack {attack} on {dataset_name} with {model_type}: {e}\")\n",
        "\n",
        "                    # ESAN Training and Attacks\n",
        "                    else:\n",
        "                        for policy in policies:\n",
        "                            try:\n",
        "                                subgraphs = generate_subgraphs(data, policy=policy, max_subgraphs=300)\n",
        "                                model = ESAN(input_dim + (1 if policy == \"ego\" else 0), hidden_dim=64, output_dim=output_dim).to(device)\n",
        "                                optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "                                print(f\"Training ESAN model ({policy} policy) for dataset {dataset_name}.\")\n",
        "                                model.train()\n",
        "                                for epoch in range(50):\n",
        "                                    optimizer.zero_grad()\n",
        "                                    out = model(subgraphs, data.num_nodes)\n",
        "                                    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "                                    loss.backward()\n",
        "                                    optimizer.step()\n",
        "                                    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                                # Evaluate ESAN accuracy\n",
        "                                model.eval()\n",
        "                                accs = test_model(model, subgraphs, data)\n",
        "                                baseline_acc = accs[\"Test\"]\n",
        "                                results_summary.append(f\"Dataset: {dataset_name}, Model: {model_type}, Policy: {policy}, Attack: None, Defense: None - ASR: N/A, Clean Accuracy: {baseline_acc * 100:.2f}%\")\n",
        "                                print(f\"Dataset: {dataset_name}, Model: {model_type}, Policy: {policy}, Baseline Test Accuracy: {baseline_acc * 100:.2f}%\")\n",
        "\n",
        "                                # Attack evaluation\n",
        "                                for attack in ['SBA-Samp', 'SBA-Gen', 'GTA', 'UGBA', 'DPGBA']:\n",
        "                                    try:\n",
        "                                        trained_model, data_poisoned = train_with_poisoned_data(\n",
        "                                            model=model,\n",
        "                                            data=data,\n",
        "                                            optimizer=optimizer,\n",
        "                                            poisoned_nodes=select_high_centrality_nodes(data, poisoned_node_budget),\n",
        "                                            trigger_gen=TriggerGenerator(input_dim=input_dim, hidden_dim=64).to(device),\n",
        "                                            attack=attack,\n",
        "                                            alpha=0.7\n",
        "                                        )\n",
        "                                        asr, clean_acc = compute_metrics(trained_model, data_poisoned, poisoned_nodes)\n",
        "                                        results_summary.append(f\"Dataset: {dataset_name}, Model: {model_type}, Policy: {policy}, Attack: {attack}, Defense: None - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "                                        print(f\"Dataset: {dataset_name}, Model: {model_type}, Policy: {policy}, Attack: {attack}, Defense: None - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"Error during attack {attack} on {dataset_name} with ESAN ({policy} policy): {e}\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error during ESAN policy {policy} on {dataset_name}: {e}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with model {model_type} on dataset {dataset_name}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing dataset {dataset_name}: {e}\")\n",
        "\n",
        "    # Save results\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nSummary of Results:\")\n",
        "    for result in results_summary:\n",
        "        print(result)\n",
        "    results_df.to_csv(\"backdoor_attack_results_summary.csv\", index=False)\n",
        "\n",
        "# Run the pipeline\n",
        "run_all_attacks()\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T05:37:22.669095Z",
          "iopub.execute_input": "2025-01-21T05:37:22.669462Z"
        },
        "id": "dEDPsUSlXyVQ",
        "outputId": "33023bd2-4bd9-4849-e96d-10133ce76a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.9.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nUsing device: cuda\nTraining baseline model (SUN) for dataset Cora.\nEpoch 1, Loss: 2.0277\nEpoch 2, Loss: 1.6632\nEpoch 3, Loss: 1.3256\nEpoch 4, Loss: 1.1587\nEpoch 5, Loss: 1.0267\nEpoch 6, Loss: 0.9171\nEpoch 7, Loss: 0.8218\nEpoch 8, Loss: 0.7465\nEpoch 9, Loss: 0.6870\nEpoch 10, Loss: 0.6369\nEpoch 11, Loss: 0.5944\nEpoch 12, Loss: 0.5532\nEpoch 13, Loss: 0.5140\nEpoch 14, Loss: 0.4829\nEpoch 15, Loss: 0.4562\nEpoch 16, Loss: 0.4299\nEpoch 17, Loss: 0.4086\nEpoch 18, Loss: 0.3895\nEpoch 19, Loss: 0.3691\nEpoch 20, Loss: 0.3489\nEpoch 21, Loss: 0.3298\nEpoch 22, Loss: 0.3098\nEpoch 23, Loss: 0.2908\nEpoch 24, Loss: 0.2738\nEpoch 25, Loss: 0.2587\nEpoch 26, Loss: 0.2449\nEpoch 27, Loss: 0.2328\nEpoch 28, Loss: 0.2221\nEpoch 29, Loss: 0.2114\nEpoch 30, Loss: 0.2007\nEpoch 31, Loss: 0.1908\nEpoch 32, Loss: 0.1809\nEpoch 33, Loss: 0.1712\nEpoch 34, Loss: 0.1621\nEpoch 35, Loss: 0.1533\nEpoch 36, Loss: 0.1449\nEpoch 37, Loss: 0.1372\nEpoch 38, Loss: 0.1298\nEpoch 39, Loss: 0.1230\nEpoch 40, Loss: 0.1167\nEpoch 41, Loss: 0.1105\nEpoch 42, Loss: 0.1046\nEpoch 43, Loss: 0.0990\nEpoch 44, Loss: 0.0936\nEpoch 45, Loss: 0.0885\nEpoch 46, Loss: 0.0837\nEpoch 47, Loss: 0.0791\nEpoch 48, Loss: 0.0749\nEpoch 49, Loss: 0.0708\nEpoch 50, Loss: 0.0670\nEpoch 51, Loss: 0.0634\nEpoch 52, Loss: 0.0601\nEpoch 53, Loss: 0.0570\nEpoch 54, Loss: 0.0540\nEpoch 55, Loss: 0.0513\nEpoch 56, Loss: 0.0487\nEpoch 57, Loss: 0.0463\nEpoch 58, Loss: 0.0439\nEpoch 59, Loss: 0.0418\nEpoch 60, Loss: 0.0398\nEpoch 61, Loss: 0.0378\nEpoch 62, Loss: 0.0361\nEpoch 63, Loss: 0.0344\nEpoch 64, Loss: 0.0328\nEpoch 65, Loss: 0.0313\nEpoch 66, Loss: 0.0299\nEpoch 67, Loss: 0.0286\nEpoch 68, Loss: 0.0274\nEpoch 69, Loss: 0.0262\nEpoch 70, Loss: 0.0251\nEpoch 71, Loss: 0.0241\nEpoch 72, Loss: 0.0231\nEpoch 73, Loss: 0.0222\nEpoch 74, Loss: 0.0214\nEpoch 75, Loss: 0.0205\nEpoch 76, Loss: 0.0198\nEpoch 77, Loss: 0.0191\nEpoch 78, Loss: 0.0184\nEpoch 79, Loss: 0.0177\nEpoch 80, Loss: 0.0171\nEpoch 81, Loss: 0.0165\nEpoch 82, Loss: 0.0160\nEpoch 83, Loss: 0.0154\nEpoch 84, Loss: 0.0149\nEpoch 85, Loss: 0.0145\nEpoch 86, Loss: 0.0140\nEpoch 87, Loss: 0.0136\nEpoch 88, Loss: 0.0132\nEpoch 89, Loss: 0.0128\nEpoch 90, Loss: 0.0124\nEpoch 91, Loss: 0.0121\nEpoch 92, Loss: 0.0118\nEpoch 93, Loss: 0.0114\nEpoch 94, Loss: 0.0111\nEpoch 95, Loss: 0.0108\nEpoch 96, Loss: 0.0106\nEpoch 97, Loss: 0.0103\nEpoch 98, Loss: 0.0100\nEpoch 99, Loss: 0.0098\nEpoch 100, Loss: 0.0095\nEpoch 101, Loss: 0.0093\nEpoch 102, Loss: 0.0091\nEpoch 103, Loss: 0.0089\nEpoch 104, Loss: 0.0087\nEpoch 105, Loss: 0.0085\nEpoch 106, Loss: 0.0083\nEpoch 107, Loss: 0.0081\nEpoch 108, Loss: 0.0080\nEpoch 109, Loss: 0.0078\nEpoch 110, Loss: 0.0076\nEpoch 111, Loss: 0.0075\nEpoch 112, Loss: 0.0073\nEpoch 113, Loss: 0.0072\nEpoch 114, Loss: 0.0070\nEpoch 115, Loss: 0.0069\nEpoch 116, Loss: 0.0068\nEpoch 117, Loss: 0.0066\nEpoch 118, Loss: 0.0065\nEpoch 119, Loss: 0.0064\nEpoch 120, Loss: 0.0063\nEpoch 121, Loss: 0.0061\nEpoch 122, Loss: 0.0060\nEpoch 123, Loss: 0.0059\nEpoch 124, Loss: 0.0058\nEpoch 125, Loss: 0.0057\nEpoch 126, Loss: 0.0056\nEpoch 127, Loss: 0.0055\nEpoch 128, Loss: 0.0054\nEpoch 129, Loss: 0.0053\nEpoch 130, Loss: 0.0053\nEpoch 131, Loss: 0.0052\nEpoch 132, Loss: 0.0051\nEpoch 133, Loss: 0.0050\nEpoch 134, Loss: 0.0049\nEpoch 135, Loss: 0.0049\nEpoch 136, Loss: 0.0048\nEpoch 137, Loss: 0.0047\nEpoch 138, Loss: 0.0046\nEpoch 139, Loss: 0.0046\nEpoch 140, Loss: 0.0045\nEpoch 141, Loss: 0.0044\nEpoch 142, Loss: 0.0044\nEpoch 143, Loss: 0.0043\nEpoch 144, Loss: 0.0042\nEpoch 145, Loss: 0.0042\nEpoch 146, Loss: 0.0041\nEpoch 147, Loss: 0.0041\nEpoch 148, Loss: 0.0040\nEpoch 149, Loss: 0.0039\nEpoch 150, Loss: 0.0039\nEpoch 151, Loss: 0.0038\nEpoch 152, Loss: 0.0038\nEpoch 153, Loss: 0.0037\nEpoch 154, Loss: 0.0037\nEpoch 155, Loss: 0.0036\nEpoch 156, Loss: 0.0036\nEpoch 157, Loss: 0.0035\nEpoch 158, Loss: 0.0035\nEpoch 159, Loss: 0.0035\nEpoch 160, Loss: 0.0034\nEpoch 161, Loss: 0.0034\nEpoch 162, Loss: 0.0033\nEpoch 163, Loss: 0.0033\nEpoch 164, Loss: 0.0032\nEpoch 165, Loss: 0.0032\nEpoch 166, Loss: 0.0032\nEpoch 167, Loss: 0.0031\nEpoch 168, Loss: 0.0031\nEpoch 169, Loss: 0.0030\nEpoch 170, Loss: 0.0030\nEpoch 171, Loss: 0.0030\nEpoch 172, Loss: 0.0029\nEpoch 173, Loss: 0.0029\nEpoch 174, Loss: 0.0029\nEpoch 175, Loss: 0.0028\nEpoch 176, Loss: 0.0028\nEpoch 177, Loss: 0.0028\nEpoch 178, Loss: 0.0027\nEpoch 179, Loss: 0.0027\nEpoch 180, Loss: 0.0027\nEpoch 181, Loss: 0.0027\nEpoch 182, Loss: 0.0026\nEpoch 183, Loss: 0.0026\nEpoch 184, Loss: 0.0026\nEpoch 185, Loss: 0.0025\nEpoch 186, Loss: 0.0025\nEpoch 187, Loss: 0.0025\nEpoch 188, Loss: 0.0025\nEpoch 189, Loss: 0.0024\nEpoch 190, Loss: 0.0024\nEpoch 191, Loss: 0.0024\nEpoch 192, Loss: 0.0024\nEpoch 193, Loss: 0.0023\nEpoch 194, Loss: 0.0023\nEpoch 195, Loss: 0.0023\nEpoch 196, Loss: 0.0023\nEpoch 197, Loss: 0.0022\nEpoch 198, Loss: 0.0022\nEpoch 199, Loss: 0.0022\nEpoch 200, Loss: 0.0022\nDataset: Cora, Model: SUN, Baseline Accuracy: 87.62%\nEpoch 0, Reconstruction Loss: 0.0129\nEpoch 0, Loss: 0.0036\nEpoch 10, Loss: 0.0025\nEpoch 20, Loss: 0.0021\nEpoch 30, Loss: 0.0019\nEpoch 40, Loss: 0.0017\nEpoch 50, Loss: 0.0015\nEpoch 60, Loss: 0.0014\nEpoch 70, Loss: 0.0013\nEpoch 80, Loss: 0.0012\nEpoch 90, Loss: 0.0011\nDataset: Cora, Model: SUN, Attack: SBA-Samp, Defense: None - ASR: 100.00%, Clean Accuracy: 87.80%\nEpoch 0, Loss: 0.0011\nEpoch 10, Loss: 0.0010\nEpoch 20, Loss: 0.0009\nEpoch 30, Loss: 0.0008\nEpoch 40, Loss: 0.0008\nEpoch 50, Loss: 0.0007\nEpoch 60, Loss: 0.0007\nEpoch 70, Loss: 0.0007\nEpoch 80, Loss: 0.0006\nEpoch 90, Loss: 0.0006\nDataset: Cora, Model: SUN, Attack: SBA-Gen, Defense: None - ASR: 100.00%, Clean Accuracy: 87.43%\nEpoch 0, Loss: 0.0186\nEpoch 10, Loss: 0.0049\nEpoch 20, Loss: 0.0185\nEpoch 30, Loss: 0.0013\nEpoch 40, Loss: 0.0010\nEpoch 50, Loss: 0.0007\nEpoch 60, Loss: 0.0005\nEpoch 70, Loss: 0.0005\nEpoch 80, Loss: 0.0004\nEpoch 90, Loss: 0.0004\nDataset: Cora, Model: SUN, Attack: GTA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.06%\nEpoch 0, Loss: 5.0885\nEpoch 10, Loss: 4.9207\nEpoch 20, Loss: 1.7380\nEpoch 30, Loss: 0.5453\nEpoch 40, Loss: 0.1801\nEpoch 50, Loss: 0.0914\nEpoch 60, Loss: 0.0550\nEpoch 70, Loss: 0.0427\nEpoch 80, Loss: 0.0366\nEpoch 90, Loss: 0.0317\nDataset: Cora, Model: SUN, Attack: UGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.06%\nEpoch 0, Loss: 0.0350\nEpoch 10, Loss: 0.0248\nEpoch 20, Loss: 0.0179\nEpoch 30, Loss: 0.0141\nEpoch 40, Loss: 0.0119\nEpoch 50, Loss: 0.0104\nEpoch 60, Loss: 0.0092\nEpoch 70, Loss: 0.0083\nEpoch 80, Loss: 0.0076\nEpoch 90, Loss: 0.0070\nDataset: Cora, Model: SUN, Attack: DPGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 87.43%\nTraining ESAN model (ego policy) for dataset Cora.\nError during ESAN policy ego on Cora: Shape mismatch! x.shape[1] = 1433, expected = 1434\nTraining ESAN model (edge_deleted policy) for dataset Cora.\nEpoch 1, Loss: 1.9575\nEpoch 2, Loss: 1.8044\nEpoch 3, Loss: 1.6204\nEpoch 4, Loss: 1.4595\nEpoch 5, Loss: 1.2097\nEpoch 6, Loss: 0.9865\nEpoch 7, Loss: 0.8000\nEpoch 8, Loss: 0.6158\nEpoch 9, Loss: 0.4689\nEpoch 10, Loss: 0.3792\nEpoch 11, Loss: 0.3205\nEpoch 12, Loss: 0.2826\nEpoch 13, Loss: 0.2623\nEpoch 14, Loss: 0.2468\nEpoch 15, Loss: 0.2307\nEpoch 16, Loss: 0.2167\nEpoch 17, Loss: 0.2068\nEpoch 18, Loss: 0.1932\nEpoch 19, Loss: 0.1757\nEpoch 20, Loss: 0.1640\nEpoch 21, Loss: 0.1501\nEpoch 22, Loss: 0.1367\nEpoch 23, Loss: 0.1234\nEpoch 24, Loss: 0.1102\nEpoch 25, Loss: 0.1029\nEpoch 26, Loss: 0.0984\nEpoch 27, Loss: 0.0907\nEpoch 28, Loss: 0.0858\nEpoch 29, Loss: 0.0804\nEpoch 30, Loss: 0.0738\nEpoch 31, Loss: 0.0688\nEpoch 32, Loss: 0.0643\nEpoch 33, Loss: 0.0607\nEpoch 34, Loss: 0.0571\nEpoch 35, Loss: 0.0541\nEpoch 36, Loss: 0.0506\nEpoch 37, Loss: 0.0470\nEpoch 38, Loss: 0.0436\nEpoch 39, Loss: 0.0406\nEpoch 40, Loss: 0.0388\nEpoch 41, Loss: 0.0363\nEpoch 42, Loss: 0.0348\nEpoch 43, Loss: 0.0327\nEpoch 44, Loss: 0.0311\nEpoch 45, Loss: 0.0300\nEpoch 46, Loss: 0.0280\nEpoch 47, Loss: 0.0266\nEpoch 48, Loss: 0.0253\nEpoch 49, Loss: 0.0248\nEpoch 50, Loss: 0.0240\nDataset: Cora, Model: ESAN, Policy: edge_deleted, Baseline Test Accuracy: 87.62%\nError during attack SBA-Samp on Cora with ESAN (edge_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack SBA-Gen on Cora with ESAN (edge_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack GTA on Cora with ESAN (edge_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack UGBA on Cora with ESAN (edge_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack DPGBA on Cora with ESAN (edge_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nTraining ESAN model (node_deleted policy) for dataset Cora.\nEpoch 1, Loss: 1.9718\nEpoch 2, Loss: 1.8333\nEpoch 3, Loss: 1.6690\nEpoch 4, Loss: 1.5379\nEpoch 5, Loss: 1.3191\nEpoch 6, Loss: 1.1050\nEpoch 7, Loss: 0.9090\nEpoch 8, Loss: 0.7149\nEpoch 9, Loss: 0.5520\nEpoch 10, Loss: 0.4425\nEpoch 11, Loss: 0.3693\nEpoch 12, Loss: 0.3192\nEpoch 13, Loss: 0.2889\nEpoch 14, Loss: 0.2659\nEpoch 15, Loss: 0.2429\nEpoch 16, Loss: 0.2251\nEpoch 17, Loss: 0.2149\nEpoch 18, Loss: 0.1984\nEpoch 19, Loss: 0.1852\nEpoch 20, Loss: 0.1745\nEpoch 21, Loss: 0.1606\nEpoch 22, Loss: 0.1491\nEpoch 23, Loss: 0.1336\nEpoch 24, Loss: 0.1205\nEpoch 25, Loss: 0.1115\nEpoch 26, Loss: 0.1016\nEpoch 27, Loss: 0.0972\nEpoch 28, Loss: 0.0917\nEpoch 29, Loss: 0.0868\nEpoch 30, Loss: 0.0807\nEpoch 31, Loss: 0.0748\nEpoch 32, Loss: 0.0693\nEpoch 33, Loss: 0.0631\nEpoch 34, Loss: 0.0600\nEpoch 35, Loss: 0.0566\nEpoch 36, Loss: 0.0542\nEpoch 37, Loss: 0.0513\nEpoch 38, Loss: 0.0483\nEpoch 39, Loss: 0.0455\nEpoch 40, Loss: 0.0421\nEpoch 41, Loss: 0.0403\nEpoch 42, Loss: 0.0373\nEpoch 43, Loss: 0.0363\nEpoch 44, Loss: 0.0337\nEpoch 45, Loss: 0.0326\nEpoch 46, Loss: 0.0308\nEpoch 47, Loss: 0.0299\nEpoch 48, Loss: 0.0284\nEpoch 49, Loss: 0.0265\nEpoch 50, Loss: 0.0254\nDataset: Cora, Model: ESAN, Policy: node_deleted, Baseline Test Accuracy: 87.80%\nError during attack SBA-Samp on Cora with ESAN (node_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack SBA-Gen on Cora with ESAN (node_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack GTA on Cora with ESAN (node_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack UGBA on Cora with ESAN (node_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nError during attack DPGBA on Cora with ESAN (node_deleted policy): zeros() received an invalid combination of arguments - got (tuple, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n\nTraining baseline model (SAGN) for dataset Cora.\nEpoch 1, Loss: 1.9587\nEpoch 2, Loss: 1.8326\nEpoch 3, Loss: 1.7154\nEpoch 4, Loss: 1.5930\nEpoch 5, Loss: 1.4681\nEpoch 6, Loss: 1.3449\nEpoch 7, Loss: 1.2274\nEpoch 8, Loss: 1.1161\nEpoch 9, Loss: 1.0121\nEpoch 10, Loss: 0.9165\nEpoch 11, Loss: 0.8296\nEpoch 12, Loss: 0.7513\nEpoch 13, Loss: 0.6813\nEpoch 14, Loss: 0.6189\nEpoch 15, Loss: 0.5634\nEpoch 16, Loss: 0.5143\nEpoch 17, Loss: 0.4706\nEpoch 18, Loss: 0.4316\nEpoch 19, Loss: 0.3964\nEpoch 20, Loss: 0.3644\nEpoch 21, Loss: 0.3355\nEpoch 22, Loss: 0.3094\nEpoch 23, Loss: 0.2857\nEpoch 24, Loss: 0.2642\nEpoch 25, Loss: 0.2443\nEpoch 26, Loss: 0.2259\nEpoch 27, Loss: 0.2089\nEpoch 28, Loss: 0.1930\nEpoch 29, Loss: 0.1782\nEpoch 30, Loss: 0.1644\nEpoch 31, Loss: 0.1515\nEpoch 32, Loss: 0.1396\nEpoch 33, Loss: 0.1287\nEpoch 34, Loss: 0.1187\nEpoch 35, Loss: 0.1094\nEpoch 36, Loss: 0.1009\nEpoch 37, Loss: 0.0931\nEpoch 38, Loss: 0.0859\nEpoch 39, Loss: 0.0794\nEpoch 40, Loss: 0.0734\nEpoch 41, Loss: 0.0679\nEpoch 42, Loss: 0.0629\nEpoch 43, Loss: 0.0583\nEpoch 44, Loss: 0.0541\nEpoch 45, Loss: 0.0502\nEpoch 46, Loss: 0.0467\nEpoch 47, Loss: 0.0435\nEpoch 48, Loss: 0.0405\nEpoch 49, Loss: 0.0379\nEpoch 50, Loss: 0.0354\nEpoch 51, Loss: 0.0332\nEpoch 52, Loss: 0.0312\nEpoch 53, Loss: 0.0293\nEpoch 54, Loss: 0.0276\nEpoch 55, Loss: 0.0261\nEpoch 56, Loss: 0.0247\nEpoch 57, Loss: 0.0234\nEpoch 58, Loss: 0.0222\nEpoch 59, Loss: 0.0211\nEpoch 60, Loss: 0.0201\nEpoch 61, Loss: 0.0192\nEpoch 62, Loss: 0.0183\nEpoch 63, Loss: 0.0176\nEpoch 64, Loss: 0.0168\nEpoch 65, Loss: 0.0161\nEpoch 66, Loss: 0.0155\nEpoch 67, Loss: 0.0149\nEpoch 68, Loss: 0.0143\nEpoch 69, Loss: 0.0138\nEpoch 70, Loss: 0.0133\nEpoch 71, Loss: 0.0129\nEpoch 72, Loss: 0.0124\nEpoch 73, Loss: 0.0120\nEpoch 74, Loss: 0.0116\nEpoch 75, Loss: 0.0113\nEpoch 76, Loss: 0.0109\nEpoch 77, Loss: 0.0106\nEpoch 78, Loss: 0.0103\nEpoch 79, Loss: 0.0100\nEpoch 80, Loss: 0.0097\nEpoch 81, Loss: 0.0095\nEpoch 82, Loss: 0.0092\nEpoch 83, Loss: 0.0090\nEpoch 84, Loss: 0.0088\nEpoch 85, Loss: 0.0085\nEpoch 86, Loss: 0.0083\nEpoch 87, Loss: 0.0081\nEpoch 88, Loss: 0.0079\nEpoch 89, Loss: 0.0078\nEpoch 90, Loss: 0.0076\nEpoch 91, Loss: 0.0074\nEpoch 92, Loss: 0.0073\nEpoch 93, Loss: 0.0071\nEpoch 94, Loss: 0.0070\nEpoch 95, Loss: 0.0068\nEpoch 96, Loss: 0.0067\nEpoch 97, Loss: 0.0065\nEpoch 98, Loss: 0.0064\nEpoch 99, Loss: 0.0063\nEpoch 100, Loss: 0.0062\nEpoch 101, Loss: 0.0061\nEpoch 102, Loss: 0.0059\nEpoch 103, Loss: 0.0058\nEpoch 104, Loss: 0.0057\nEpoch 105, Loss: 0.0056\nEpoch 106, Loss: 0.0055\nEpoch 107, Loss: 0.0054\nEpoch 108, Loss: 0.0053\nEpoch 109, Loss: 0.0053\nEpoch 110, Loss: 0.0052\nEpoch 111, Loss: 0.0051\nEpoch 112, Loss: 0.0050\nEpoch 113, Loss: 0.0049\nEpoch 114, Loss: 0.0048\nEpoch 115, Loss: 0.0048\nEpoch 116, Loss: 0.0047\nEpoch 117, Loss: 0.0046\nEpoch 118, Loss: 0.0045\nEpoch 119, Loss: 0.0045\nEpoch 120, Loss: 0.0044\nEpoch 121, Loss: 0.0043\nEpoch 122, Loss: 0.0043\nEpoch 123, Loss: 0.0042\nEpoch 124, Loss: 0.0042\nEpoch 125, Loss: 0.0041\nEpoch 126, Loss: 0.0040\nEpoch 127, Loss: 0.0040\nEpoch 128, Loss: 0.0039\nEpoch 129, Loss: 0.0039\nEpoch 130, Loss: 0.0038\nEpoch 131, Loss: 0.0038\nEpoch 132, Loss: 0.0037\nEpoch 133, Loss: 0.0037\nEpoch 134, Loss: 0.0036\nEpoch 135, Loss: 0.0036\nEpoch 136, Loss: 0.0035\nEpoch 137, Loss: 0.0035\nEpoch 138, Loss: 0.0034\nEpoch 139, Loss: 0.0034\nEpoch 140, Loss: 0.0033\nEpoch 141, Loss: 0.0033\nEpoch 142, Loss: 0.0033\nEpoch 143, Loss: 0.0032\nEpoch 144, Loss: 0.0032\nEpoch 145, Loss: 0.0031\nEpoch 146, Loss: 0.0031\nEpoch 147, Loss: 0.0031\nEpoch 148, Loss: 0.0030\nEpoch 149, Loss: 0.0030\nEpoch 150, Loss: 0.0030\nEpoch 151, Loss: 0.0029\nEpoch 152, Loss: 0.0029\nEpoch 153, Loss: 0.0029\nEpoch 154, Loss: 0.0028\nEpoch 155, Loss: 0.0028\nEpoch 156, Loss: 0.0028\nEpoch 157, Loss: 0.0027\nEpoch 158, Loss: 0.0027\nEpoch 159, Loss: 0.0027\nEpoch 160, Loss: 0.0026\nEpoch 161, Loss: 0.0026\nEpoch 162, Loss: 0.0026\nEpoch 163, Loss: 0.0026\nEpoch 164, Loss: 0.0025\nEpoch 165, Loss: 0.0025\nEpoch 166, Loss: 0.0025\nEpoch 167, Loss: 0.0024\nEpoch 168, Loss: 0.0024\nEpoch 169, Loss: 0.0024\nEpoch 170, Loss: 0.0024\nEpoch 171, Loss: 0.0023\nEpoch 172, Loss: 0.0023\nEpoch 173, Loss: 0.0023\nEpoch 174, Loss: 0.0023\nEpoch 175, Loss: 0.0023\nEpoch 176, Loss: 0.0022\nEpoch 177, Loss: 0.0022\nEpoch 178, Loss: 0.0022\nEpoch 179, Loss: 0.0022\nEpoch 180, Loss: 0.0021\nEpoch 181, Loss: 0.0021\nEpoch 182, Loss: 0.0021\nEpoch 183, Loss: 0.0021\nEpoch 184, Loss: 0.0021\nEpoch 185, Loss: 0.0020\nEpoch 186, Loss: 0.0020\nEpoch 187, Loss: 0.0020\nEpoch 188, Loss: 0.0020\nEpoch 189, Loss: 0.0020\nEpoch 190, Loss: 0.0019\nEpoch 191, Loss: 0.0019\nEpoch 192, Loss: 0.0019\nEpoch 193, Loss: 0.0019\nEpoch 194, Loss: 0.0019\nEpoch 195, Loss: 0.0019\nEpoch 196, Loss: 0.0018\nEpoch 197, Loss: 0.0018\nEpoch 198, Loss: 0.0018\nEpoch 199, Loss: 0.0018\nEpoch 200, Loss: 0.0018\nDataset: Cora, Model: SAGN, Baseline Accuracy: 86.69%\nEpoch 0, Reconstruction Loss: 0.0128\nEpoch 0, Loss: 0.0025\nEpoch 10, Loss: 0.0020\nEpoch 20, Loss: 0.0018\nEpoch 30, Loss: 0.0016\nEpoch 40, Loss: 0.0014\nEpoch 50, Loss: 0.0013\nEpoch 60, Loss: 0.0012\nEpoch 70, Loss: 0.0011\nEpoch 80, Loss: 0.0010\nEpoch 90, Loss: 0.0010\nDataset: Cora, Model: SAGN, Attack: SBA-Samp, Defense: None - ASR: 100.00%, Clean Accuracy: 86.69%\nEpoch 0, Loss: 0.0010\nEpoch 10, Loss: 0.0009\nEpoch 20, Loss: 0.0008\nEpoch 30, Loss: 0.0008\nEpoch 40, Loss: 0.0007\nEpoch 50, Loss: 0.0007\nEpoch 60, Loss: 0.0006\nEpoch 70, Loss: 0.0006\nEpoch 80, Loss: 0.0006\nEpoch 90, Loss: 0.0005\nDataset: Cora, Model: SAGN, Attack: SBA-Gen, Defense: None - ASR: 100.00%, Clean Accuracy: 86.88%\nEpoch 0, Loss: 0.0008\nEpoch 10, Loss: 0.0006\nEpoch 20, Loss: 0.0005\nEpoch 30, Loss: 0.0005\nEpoch 40, Loss: 0.0005\nEpoch 50, Loss: 0.0004\nEpoch 60, Loss: 0.0004\nEpoch 70, Loss: 0.0004\nEpoch 80, Loss: 0.0004\nEpoch 90, Loss: 0.0004\nDataset: Cora, Model: SAGN, Attack: GTA, Defense: None - ASR: 100.00%, Clean Accuracy: 86.88%\nEpoch 0, Loss: 4.5422\nEpoch 10, Loss: 3.0728\nEpoch 20, Loss: 1.1626\nEpoch 30, Loss: 0.5265\nEpoch 40, Loss: 0.1310\nEpoch 50, Loss: 0.0536\nEpoch 60, Loss: 0.0255\nEpoch 70, Loss: 0.0123\nEpoch 80, Loss: 0.0067\nEpoch 90, Loss: 0.0044\nDataset: Cora, Model: SAGN, Attack: UGBA, Defense: None - ASR: 90.00%, Clean Accuracy: 84.10%\nEpoch 0, Loss: 0.0032\nEpoch 10, Loss: 0.0016\nEpoch 20, Loss: 0.0012\nEpoch 30, Loss: 0.0010\nEpoch 40, Loss: 0.0010\nEpoch 50, Loss: 0.0009\nEpoch 60, Loss: 0.0009\nEpoch 70, Loss: 0.0008\nEpoch 80, Loss: 0.0008\nEpoch 90, Loss: 0.0008\nDataset: Cora, Model: SAGN, Attack: DPGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 83.92%\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training baseline model (SUN) for dataset PubMed.\nEpoch 1, Loss: 1.1009\nEpoch 2, Loss: 0.9953\nEpoch 3, Loss: 0.9319\nEpoch 4, Loss: 0.8860\nEpoch 5, Loss: 0.8427\nEpoch 6, Loss: 0.8019\nEpoch 7, Loss: 0.7707\nEpoch 8, Loss: 0.7481\nEpoch 9, Loss: 0.7272\nEpoch 10, Loss: 0.7066\nEpoch 11, Loss: 0.6882\nEpoch 12, Loss: 0.6762\nEpoch 13, Loss: 0.6676\nEpoch 14, Loss: 0.6564\nEpoch 15, Loss: 0.6453\nEpoch 16, Loss: 0.6370\nEpoch 17, Loss: 0.6305\nEpoch 18, Loss: 0.6247\nEpoch 19, Loss: 0.6175\nEpoch 20, Loss: 0.6107\nEpoch 21, Loss: 0.6060\nEpoch 22, Loss: 0.6013\nEpoch 23, Loss: 0.5955\nEpoch 24, Loss: 0.5898\nEpoch 25, Loss: 0.5848\nEpoch 26, Loss: 0.5807\nEpoch 27, Loss: 0.5762\nEpoch 28, Loss: 0.5710\nEpoch 29, Loss: 0.5667\nEpoch 30, Loss: 0.5627\nEpoch 31, Loss: 0.5584\nEpoch 32, Loss: 0.5540\nEpoch 33, Loss: 0.5499\nEpoch 34, Loss: 0.5465\nEpoch 35, Loss: 0.5428\nEpoch 36, Loss: 0.5388\nEpoch 37, Loss: 0.5351\nEpoch 38, Loss: 0.5315\nEpoch 39, Loss: 0.5278\nEpoch 40, Loss: 0.5239\nEpoch 41, Loss: 0.5202\nEpoch 42, Loss: 0.5165\nEpoch 43, Loss: 0.5126\nEpoch 44, Loss: 0.5088\nEpoch 45, Loss: 0.5052\nEpoch 46, Loss: 0.5016\nEpoch 47, Loss: 0.4979\nEpoch 48, Loss: 0.4944\nEpoch 49, Loss: 0.4910\nEpoch 50, Loss: 0.4875\nEpoch 51, Loss: 0.4841\nEpoch 52, Loss: 0.4808\nEpoch 53, Loss: 0.4774\nEpoch 54, Loss: 0.4741\nEpoch 55, Loss: 0.4709\nEpoch 56, Loss: 0.4676\nEpoch 57, Loss: 0.4644\nEpoch 58, Loss: 0.4613\nEpoch 59, Loss: 0.4581\nEpoch 60, Loss: 0.4550\nEpoch 61, Loss: 0.4520\nEpoch 62, Loss: 0.4489\nEpoch 63, Loss: 0.4459\nEpoch 64, Loss: 0.4428\nEpoch 65, Loss: 0.4398\nEpoch 66, Loss: 0.4368\nEpoch 67, Loss: 0.4339\nEpoch 68, Loss: 0.4309\nEpoch 69, Loss: 0.4280\nEpoch 70, Loss: 0.4251\nEpoch 71, Loss: 0.4223\nEpoch 72, Loss: 0.4194\nEpoch 73, Loss: 0.4166\nEpoch 74, Loss: 0.4139\nEpoch 75, Loss: 0.4111\nEpoch 76, Loss: 0.4083\nEpoch 77, Loss: 0.4056\nEpoch 78, Loss: 0.4028\nEpoch 79, Loss: 0.4001\nEpoch 80, Loss: 0.3974\nEpoch 81, Loss: 0.3948\nEpoch 82, Loss: 0.3921\nEpoch 83, Loss: 0.3895\nEpoch 84, Loss: 0.3869\nEpoch 85, Loss: 0.3843\nEpoch 86, Loss: 0.3817\nEpoch 87, Loss: 0.3792\nEpoch 88, Loss: 0.3766\nEpoch 89, Loss: 0.3741\nEpoch 90, Loss: 0.3716\nEpoch 91, Loss: 0.3692\nEpoch 92, Loss: 0.3668\nEpoch 93, Loss: 0.3643\nEpoch 94, Loss: 0.3619\nEpoch 95, Loss: 0.3596\nEpoch 96, Loss: 0.3572\nEpoch 97, Loss: 0.3549\nEpoch 98, Loss: 0.3526\nEpoch 99, Loss: 0.3504\nEpoch 100, Loss: 0.3481\nEpoch 101, Loss: 0.3459\nEpoch 102, Loss: 0.3437\nEpoch 103, Loss: 0.3415\nEpoch 104, Loss: 0.3394\nEpoch 105, Loss: 0.3373\nEpoch 106, Loss: 0.3351\nEpoch 107, Loss: 0.3331\nEpoch 108, Loss: 0.3310\nEpoch 109, Loss: 0.3289\nEpoch 110, Loss: 0.3269\nEpoch 111, Loss: 0.3249\nEpoch 112, Loss: 0.3229\nEpoch 113, Loss: 0.3209\nEpoch 114, Loss: 0.3190\nEpoch 115, Loss: 0.3171\nEpoch 116, Loss: 0.3152\nEpoch 117, Loss: 0.3133\nEpoch 118, Loss: 0.3115\nEpoch 119, Loss: 0.3098\nEpoch 120, Loss: 0.3081\nEpoch 121, Loss: 0.3064\nEpoch 122, Loss: 0.3045\nEpoch 123, Loss: 0.3026\nEpoch 124, Loss: 0.3008\nEpoch 125, Loss: 0.2992\nEpoch 126, Loss: 0.2977\nEpoch 127, Loss: 0.2961\nEpoch 128, Loss: 0.2944\nEpoch 129, Loss: 0.2926\nEpoch 130, Loss: 0.2910\nEpoch 131, Loss: 0.2895\nEpoch 132, Loss: 0.2880\nEpoch 133, Loss: 0.2866\nEpoch 134, Loss: 0.2850\nEpoch 135, Loss: 0.2835\nEpoch 136, Loss: 0.2819\nEpoch 137, Loss: 0.2804\nEpoch 138, Loss: 0.2790\nEpoch 139, Loss: 0.2776\nEpoch 140, Loss: 0.2763\nEpoch 141, Loss: 0.2750\nEpoch 142, Loss: 0.2737\nEpoch 143, Loss: 0.2724\nEpoch 144, Loss: 0.2711\nEpoch 145, Loss: 0.2697\nEpoch 146, Loss: 0.2683\nEpoch 147, Loss: 0.2669\nEpoch 148, Loss: 0.2656\nEpoch 149, Loss: 0.2643\nEpoch 150, Loss: 0.2632\nEpoch 151, Loss: 0.2621\nEpoch 152, Loss: 0.2609\nEpoch 153, Loss: 0.2597\nEpoch 154, Loss: 0.2585\nEpoch 155, Loss: 0.2572\nEpoch 156, Loss: 0.2560\nEpoch 157, Loss: 0.2547\nEpoch 158, Loss: 0.2536\nEpoch 159, Loss: 0.2525\nEpoch 160, Loss: 0.2515\nEpoch 161, Loss: 0.2505\nEpoch 162, Loss: 0.2495\nEpoch 163, Loss: 0.2485\nEpoch 164, Loss: 0.2474\nEpoch 165, Loss: 0.2463\nEpoch 166, Loss: 0.2452\nEpoch 167, Loss: 0.2440\nEpoch 168, Loss: 0.2429\nEpoch 169, Loss: 0.2418\nEpoch 170, Loss: 0.2409\nEpoch 171, Loss: 0.2399\nEpoch 172, Loss: 0.2391\nEpoch 173, Loss: 0.2382\nEpoch 174, Loss: 0.2374\nEpoch 175, Loss: 0.2365\nEpoch 176, Loss: 0.2355\nEpoch 177, Loss: 0.2344\nEpoch 178, Loss: 0.2333\nEpoch 179, Loss: 0.2322\nEpoch 180, Loss: 0.2313\nEpoch 181, Loss: 0.2305\nEpoch 182, Loss: 0.2297\nEpoch 183, Loss: 0.2290\nEpoch 184, Loss: 0.2282\nEpoch 185, Loss: 0.2273\nEpoch 186, Loss: 0.2263\nEpoch 187, Loss: 0.2253\nEpoch 188, Loss: 0.2244\nEpoch 189, Loss: 0.2235\nEpoch 190, Loss: 0.2227\nEpoch 191, Loss: 0.2219\nEpoch 192, Loss: 0.2212\nEpoch 193, Loss: 0.2205\nEpoch 194, Loss: 0.2198\nEpoch 195, Loss: 0.2190\nEpoch 196, Loss: 0.2181\nEpoch 197, Loss: 0.2172\nEpoch 198, Loss: 0.2163\nEpoch 199, Loss: 0.2155\nEpoch 200, Loss: 0.2147\nDataset: PubMed, Model: SUN, Baseline Accuracy: 89.30%\nEpoch 0, Reconstruction Loss: 0.0003\nEpoch 0, Loss: 0.2147\nEpoch 10, Loss: 0.2074\nEpoch 20, Loss: 0.2003\nEpoch 30, Loss: 0.1942\nEpoch 40, Loss: 0.1882\nEpoch 50, Loss: 0.1828\nEpoch 60, Loss: 0.1777\nEpoch 70, Loss: 0.1728\nEpoch 80, Loss: 0.1689\nEpoch 90, Loss: 0.1637\nDataset: PubMed, Model: SUN, Attack: SBA-Samp, Defense: None - ASR: 100.00%, Clean Accuracy: 89.35%\nEpoch 0, Loss: 0.1610\nEpoch 10, Loss: 0.1562\nEpoch 20, Loss: 0.1521\nEpoch 30, Loss: 0.1483\nEpoch 40, Loss: 0.1447\nEpoch 50, Loss: 0.1410\nEpoch 60, Loss: 0.1383\nEpoch 70, Loss: 0.1345\nEpoch 80, Loss: 0.1312\nEpoch 90, Loss: 0.1281\nDataset: PubMed, Model: SUN, Attack: SBA-Gen, Defense: None - ASR: 100.00%, Clean Accuracy: 88.76%\nEpoch 0, Loss: 0.1660\nEpoch 10, Loss: 0.1445\nEpoch 20, Loss: 0.1384\nEpoch 30, Loss: 0.1280\nEpoch 40, Loss: 0.1241\nEpoch 50, Loss: 0.1208\nEpoch 60, Loss: 0.1185\nEpoch 70, Loss: 0.1165\nEpoch 80, Loss: 0.1146\nEpoch 90, Loss: 0.1129\nDataset: PubMed, Model: SUN, Attack: GTA, Defense: None - ASR: 100.00%, Clean Accuracy: 89.02%\nEpoch 0, Loss: 4.7371\nEpoch 10, Loss: 0.8551\nEpoch 20, Loss: 0.2975\nEpoch 30, Loss: 0.2372\nEpoch 40, Loss: 0.1991\nEpoch 50, Loss: 0.1631\nEpoch 60, Loss: 0.1505\nEpoch 70, Loss: 0.1425\nEpoch 80, Loss: 0.1376\nEpoch 90, Loss: 0.1341\nDataset: PubMed, Model: SUN, Attack: UGBA, Defense: None - ASR: 97.50%, Clean Accuracy: 88.84%\nEpoch 0, Loss: 0.1495\nEpoch 10, Loss: 0.1346\nEpoch 20, Loss: 0.1286\nEpoch 30, Loss: 0.1247\nEpoch 40, Loss: 0.1223\nEpoch 50, Loss: 0.1205\nEpoch 60, Loss: 0.1190\nEpoch 70, Loss: 0.1177\nEpoch 80, Loss: 0.1165\nEpoch 90, Loss: 0.1154\nDataset: PubMed, Model: SUN, Attack: DPGBA, Defense: None - ASR: 100.00%, Clean Accuracy: 89.15%\nTraining ESAN model (ego policy) for dataset PubMed.\nError during ESAN policy ego on PubMed: Shape mismatch! x.shape[1] = 500, expected = 501\nTraining ESAN model (edge_deleted policy) for dataset PubMed.\nError during ESAN policy edge_deleted on PubMed: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 31.12 MiB is free. Process 23406 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 50.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nError during ESAN policy node_deleted on PubMed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 35.12 MiB is free. Process 23406 has 15.85 GiB memory in use. Of the allocated memory 15.37 GiB is allocated by PyTorch, and 191.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nTraining baseline model (SAGN) for dataset PubMed.\nEpoch 1, Loss: 1.1007\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}