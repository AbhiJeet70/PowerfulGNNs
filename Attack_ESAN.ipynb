{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Attack_ESAN",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/PowerfulGNNs/blob/main/Attack_ESAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import subgraph\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, Flickr\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import networkx as nx\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.nn.models.autoencoder import GAE\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gc\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name in [\"Cora\", \"PubMed\", \"CiteSeer\"]:\n",
        "        dataset = Planetoid(root=f\"./data/{dataset_name}\", name=dataset_name)\n",
        "    elif dataset_name == \"Flickr\":\n",
        "        dataset = Flickr(root=\"./data/Flickr\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(data, test_size=0.2, val_size=0.1, seed=None):\n",
        "    # Get the total number of nodes in the graph\n",
        "    num_nodes = int(data.num_nodes)\n",
        "\n",
        "    # Shuffle indices\n",
        "    indices = np.arange(num_nodes)\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    num_test = int(test_size * num_nodes)\n",
        "    num_val = int(val_size * num_nodes)\n",
        "    num_train = num_nodes - num_test - num_val\n",
        "    num_target = int(0.1 * num_nodes)\n",
        "\n",
        "    # Ensure there are enough nodes for the splits\n",
        "    assert num_train > 0, \"Training set size is too small!\"\n",
        "    assert num_val > 0, \"Validation set size is too small!\"\n",
        "    assert num_test > 0, \"Test set size is too small!\"\n",
        "    assert num_target > 0, \"Target mask size is too small!\"\n",
        "\n",
        "    # Initialize boolean masks as NumPy arrays\n",
        "    train_mask = np.zeros(num_nodes, dtype=bool)\n",
        "    val_mask = np.zeros(num_nodes, dtype=bool)\n",
        "    test_mask = np.zeros(num_nodes, dtype=bool)\n",
        "    target_mask = np.zeros(num_nodes, dtype=bool)\n",
        "    clean_test_mask = np.zeros(num_nodes, dtype=bool)\n",
        "\n",
        "    # Assign indices to each mask\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train + num_val]] = True\n",
        "    test_mask[indices[num_train + num_val:]] = True\n",
        "    target_mask[indices[num_train + num_val:num_train + num_val + num_target]] = True\n",
        "    clean_test_mask[indices[num_train + num_val + num_target:]] = True\n",
        "\n",
        "    # Convert the NumPy arrays to torch tensors (and send them to the same device as data.x)\n",
        "    device = data.x.device if hasattr(data.x, \"device\") else torch.device(\"cpu\")\n",
        "    data.train_mask = torch.tensor(train_mask, dtype=torch.bool, device=device)\n",
        "    data.val_mask = torch.tensor(val_mask, dtype=torch.bool, device=device)\n",
        "    data.test_mask = torch.tensor(test_mask, dtype=torch.bool, device=device)\n",
        "    data.target_mask = torch.tensor(target_mask, dtype=torch.bool, device=device)\n",
        "    data.clean_test_mask = torch.tensor(clean_test_mask, dtype=torch.bool, device=device)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def select_high_centrality_nodes(data, num_nodes_to_select):\n",
        "    graph = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    graph.add_edges_from(edge_index.T)\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    sorted_nodes = sorted(centrality, key=centrality.get, reverse=True)\n",
        "    return torch.tensor(sorted_nodes[:num_nodes_to_select], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "class TriggerGenerator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(TriggerGenerator, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),  # Reduce dimensions\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, input_dim)  # Restore to input_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class GAE(torch.nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        return self.encoder(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "class OODDetector(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(OODDetector, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.gae = GAE(self.encoder, self.decoder)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        return z\n",
        "\n",
        "    def reconstruct(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "    def reconstruction_loss(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        reconstructed = self.reconstruct(z, edge_index)\n",
        "        return F.mse_loss(reconstructed, x)\n",
        "\n",
        "    def detect_ood(self, x, edge_index, threshold):\n",
        "        loss = self.reconstruction_loss(x, edge_index)\n",
        "        return loss > threshold\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = F.relu(self.conv1(x, edge_index))\n",
        "        z = self.conv2(z, edge_index)\n",
        "        return z\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = GCNConv(latent_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        x = F.relu(self.conv1(z, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def train_ood_detector(ood_detector, data, optimizer, epochs=50):\n",
        "    ood_detector.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        z = ood_detector(data.x, data.edge_index)\n",
        "\n",
        "        # Reconstruct data using the latent embedding\n",
        "        reconstructed_x = ood_detector.reconstruct(z, data.edge_index)\n",
        "\n",
        "        # Use only the training mask to compute reconstruction loss\n",
        "        loss = F.mse_loss(reconstructed_x[data.train_mask], data.x[data.train_mask])\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Print loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Reconstruction Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_with_poisoned_data(model, data, optimizer, poisoned_nodes, trigger_gen, attack, ood_detector=None, alpha=0.7, early_stopping=False):\n",
        "    # Apply trigger injection\n",
        "    data_poisoned = inject_trigger(data, poisoned_nodes, attack, trigger_gen, ood_detector, alpha)\n",
        "\n",
        "    # Ensure data_poisoned is a valid PyG Data object\n",
        "    if not isinstance(data_poisoned, Data):\n",
        "        raise TypeError(\"data_poisoned must be a PyTorch Geometric Data object.\")\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data_poisoned.x, data_poisoned.edge_index)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.cross_entropy(out[data_poisoned.train_mask], data_poisoned.y[data_poisoned.train_mask])\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional logging\n",
        "        if early_stopping and epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, data_poisoned\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def select_diverse_nodes(data, num_nodes_to_select, num_clusters=None):\n",
        "    if num_clusters is None:\n",
        "        num_clusters = len(torch.unique(data.y))\n",
        "\n",
        "    # Use GCN encoder to get node embeddings\n",
        "    encoder = GCNEncoder(data.num_features, out_channels=16).to(data.x.device)  # Move encoder to the correct device\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = encoder(data.x, data.edge_index).to(data.x.device)  # Ensure embeddings are on the correct device\n",
        "\n",
        "    # Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(embeddings.cpu().numpy())  # Clustering runs on CPU\n",
        "    labels = kmeans.labels_\n",
        "    cluster_centers = torch.tensor(kmeans.cluster_centers_, device=data.x.device)  # Move cluster centers to device\n",
        "\n",
        "    # Select nodes closest to the cluster centers\n",
        "    selected_nodes = []\n",
        "    for i in range(num_clusters):\n",
        "        cluster_indices = torch.where(torch.tensor(labels, device=data.x.device) == i)[0]\n",
        "        center = cluster_centers[i]\n",
        "        distances = torch.norm(embeddings[cluster_indices] - center, dim=1)\n",
        "        closest_node = cluster_indices[torch.argmin(distances)]\n",
        "        selected_nodes.append(closest_node)\n",
        "\n",
        "    # Calculate node degrees\n",
        "    degree = torch.bincount(data.edge_index[0], minlength=data.num_nodes).to(data.x.device)\n",
        "    high_degree_nodes = torch.topk(degree, len(selected_nodes) // 2).indices\n",
        "\n",
        "    # Combine diverse nodes and high-degree nodes\n",
        "    combined_nodes = torch.cat([torch.tensor(selected_nodes, device=data.x.device), high_degree_nodes])\n",
        "    unique_nodes = torch.unique(combined_nodes)[:num_nodes_to_select]\n",
        "\n",
        "    return unique_nodes.to(data.x.device)  # Ensure the selected nodes are on the correct device\n",
        "\n",
        "def inject_trigger(data, poisoned_nodes, attack_type, model, trigger_gen=None, ood_detector=None,\n",
        "                   alpha=0.7, trigger_size=5, trigger_density=0.5, input_dim=None):\n",
        "    \"\"\"\n",
        "    Injects a trigger into the graph according to the specified attack type.\n",
        "    This version includes modifications for the GTA branch to ensure that subgraphs\n",
        "    have the correct feature matrix.\n",
        "\n",
        "    Returns:\n",
        "      data_poisoned: The modified (poisoned) PyG Data object.\n",
        "      target_labels: The new target labels for the poisoned nodes.\n",
        "    \"\"\"\n",
        "    # Clone the original data so that the graph remains unchanged.\n",
        "    data_poisoned = data.clone()\n",
        "    device = data_poisoned.x.device\n",
        "\n",
        "    if poisoned_nodes.numel() == 0:\n",
        "        raise ValueError(\"No poisoned nodes selected. Ensure 'poisoned_nodes' is non-empty.\")\n",
        "\n",
        "    # Use at most trigger_size poisoned nodes.\n",
        "    trigger_size = min(trigger_size, poisoned_nodes.numel())\n",
        "    p_nodes_list = poisoned_nodes[:trigger_size].tolist()\n",
        "    p_nodes_tensor = torch.tensor(p_nodes_list, dtype=torch.long, device=device)\n",
        "    # Copy target labels for all poisoned nodes.\n",
        "    target_labels = data.y[poisoned_nodes].clone()\n",
        "\n",
        "    if attack_type == 'GTA':\n",
        "        # Graph Trojan Attack\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        trigger_features = avg_features + torch.randn_like(avg_features) * 0.05\n",
        "        data_poisoned = data.clone()\n",
        "        data_poisoned.x[poisoned_nodes] = trigger_features\n",
        "\n",
        "        # Define target labels (for example, all zeros)\n",
        "        target_labels = torch.zeros(len(poisoned_nodes), dtype=torch.long, device=data.x.device)\n",
        "\n",
        "        # Return both the modified data and the target labels\n",
        "        return data_poisoned, target_labels\n",
        "\n",
        "\n",
        "    elif attack_type == 'SBA-Samp':\n",
        "        # --- SBA-Samp: Subgraph-Based Attack via Random Sampling ---\n",
        "        connected_nodes_list = []\n",
        "        for node in p_nodes_list:\n",
        "            mask = (data.edge_index[1] == node)\n",
        "            src_nodes = data.edge_index[0][mask]\n",
        "            connected_nodes_list.append(src_nodes.tolist())\n",
        "        avg_features = []\n",
        "        for nodes_list in connected_nodes_list:\n",
        "            if len(nodes_list) > 0:\n",
        "                nodes_tensor = torch.tensor(nodes_list, dtype=torch.long, device=device)\n",
        "                avg_features.append(data.x[nodes_tensor].mean(dim=0))\n",
        "            else:\n",
        "                avg_features.append(data.x.mean(dim=0))\n",
        "        avg_features = torch.stack(avg_features)\n",
        "        natural_features = avg_features + torch.randn_like(avg_features) * 0.02\n",
        "\n",
        "        import networkx as nx\n",
        "        G = nx.erdos_renyi_graph(trigger_size, trigger_density)\n",
        "        edges = list(G.edges)\n",
        "        if edges:\n",
        "            trigger_edge_index = torch.tensor(edges, dtype=torch.long, device=device).t().contiguous()\n",
        "        else:\n",
        "            trigger_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
        "\n",
        "        rand_edges = torch.stack([\n",
        "            p_nodes_tensor,\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index, rand_edges], dim=1)\n",
        "        data_poisoned.x[p_nodes_tensor] = natural_features\n",
        "\n",
        "        num_classes = int(data.y.max().item()) + 1\n",
        "        for i, node in enumerate(p_nodes_list):\n",
        "            orig = int(data.y[node].item())\n",
        "            possible = list(set(range(num_classes)) - {orig})\n",
        "            target_labels[i] = random.choice(possible)\n",
        "        return data_poisoned, target_labels\n",
        "\n",
        "    elif attack_type == 'SBA-Gen':\n",
        "        # --- SBA-Gen: Subgraph-Based Attack using Gaussian perturbations ---\n",
        "        connected_nodes_list = []\n",
        "        for node in p_nodes_list:\n",
        "            mask = (data.edge_index[1] == node)\n",
        "            src_nodes = data.edge_index[0][mask]\n",
        "            connected_nodes_list.append(src_nodes.tolist())\n",
        "        feature_mean = data.x.mean(dim=0)\n",
        "        feature_std = data.x.std(dim=0) + 1e-6\n",
        "\n",
        "        avg_features = []\n",
        "        for nodes_list in connected_nodes_list:\n",
        "            if len(nodes_list) > 0:\n",
        "                nodes_tensor = torch.tensor(nodes_list, dtype=torch.long, device=device)\n",
        "                avg_features.append(data.x[nodes_tensor].mean(dim=0))\n",
        "            else:\n",
        "                avg_features.append(feature_mean)\n",
        "        avg_features = torch.stack(avg_features)\n",
        "        natural_features = avg_features + torch.normal(mean=0.0, std=0.03, size=avg_features.shape, device=device)\n",
        "\n",
        "        local_edges = []\n",
        "        for i in range(trigger_size):\n",
        "            for j in range(i+1, trigger_size):\n",
        "                diff = natural_features[i] - natural_features[j]\n",
        "                sim = torch.exp(-torch.norm(diff / feature_std)**2)\n",
        "                if sim.item() > random.random():\n",
        "                    local_edges.append([i, j])\n",
        "        if local_edges:\n",
        "            local_edge_index = torch.tensor(local_edges, dtype=torch.long, device=device)\n",
        "            mapped_edge_index = torch.stack([\n",
        "                p_nodes_tensor[local_edge_index[:, 0]],\n",
        "                p_nodes_tensor[local_edge_index[:, 1]]\n",
        "            ], dim=0)\n",
        "            trigger_edge_index = mapped_edge_index\n",
        "        else:\n",
        "            trigger_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
        "\n",
        "        rand_edges = torch.stack([\n",
        "            p_nodes_tensor,\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index, rand_edges], dim=1)\n",
        "        data_poisoned.x[p_nodes_tensor] = natural_features\n",
        "        num_classes = int(data.y.max().item()) + 1\n",
        "        for i, node in enumerate(p_nodes_list):\n",
        "            orig = int(data.y[node].item())\n",
        "            possible = list(set(range(num_classes)) - {orig})\n",
        "            target_labels[i] = random.choice(possible)\n",
        "        return data_poisoned, target_labels\n",
        "\n",
        "    elif attack_type == 'DPGBA':\n",
        "        # --- DPGBA: Distribution-Preserving Graph Backdoor Attack ---\n",
        "        p_nodes_full = poisoned_nodes\n",
        "        p_nodes_full_list = p_nodes_full.tolist()\n",
        "        connected_nodes_list = []\n",
        "        for node in p_nodes_full_list:\n",
        "            mask = (data.edge_index[1] == node)\n",
        "            src_nodes = data.edge_index[0][mask]\n",
        "            connected_nodes_list.append(src_nodes.tolist())\n",
        "        avg_features = []\n",
        "        for nodes_list in connected_nodes_list:\n",
        "            if len(nodes_list) > 0:\n",
        "                nodes_tensor = torch.tensor(nodes_list, dtype=torch.long, device=device)\n",
        "                avg_features.append(data.x[nodes_tensor].mean(dim=0))\n",
        "            else:\n",
        "                avg_features.append(data.x.mean(dim=0))\n",
        "        avg_features = torch.stack(avg_features).to(device)\n",
        "        if trigger_gen is None:\n",
        "            raise ValueError(\"Trigger generator is required for DPGBA attack.\")\n",
        "        with torch.no_grad():\n",
        "            trigger_features = trigger_gen(avg_features)\n",
        "        if trigger_features.shape[1] != data.x.shape[1]:\n",
        "            raise ValueError(f\"Trigger feature dimension mismatch: {trigger_features.shape[1]} vs {data.x.shape[1]}\")\n",
        "        node_alphas = torch.rand(p_nodes_full.numel(), device=device) * 0.3 + 0.5\n",
        "        distribution_preserved_features = (node_alphas.unsqueeze(1) * data.x[p_nodes_full] +\n",
        "                                           (1 - node_alphas.unsqueeze(1)) * trigger_features)\n",
        "        num_classes = int(data.y.max().item()) + 1\n",
        "        target_labels = (data.y[p_nodes_full] + 1) % num_classes\n",
        "        data_poisoned.x[p_nodes_full] = distribution_preserved_features\n",
        "        return data_poisoned, target_labels\n",
        "\n",
        "    elif attack_type == 'UGBA':\n",
        "        # --- UGBA: Unnoticeable Graph Backdoor Attack ---\n",
        "        diverse_nodes = select_diverse_nodes(data_poisoned, poisoned_nodes.numel())\n",
        "        d_nodes_list = diverse_nodes.tolist()\n",
        "        d_nodes_tensor = torch.tensor(d_nodes_list, dtype=torch.long, device=device)\n",
        "        connected_nodes_list = []\n",
        "        for node in d_nodes_list:\n",
        "            mask = (data_poisoned.edge_index[1] == node)\n",
        "            src_nodes = data_poisoned.edge_index[0][mask]\n",
        "            connected_nodes_list.append(src_nodes.tolist())\n",
        "        avg_features = []\n",
        "        for nodes_list in connected_nodes_list:\n",
        "            if len(nodes_list) > 0:\n",
        "                nodes_tensor = torch.tensor(nodes_list, dtype=torch.long, device=device)\n",
        "                avg_features.append(data_poisoned.x[nodes_tensor].mean(dim=0))\n",
        "            else:\n",
        "                avg_features.append(data_poisoned.x.mean(dim=0))\n",
        "        avg_features = torch.stack(avg_features)\n",
        "        refined_features = avg_features + torch.normal(mean=2.0, std=0.5, size=avg_features.shape, device=device)\n",
        "        data_poisoned.x[d_nodes_tensor] = refined_features\n",
        "        new_edges = []\n",
        "        for i, node in enumerate(d_nodes_list):\n",
        "            if len(connected_nodes_list[i]) > 0:\n",
        "                neighbor = int(connected_nodes_list[i][0])\n",
        "            else:\n",
        "                neighbor = d_nodes_list[(i + 1) % len(d_nodes_list)]\n",
        "            new_edges.append([node, neighbor])\n",
        "        if new_edges:\n",
        "            new_edges = torch.tensor(new_edges, dtype=torch.long, device=device).t().contiguous()\n",
        "            data_poisoned.edge_index = torch.cat([data_poisoned.edge_index, new_edges], dim=1)\n",
        "        target_labels[:] = 0\n",
        "        return data_poisoned, target_labels\n",
        "\n",
        "    else:\n",
        "        # If an unsupported attack type is provided.\n",
        "        raise ValueError(f\"Unsupported attack type: {attack_type}\")\n",
        "\n",
        "\n",
        "\n",
        "def dominant_set_clustering(data, threshold=0.7, use_pca=True, pca_components=10):\n",
        "    \"\"\"\n",
        "    Applies a simplified outlier detection framework using a combination of K-Means clustering and distance-based heuristics.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - threshold: Quantile threshold for identifying outliers based on cluster distances.\n",
        "    - use_pca: Whether to use PCA for dimensionality reduction.\n",
        "    - pca_components: Number of PCA components to use if PCA is applied.\n",
        "\n",
        "    Returns:\n",
        "    - pruned_nodes: Set of nodes identified as outliers.\n",
        "    - data: Updated PyG data object with modified features and labels for outliers.\n",
        "    \"\"\"\n",
        "    # Step 1: Determine the number of clusters based on the number of classes\n",
        "    n_clusters = len(data.y.unique())  # Number of unique classes in the dataset\n",
        "\n",
        "    # Step 2: Dimensionality reduction using PCA (optional)\n",
        "    node_features = data.x.detach().cpu().numpy()\n",
        "    if use_pca and node_features.shape[1] > pca_components:\n",
        "        pca = PCA(n_components=pca_components)\n",
        "        node_features = pca.fit_transform(node_features)\n",
        "\n",
        "    # Step 3: K-Means Clustering to identify clusters and potential outliers\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(node_features)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculate distances to cluster centers\n",
        "    distances = np.linalg.norm(node_features - cluster_centers[cluster_labels], axis=1)\n",
        "\n",
        "    # Identify outlier candidates based on distance threshold\n",
        "    distance_threshold = np.percentile(distances, 100 * threshold)\n",
        "    outlier_candidates = np.where(distances > distance_threshold)[0]\n",
        "\n",
        "    # Step 4: Update data to reflect removal of outlier influence\n",
        "    pruned_nodes = set(outlier_candidates)\n",
        "    if len(pruned_nodes) > 0:\n",
        "        outliers = torch.tensor(list(pruned_nodes), dtype=torch.long, device=data.x.device)\n",
        "\n",
        "        # Assign an invalid label (-1) to outlier nodes to discard them during training\n",
        "        data.y[outliers] = -1\n",
        "\n",
        "        # Replace the features of outliers with the average feature value to reduce their impact\n",
        "        data.x[outliers] = data.x.mean(dim=0).to(data.x.device)\n",
        "\n",
        "    return pruned_nodes, data\n",
        "\n",
        "def defense_prune_edges(data, quantile_threshold=0.9):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity between node features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile to determine pruning threshold (e.g., 0.9 means pruning edges in the top 10% dissimilar).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Adaptive threshold based on quantile of similarity distribution\n",
        "    similarity_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Keep edges with cosine similarity above the threshold\n",
        "    pruned_mask = cosine_similarities >= similarity_threshold\n",
        "    pruned_edges = edge_index[:, pruned_mask]\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def defense_prune_and_discard_labels(data, quantile_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity and discards labels of nodes connected by pruned edges selectively.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile threshold for cosine similarity pruning (e.g., 0.2 means pruning edges in the bottom 20%).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges and selectively discarded labels.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features using PyTorch\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Use quantile to determine adaptive threshold for pruning\n",
        "    adaptive_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Mask edges with similarity below the adaptive threshold\n",
        "    pruned_mask = cosine_similarities < adaptive_threshold\n",
        "    pruned_edges = edge_index[:, ~pruned_mask]  # Retain edges that are above the threshold\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    # Selectively discard labels of nodes connected by many pruned edges\n",
        "    pruned_src, pruned_dst = edge_index[:, pruned_mask]\n",
        "    pruned_nodes_count = torch.bincount(torch.cat([pruned_src, pruned_dst]), minlength=data.num_nodes)\n",
        "\n",
        "    # Only discard labels if the node has a high count of pruned edges\n",
        "    threshold_count = int(torch.median(pruned_nodes_count).item())  # Use median count as a threshold\n",
        "    nodes_to_discard = torch.where(pruned_nodes_count > threshold_count)[0]\n",
        "\n",
        "    data.y[nodes_to_discard] = -1  # Use -1 to represent discarded labels\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Visualization Function\n",
        "# Visualize PCA for Attacks\n",
        "# Added function to visualize PCA projections of node embeddings for different attacks\n",
        "def visualize_pca_for_attacks(attack_embeddings_dict):\n",
        "    pca = PCA(n_components=2)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    for i, (attack, attack_data) in enumerate(attack_embeddings_dict.items(), 1):\n",
        "        embeddings = attack_data['data'].detach().cpu().numpy()\n",
        "        poisoned_nodes = attack_data['poisoned_nodes'].detach().cpu().numpy()\n",
        "\n",
        "        # Apply PCA to the node embeddings\n",
        "        pca_result = pca.fit_transform(embeddings)\n",
        "\n",
        "        # Create masks for clean and poisoned nodes\n",
        "        clean_mask = np.ones(embeddings.shape[0], dtype=bool)\n",
        "        clean_mask[poisoned_nodes] = False\n",
        "\n",
        "        # Extract clean and poisoned node embeddings after PCA\n",
        "        clean_embeddings = pca_result[clean_mask]\n",
        "        poisoned_embeddings = pca_result[~clean_mask]\n",
        "\n",
        "        # Plotting clean and poisoned nodes\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.scatter(clean_embeddings[:, 0], clean_embeddings[:, 1], s=10, alpha=0.5, label='Clean Nodes', c='b')\n",
        "        plt.scatter(poisoned_embeddings[:, 0], poisoned_embeddings[:, 1], s=10, alpha=0.8, label='Poisoned Nodes', c='r')\n",
        "        plt.title(f'PCA Visualization for {attack}')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx, from_networkx\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def load_dataset(name):\n",
        "    dataset = Planetoid(root=f\"./data/{name}\", name=name)\n",
        "    data = dataset[0]\n",
        "\n",
        "    # Ensure node features are initialized\n",
        "    if data.x is None:\n",
        "        num_nodes = data.num_nodes\n",
        "        data.x = torch.eye(num_nodes)  # One-hot encoding for nodes\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Define the ESAN Model\n",
        "class ESAN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ESAN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.shared_aggregator = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, subgraphs, num_nodes, batch_size=50):\n",
        "        # Initialize tensors to store node predictions and counts\n",
        "        device = next(self.parameters()).device\n",
        "        node_predictions = torch.zeros((num_nodes, self.shared_aggregator.out_features), device=device)\n",
        "        node_counts = torch.zeros(num_nodes, device=device)\n",
        "\n",
        "        # Process subgraphs in batches\n",
        "        for i in range(0, len(subgraphs), batch_size):\n",
        "            batch = subgraphs[i:i + batch_size]\n",
        "            for subgraph in batch:\n",
        "                x, edge_index = subgraph.x.to(device), subgraph.edge_index.to(device)\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "                x = self.conv2(x, edge_index)\n",
        "\n",
        "                # Map to output dimension (num_classes)\n",
        "                x = self.shared_aggregator(x)\n",
        "\n",
        "                # Aggregate features for nodes in the subgraph\n",
        "                node_predictions[subgraph.n_id] += x\n",
        "                node_counts[subgraph.n_id] += 1\n",
        "\n",
        "        # Average predictions for nodes that appear in multiple subgraphs\n",
        "        node_predictions = node_predictions / node_counts.unsqueeze(1).clamp(min=1)\n",
        "        return F.log_softmax(node_predictions, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# Train the ESAN model\n",
        "def train_model(model, subgraphs, data, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(subgraphs, data.num_nodes)  # Process subgraphs through the model\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
        "            train_acc = (out[data.train_mask].argmax(dim=1) == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Test the ESAN model\n",
        "def test_model(model, subgraphs, data):\n",
        "    model.eval()\n",
        "    logits = model(subgraphs, data.num_nodes)  # Process subgraphs through the model\n",
        "    accs = []\n",
        "    for mask_name, mask in zip([\"Train\", \"Validation\", \"Test\"], [data.train_mask, data.val_mask, data.test_mask]):\n",
        "        pred = logits[mask].argmax(dim=1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "        print(f\"{mask_name} Accuracy: {acc:.4f}\")\n",
        "    return accs\n",
        "\n",
        "def generate_subgraphs(data, policy=\"edge_deleted\", max_subgraphs=300):\n",
        "    graph = to_networkx(data, to_undirected=True)\n",
        "    subgraphs = []\n",
        "\n",
        "    if policy == \"edge_deleted\":\n",
        "        for i, edge in enumerate(graph.edges):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = graph.copy()\n",
        "            subgraph.remove_edge(*edge)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes))  # Map subgraph nodes to original graph nodes\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[pyg_subgraph.n_id]  # Use features from the original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"node_deleted\":\n",
        "        for i, node in enumerate(graph.nodes):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = graph.copy()\n",
        "            subgraph.remove_node(node)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes))  # Map subgraph nodes to original graph nodes\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[pyg_subgraph.n_id]  # Use features from the original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"ego\":\n",
        "        radius = 2\n",
        "        for i, node in enumerate(graph.nodes):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            # Generate ego graph for the node with the specified radius\n",
        "            subgraph = nx.ego_graph(graph, node, radius=radius)\n",
        "\n",
        "            # Convert the subgraph to PyTorch Geometric format\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "\n",
        "            # Add mapping of subgraph nodes to original graph nodes\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes))  # Map subgraph nodes\n",
        "\n",
        "            # Add central node feature\n",
        "            central_node_feature = torch.zeros(len(subgraph.nodes), 1)\n",
        "            central_node_idx = list(subgraph.nodes).index(node)  # Index of the central node\n",
        "            central_node_feature[central_node_idx] = 1\n",
        "\n",
        "            # Combine central node feature with original features\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[pyg_subgraph.n_id]  # Use original features from the graph\n",
        "            pyg_subgraph.x = torch.cat([pyg_subgraph.x, central_node_feature], dim=1)  # Add central node feature\n",
        "\n",
        "            # Normalize the features (optional)\n",
        "            pyg_subgraph.x = F.normalize(pyg_subgraph.x, p=2, dim=1)\n",
        "\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "\n",
        "    return subgraphs\n",
        "\n",
        "\n",
        "# --- Modified compute_metrics for ESAN ---\n",
        "def compute_metrics(model, subgraphs, data, poisoned_nodes, target_labels):\n",
        "    \"\"\"\n",
        "    Computes the Attack Success Rate (ASR) and clean test accuracy for the ESAN model.\n",
        "\n",
        "    Parameters:\n",
        "      model: The ESAN model.\n",
        "      subgraphs: A list of subgraphs generated (e.g., by generate_subgraphs) from the current data.\n",
        "      data: The (possibly poisoned) PyG data object.\n",
        "      poisoned_nodes: Indices of the nodes that were targeted by the attack.\n",
        "      target_labels: The target labels (as a tensor) for the poisoned nodes.\n",
        "\n",
        "    Returns:\n",
        "      asr: Attack success rate (%)\n",
        "      clean_acc: Clean test accuracy (%)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Call the ESAN forward method with the subgraphs and total number of nodes.\n",
        "        out = model(subgraphs, data.num_nodes)\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "        # ASR: percentage of poisoned nodes that are classified as the target label.\n",
        "        asr = (pred[poisoned_nodes] == target_labels).sum().item() / len(poisoned_nodes) * 100\n",
        "        # Clean accuracy: standard accuracy on nodes marked as test nodes.\n",
        "        clean_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu()) * 100\n",
        "\n",
        "    return asr, clean_acc\n",
        "\n",
        "\n",
        "\n",
        "def run_attacks_on_esan():\n",
        "    datasets = [\"Cora\", \"CiteSeer\", \"PubMed\"]\n",
        "    policies = [\"ego\", \"edge_deleted\", \"node_deleted\"]  # ESAN-specific policies\n",
        "    dataset_budgets = {'Cora': 10, 'CiteSeer': 20, 'PubMed': 30}  # Poisoning budgets\n",
        "    results_summary = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        try:\n",
        "            print(f\"\\nStarting process for dataset: {dataset_name}\")\n",
        "            # Load dataset and split into train/val/test\n",
        "            dataset = load_dataset(dataset_name)\n",
        "            data = dataset[0].to(device)\n",
        "            input_dim = data.num_features\n",
        "            output_dim = dataset.num_classes if isinstance(dataset.num_classes, int) else dataset.num_classes[0]\n",
        "            data = split_dataset(data)\n",
        "            poisoned_node_budget = dataset_budgets.get(dataset_name, 10)\n",
        "\n",
        "            # Loop over ESAN-specific policies\n",
        "            for policy in policies:\n",
        "                try:\n",
        "                    print(f\"\\nGenerating subgraphs using {policy} policy for {dataset_name}.\")\n",
        "                    subgraphs = generate_subgraphs(data, policy=policy, max_subgraphs=100)\n",
        "                    # Adjust input dimension if the policy adds extra features (e.g., ego policy)\n",
        "                    adjusted_input_dim = input_dim + 1 if policy == \"ego\" else input_dim\n",
        "\n",
        "                    # === Baseline Training for ESAN ===\n",
        "                    print(f\"Training baseline ESAN model for dataset {dataset_name} (Policy: {policy}).\")\n",
        "                    model = ESAN(adjusted_input_dim, hidden_dim=64, output_dim=output_dim).to(device)\n",
        "                    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "                    model = train_model(model, subgraphs, data, optimizer, epochs=200)\n",
        "                    baseline_accs = test_model(model, subgraphs, data)\n",
        "                    baseline_test_acc = baseline_accs[2]  # Test accuracy\n",
        "                    print(f\"Dataset: {dataset_name}, Policy: {policy}, Baseline Test Accuracy: {baseline_test_acc * 100:.2f}%\")\n",
        "                    results_summary.append({\n",
        "                        \"Dataset\": dataset_name,\n",
        "                        \"Model\": \"ESAN\",\n",
        "                        \"Policy\": policy,\n",
        "                        \"Attack\": \"None\",\n",
        "                        \"ASR\": \"N/A\",\n",
        "                        \"Clean Accuracy\": baseline_test_acc * 100\n",
        "                    })\n",
        "\n",
        "                    # === Attack Evaluation ===\n",
        "                    attack_methods = ['SBA-Samp', 'SBA-Gen', 'GTA', 'UGBA', 'DPGBA']\n",
        "                    for attack in attack_methods:\n",
        "                        try:\n",
        "                            print(f\"\\nStarting attack {attack} on dataset: {dataset_name} (Policy: {policy})\")\n",
        "                            # Reinitialize a fresh ESAN model and optimizer for the attack\n",
        "                            model = ESAN(adjusted_input_dim, hidden_dim=64, output_dim=output_dim).to(device)\n",
        "                            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "                            # Select poisoned nodes (using high-centrality nodes)\n",
        "                            poisoned_nodes = select_high_centrality_nodes(data, poisoned_node_budget).to(device)\n",
        "\n",
        "                            # For DPGBA, initialize trigger generator and train OOD Detector; else, set them to None\n",
        "                            if attack == \"DPGBA\":\n",
        "                                print(f\"Initializing trigger generator and training OOD Detector for DPGBA on dataset: {dataset_name} (Policy: {policy})\")\n",
        "                                trigger_gen = TriggerGenerator(input_dim=input_dim, hidden_dim=64).to(device)\n",
        "                                ood_detector = OODDetector(input_dim=input_dim, hidden_dim=64, latent_dim=16).to(device)\n",
        "\n",
        "                                ood_optimizer = torch.optim.Adam(ood_detector.parameters(), lr=0.001)\n",
        "                                train_ood_detector(ood_detector, data, ood_optimizer, epochs=100)\n",
        "                            else:\n",
        "                                trigger_gen = None\n",
        "                                ood_detector = None\n",
        "\n",
        "                            data_poisoned, target_labels = inject_trigger(\n",
        "                                data=data,\n",
        "                                poisoned_nodes=poisoned_nodes,\n",
        "                                attack_type=attack,\n",
        "                                model=model,  # Pass the current model here\n",
        "                                trigger_gen=trigger_gen,  # Only for DPGBA\n",
        "                                ood_detector=ood_detector,         # Only for DPGBA\n",
        "                                alpha=0.7,\n",
        "                                trigger_size=10,\n",
        "                                trigger_density=0.5\n",
        "                            )\n",
        "\n",
        "\n",
        "                            # Train the ESAN model on the poisoned data\n",
        "                            model = train_model(model, subgraphs, data_poisoned, optimizer, epochs=200)\n",
        "\n",
        "                            subgraphs_poisoned = generate_subgraphs(data_poisoned, policy=policy, max_subgraphs=100)\n",
        "                            asr, clean_acc = compute_metrics(model, subgraphs_poisoned, data_poisoned, poisoned_nodes, target_labels)\n",
        "\n",
        "                            results_summary.append({\n",
        "                                \"Dataset\": dataset_name,\n",
        "                                \"Model\": \"ESAN\",\n",
        "                                \"Policy\": policy,\n",
        "                                \"Attack\": attack,\n",
        "                                \"ASR\": asr,\n",
        "                                \"Clean Accuracy\": clean_acc\n",
        "                            })\n",
        "                            print(f\"Dataset: {dataset_name}, Policy: {policy}, Attack: {attack} - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error during attack {attack} on {dataset_name} with ESAN (Policy: {policy}): {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during ESAN policy {policy} on {dataset_name}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing dataset {dataset_name}: {e}\")\n",
        "\n",
        "    # Save and display results\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nSummary of Results:\")\n",
        "    print(results_df)\n",
        "    results_df.to_csv(\"esan_attack_results_summary.csv\", index=False)\n",
        "\n",
        "# Run the attack function\n",
        "run_attacks_on_esan()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T09:09:10.203177Z",
          "iopub.execute_input": "2025-02-04T09:09:10.203729Z"
        },
        "id": "50gIj2wkmNuH",
        "outputId": "3539018a-07e6-45d1-aa73-b429bbc3b18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nUsing device: cpu\n\nStarting process for dataset: Cora\n\nGenerating subgraphs using ego policy for Cora.\nTraining baseline ESAN model for dataset Cora (Policy: ego).\nEpoch 10/200, Loss: 1.4901, Train Accuracy: 0.3917\nEpoch 20/200, Loss: 1.1218, Train Accuracy: 0.4955\nEpoch 30/200, Loss: 1.0236, Train Accuracy: 0.5245\nEpoch 40/200, Loss: 0.9774, Train Accuracy: 0.5424\nEpoch 50/200, Loss: 0.9571, Train Accuracy: 0.5530\nEpoch 60/200, Loss: 0.9505, Train Accuracy: 0.5551\nEpoch 70/200, Loss: 0.9458, Train Accuracy: 0.5567\nEpoch 80/200, Loss: 0.9418, Train Accuracy: 0.5572\nEpoch 90/200, Loss: 0.9404, Train Accuracy: 0.5572\nEpoch 100/200, Loss: 0.9396, Train Accuracy: 0.5577\nEpoch 110/200, Loss: 0.9385, Train Accuracy: 0.5577\nEpoch 120/200, Loss: 0.9379, Train Accuracy: 0.5582\nEpoch 130/200, Loss: 0.9379, Train Accuracy: 0.5577\nEpoch 140/200, Loss: 0.9388, Train Accuracy: 0.5567\nEpoch 150/200, Loss: 0.9369, Train Accuracy: 0.5588\nEpoch 160/200, Loss: 0.9368, Train Accuracy: 0.5577\nEpoch 170/200, Loss: 0.9371, Train Accuracy: 0.5582\nEpoch 180/200, Loss: 0.9367, Train Accuracy: 0.5582\nEpoch 190/200, Loss: 0.9364, Train Accuracy: 0.5588\nEpoch 200/200, Loss: 0.9353, Train Accuracy: 0.5588\nTrain Accuracy: 0.5588\nValidation Accuracy: 0.4481\nTest Accuracy: 0.4843\nDataset: Cora, Policy: ego, Baseline Test Accuracy: 48.43%\n\nStarting attack SBA-Samp on dataset: Cora (Policy: ego)\nEpoch 10/200, Loss: 1.5007, Train Accuracy: 0.4164\nEpoch 20/200, Loss: 1.1150, Train Accuracy: 0.4955\nEpoch 30/200, Loss: 1.0207, Train Accuracy: 0.5277\nEpoch 40/200, Loss: 0.9757, Train Accuracy: 0.5472\nEpoch 50/200, Loss: 0.9560, Train Accuracy: 0.5540\nEpoch 60/200, Loss: 0.9485, Train Accuracy: 0.5556\nEpoch 70/200, Loss: 0.9438, Train Accuracy: 0.5561\nEpoch 80/200, Loss: 0.9435, Train Accuracy: 0.5567\nEpoch 90/200, Loss: 0.9412, Train Accuracy: 0.5577\nEpoch 100/200, Loss: 0.9415, Train Accuracy: 0.5582\nEpoch 110/200, Loss: 0.9385, Train Accuracy: 0.5577\nEpoch 120/200, Loss: 0.9378, Train Accuracy: 0.5577\nEpoch 130/200, Loss: 0.9381, Train Accuracy: 0.5577\nEpoch 140/200, Loss: 0.9368, Train Accuracy: 0.5588\nEpoch 150/200, Loss: 0.9369, Train Accuracy: 0.5582\nEpoch 160/200, Loss: 0.9361, Train Accuracy: 0.5588\nEpoch 170/200, Loss: 0.9376, Train Accuracy: 0.5572\nEpoch 180/200, Loss: 0.9364, Train Accuracy: 0.5588\nEpoch 190/200, Loss: 0.9359, Train Accuracy: 0.5582\nEpoch 200/200, Loss: 0.9353, Train Accuracy: 0.5582\nDataset: Cora, Policy: ego, Attack: SBA-Samp - ASR: 0.00%, Clean Accuracy: 48.98%\n\nStarting attack SBA-Gen on dataset: Cora (Policy: ego)\nEpoch 10/200, Loss: 1.5020, Train Accuracy: 0.4059\nEpoch 20/200, Loss: 1.1295, Train Accuracy: 0.4945\nEpoch 30/200, Loss: 1.0342, Train Accuracy: 0.5219\nEpoch 40/200, Loss: 0.9774, Train Accuracy: 0.5430\nEpoch 50/200, Loss: 0.9589, Train Accuracy: 0.5540\nEpoch 60/200, Loss: 0.9501, Train Accuracy: 0.5546\nEpoch 70/200, Loss: 0.9476, Train Accuracy: 0.5540\nEpoch 80/200, Loss: 0.9446, Train Accuracy: 0.5556\nEpoch 90/200, Loss: 0.9441, Train Accuracy: 0.5561\nEpoch 100/200, Loss: 0.9394, Train Accuracy: 0.5577\nEpoch 110/200, Loss: 0.9394, Train Accuracy: 0.5577\nEpoch 120/200, Loss: 0.9395, Train Accuracy: 0.5582\nEpoch 130/200, Loss: 0.9392, Train Accuracy: 0.5572\nEpoch 140/200, Loss: 0.9373, Train Accuracy: 0.5577\nEpoch 150/200, Loss: 0.9370, Train Accuracy: 0.5582\nEpoch 160/200, Loss: 0.9366, Train Accuracy: 0.5588\nEpoch 170/200, Loss: 0.9370, Train Accuracy: 0.5582\nEpoch 180/200, Loss: 0.9356, Train Accuracy: 0.5588\nEpoch 190/200, Loss: 0.9356, Train Accuracy: 0.5588\nEpoch 200/200, Loss: 0.9356, Train Accuracy: 0.5588\nDataset: Cora, Policy: ego, Attack: SBA-Gen - ASR: 0.00%, Clean Accuracy: 48.43%\n\nStarting attack GTA on dataset: Cora (Policy: ego)\nEpoch 10/200, Loss: 1.5262, Train Accuracy: 0.3954\nEpoch 20/200, Loss: 1.1445, Train Accuracy: 0.4918\nEpoch 30/200, Loss: 1.0341, Train Accuracy: 0.5203\nEpoch 40/200, Loss: 0.9875, Train Accuracy: 0.5387\nEpoch 50/200, Loss: 0.9597, Train Accuracy: 0.5540\nEpoch 60/200, Loss: 0.9555, Train Accuracy: 0.5551\nEpoch 70/200, Loss: 0.9474, Train Accuracy: 0.5551\nEpoch 80/200, Loss: 0.9428, Train Accuracy: 0.5561\nEpoch 90/200, Loss: 0.9418, Train Accuracy: 0.5577\nEpoch 100/200, Loss: 0.9412, Train Accuracy: 0.5572\nEpoch 110/200, Loss: 0.9405, Train Accuracy: 0.5572\nEpoch 120/200, Loss: 0.9391, Train Accuracy: 0.5577\nEpoch 130/200, Loss: 0.9377, Train Accuracy: 0.5582\nEpoch 140/200, Loss: 0.9382, Train Accuracy: 0.5582\nEpoch 150/200, Loss: 0.9381, Train Accuracy: 0.5577\nEpoch 160/200, Loss: 0.9370, Train Accuracy: 0.5577\nEpoch 170/200, Loss: 0.9381, Train Accuracy: 0.5577\nEpoch 180/200, Loss: 0.9363, Train Accuracy: 0.5588\nEpoch 190/200, Loss: 0.9374, Train Accuracy: 0.5561\nEpoch 200/200, Loss: 0.9367, Train Accuracy: 0.5582\nDataset: Cora, Policy: ego, Attack: GTA - ASR: 20.00%, Clean Accuracy: 48.98%\n\nStarting attack UGBA on dataset: Cora (Policy: ego)\nEpoch 10/200, Loss: 1.4801, Train Accuracy: 0.4001\nEpoch 20/200, Loss: 1.1163, Train Accuracy: 0.4945\nEpoch 30/200, Loss: 1.0256, Train Accuracy: 0.5240\nEpoch 40/200, Loss: 0.9772, Train Accuracy: 0.5451\nEpoch 50/200, Loss: 0.9578, Train Accuracy: 0.5551\nEpoch 60/200, Loss: 0.9483, Train Accuracy: 0.5556\nEpoch 70/200, Loss: 0.9443, Train Accuracy: 0.5567\nEpoch 80/200, Loss: 0.9416, Train Accuracy: 0.5577\nEpoch 90/200, Loss: 0.9420, Train Accuracy: 0.5567\nEpoch 100/200, Loss: 0.9402, Train Accuracy: 0.5572\nEpoch 110/200, Loss: 0.9399, Train Accuracy: 0.5582\nEpoch 120/200, Loss: 0.9373, Train Accuracy: 0.5588\nEpoch 130/200, Loss: 0.9371, Train Accuracy: 0.5582\nEpoch 140/200, Loss: 0.9375, Train Accuracy: 0.5582\nEpoch 150/200, Loss: 0.9373, Train Accuracy: 0.5582\nEpoch 160/200, Loss: 0.9376, Train Accuracy: 0.5567\nEpoch 170/200, Loss: 0.9366, Train Accuracy: 0.5588\nEpoch 180/200, Loss: 0.9372, Train Accuracy: 0.5582\nEpoch 190/200, Loss: 0.9358, Train Accuracy: 0.5588\nEpoch 200/200, Loss: 0.9352, Train Accuracy: 0.5588\nDataset: Cora, Policy: ego, Attack: UGBA - ASR: 20.00%, Clean Accuracy: 48.61%\n\nStarting attack DPGBA on dataset: Cora (Policy: ego)\nInitializing trigger generator and training OOD Detector for DPGBA on dataset: Cora (Policy: ego)\nEpoch 0, Reconstruction Loss: 0.0127\nEpoch 10, Reconstruction Loss: 0.0124\nEpoch 20, Reconstruction Loss: 0.0121\nEpoch 30, Reconstruction Loss: 0.0120\nEpoch 40, Reconstruction Loss: 0.0118\nEpoch 50, Reconstruction Loss: 0.0117\nEpoch 60, Reconstruction Loss: 0.0116\nEpoch 70, Reconstruction Loss: 0.0115\nEpoch 80, Reconstruction Loss: 0.0113\nEpoch 90, Reconstruction Loss: 0.0112\nEpoch 10/200, Loss: 1.4967, Train Accuracy: 0.4317\nEpoch 20/200, Loss: 1.1164, Train Accuracy: 0.4997\nEpoch 30/200, Loss: 1.0141, Train Accuracy: 0.5293\nEpoch 40/200, Loss: 0.9828, Train Accuracy: 0.5398\nEpoch 50/200, Loss: 0.9553, Train Accuracy: 0.5535\nEpoch 60/200, Loss: 0.9497, Train Accuracy: 0.5546\nEpoch 70/200, Loss: 0.9465, Train Accuracy: 0.5556\nEpoch 80/200, Loss: 0.9445, Train Accuracy: 0.5561\nEpoch 90/200, Loss: 0.9406, Train Accuracy: 0.5567\nEpoch 100/200, Loss: 0.9406, Train Accuracy: 0.5572\nEpoch 110/200, Loss: 0.9386, Train Accuracy: 0.5577\nEpoch 120/200, Loss: 0.9392, Train Accuracy: 0.5577\nEpoch 130/200, Loss: 0.9375, Train Accuracy: 0.5577\nEpoch 140/200, Loss: 0.9367, Train Accuracy: 0.5588\nEpoch 150/200, Loss: 0.9375, Train Accuracy: 0.5582\nEpoch 160/200, Loss: 0.9362, Train Accuracy: 0.5588\nEpoch 170/200, Loss: 0.9362, Train Accuracy: 0.5588\nEpoch 180/200, Loss: 0.9359, Train Accuracy: 0.5588\nEpoch 190/200, Loss: 0.9356, Train Accuracy: 0.5588\nEpoch 200/200, Loss: 0.9370, Train Accuracy: 0.5577\nDataset: Cora, Policy: ego, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 48.80%\n\nGenerating subgraphs using edge_deleted policy for Cora.\nTraining baseline ESAN model for dataset Cora (Policy: edge_deleted).\nEpoch 10/200, Loss: 0.5047, Train Accuracy: 0.8819\nEpoch 20/200, Loss: 0.1763, Train Accuracy: 0.9383\nEpoch 30/200, Loss: 0.0848, Train Accuracy: 0.9715\nEpoch 40/200, Loss: 0.0435, Train Accuracy: 0.9847\nEpoch 50/200, Loss: 0.0270, Train Accuracy: 0.9931\nEpoch 60/200, Loss: 0.0177, Train Accuracy: 0.9937\nEpoch 70/200, Loss: 0.0135, Train Accuracy: 0.9968\nEpoch 80/200, Loss: 0.0110, Train Accuracy: 0.9974\nEpoch 90/200, Loss: 0.0086, Train Accuracy: 0.9974\nEpoch 100/200, Loss: 0.0081, Train Accuracy: 0.9979\nEpoch 110/200, Loss: 0.0092, Train Accuracy: 0.9968\nEpoch 120/200, Loss: 0.0059, Train Accuracy: 0.9984\nEpoch 130/200, Loss: 0.0061, Train Accuracy: 0.9979\nEpoch 140/200, Loss: 0.0051, Train Accuracy: 0.9979\nEpoch 150/200, Loss: 0.0047, Train Accuracy: 0.9984\nEpoch 160/200, Loss: 0.0044, Train Accuracy: 0.9984\nEpoch 170/200, Loss: 0.0055, Train Accuracy: 0.9984\nEpoch 180/200, Loss: 0.0076, Train Accuracy: 0.9979\nEpoch 190/200, Loss: 0.0041, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0044, Train Accuracy: 0.9984\nTrain Accuracy: 0.9984\nValidation Accuracy: 0.8481\nTest Accuracy: 0.8669\nDataset: Cora, Policy: edge_deleted, Baseline Test Accuracy: 86.69%\n\nStarting attack SBA-Samp on dataset: Cora (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.3945, Train Accuracy: 0.8877\nEpoch 20/200, Loss: 0.1538, Train Accuracy: 0.9483\nEpoch 30/200, Loss: 0.0721, Train Accuracy: 0.9763\nEpoch 40/200, Loss: 0.0379, Train Accuracy: 0.9868\nEpoch 50/200, Loss: 0.0231, Train Accuracy: 0.9931\nEpoch 60/200, Loss: 0.0158, Train Accuracy: 0.9963\nEpoch 70/200, Loss: 0.0125, Train Accuracy: 0.9968\nEpoch 80/200, Loss: 0.0111, Train Accuracy: 0.9968\nEpoch 90/200, Loss: 0.0105, Train Accuracy: 0.9974\nEpoch 100/200, Loss: 0.0094, Train Accuracy: 0.9968\nEpoch 110/200, Loss: 0.0088, Train Accuracy: 0.9974\nEpoch 120/200, Loss: 0.0088, Train Accuracy: 0.9979\nEpoch 130/200, Loss: 0.0055, Train Accuracy: 0.9979\nEpoch 140/200, Loss: 0.0050, Train Accuracy: 0.9984\nEpoch 150/200, Loss: 0.0068, Train Accuracy: 0.9974\nEpoch 160/200, Loss: 0.0051, Train Accuracy: 0.9979\nEpoch 170/200, Loss: 0.0045, Train Accuracy: 0.9984\nEpoch 180/200, Loss: 0.0043, Train Accuracy: 0.9979\nEpoch 190/200, Loss: 0.0035, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0040, Train Accuracy: 0.9984\nDataset: Cora, Policy: edge_deleted, Attack: SBA-Samp - ASR: 10.00%, Clean Accuracy: 86.88%\n\nStarting attack SBA-Gen on dataset: Cora (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.3978, Train Accuracy: 0.8935\nEpoch 20/200, Loss: 0.1594, Train Accuracy: 0.9410\nEpoch 30/200, Loss: 0.0725, Train Accuracy: 0.9752\nEpoch 40/200, Loss: 0.0374, Train Accuracy: 0.9868\nEpoch 50/200, Loss: 0.0237, Train Accuracy: 0.9921\nEpoch 60/200, Loss: 0.0168, Train Accuracy: 0.9958\nEpoch 70/200, Loss: 0.0121, Train Accuracy: 0.9963\nEpoch 80/200, Loss: 0.0103, Train Accuracy: 0.9968\nEpoch 90/200, Loss: 0.0088, Train Accuracy: 0.9974\nEpoch 100/200, Loss: 0.0079, Train Accuracy: 0.9979\nEpoch 110/200, Loss: 0.0099, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0107, Train Accuracy: 0.9963\nEpoch 130/200, Loss: 0.0075, Train Accuracy: 0.9974\nEpoch 140/200, Loss: 0.0084, Train Accuracy: 0.9974\nEpoch 150/200, Loss: 0.0056, Train Accuracy: 0.9979\nEpoch 160/200, Loss: 0.0049, Train Accuracy: 0.9984\nEpoch 170/200, Loss: 0.0065, Train Accuracy: 0.9974\nEpoch 180/200, Loss: 0.0038, Train Accuracy: 0.9984\nEpoch 190/200, Loss: 0.0039, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0067, Train Accuracy: 0.9984\nDataset: Cora, Policy: edge_deleted, Attack: SBA-Gen - ASR: 10.00%, Clean Accuracy: 86.69%\n\nStarting attack GTA on dataset: Cora (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.4476, Train Accuracy: 0.8724\nEpoch 20/200, Loss: 0.1845, Train Accuracy: 0.9373\nEpoch 30/200, Loss: 0.0871, Train Accuracy: 0.9689\nEpoch 40/200, Loss: 0.0473, Train Accuracy: 0.9863\nEpoch 50/200, Loss: 0.0292, Train Accuracy: 0.9916\nEpoch 60/200, Loss: 0.0203, Train Accuracy: 0.9958\nEpoch 70/200, Loss: 0.0157, Train Accuracy: 0.9963\nEpoch 80/200, Loss: 0.0125, Train Accuracy: 0.9968\nEpoch 90/200, Loss: 0.0109, Train Accuracy: 0.9979\nEpoch 100/200, Loss: 0.0102, Train Accuracy: 0.9979\nEpoch 110/200, Loss: 0.0078, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0062, Train Accuracy: 0.9984\nEpoch 130/200, Loss: 0.0069, Train Accuracy: 0.9974\nEpoch 140/200, Loss: 0.0056, Train Accuracy: 0.9984\nEpoch 150/200, Loss: 0.0061, Train Accuracy: 0.9979\nEpoch 160/200, Loss: 0.0050, Train Accuracy: 0.9984\nEpoch 170/200, Loss: 0.0052, Train Accuracy: 0.9984\nEpoch 180/200, Loss: 0.0046, Train Accuracy: 0.9984\nEpoch 190/200, Loss: 0.0039, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0045, Train Accuracy: 0.9979\nDataset: Cora, Policy: edge_deleted, Attack: GTA - ASR: 10.00%, Clean Accuracy: 86.51%\n\nStarting attack UGBA on dataset: Cora (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.3788, Train Accuracy: 0.9004\nEpoch 20/200, Loss: 0.1513, Train Accuracy: 0.9462\nEpoch 30/200, Loss: 0.0664, Train Accuracy: 0.9742\nEpoch 40/200, Loss: 0.0348, Train Accuracy: 0.9900\nEpoch 50/200, Loss: 0.0215, Train Accuracy: 0.9931\nEpoch 60/200, Loss: 0.0156, Train Accuracy: 0.9968\nEpoch 70/200, Loss: 0.0126, Train Accuracy: 0.9968\nEpoch 80/200, Loss: 0.0104, Train Accuracy: 0.9974\nEpoch 90/200, Loss: 0.0086, Train Accuracy: 0.9968\nEpoch 100/200, Loss: 0.0071, Train Accuracy: 0.9979\nEpoch 110/200, Loss: 0.0084, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0071, Train Accuracy: 0.9984\nEpoch 130/200, Loss: 0.0052, Train Accuracy: 0.9984\nEpoch 140/200, Loss: 0.0054, Train Accuracy: 0.9984\nEpoch 150/200, Loss: 0.0056, Train Accuracy: 0.9974\nEpoch 160/200, Loss: 0.0068, Train Accuracy: 0.9979\nEpoch 170/200, Loss: 0.0052, Train Accuracy: 0.9979\nEpoch 180/200, Loss: 0.0068, Train Accuracy: 0.9974\nEpoch 190/200, Loss: 0.0046, Train Accuracy: 0.9974\nEpoch 200/200, Loss: 0.0040, Train Accuracy: 0.9984\nDataset: Cora, Policy: edge_deleted, Attack: UGBA - ASR: 0.00%, Clean Accuracy: 78.19%\n\nStarting attack DPGBA on dataset: Cora (Policy: edge_deleted)\nInitializing trigger generator and training OOD Detector for DPGBA on dataset: Cora (Policy: edge_deleted)\nEpoch 0, Reconstruction Loss: 0.0128\nEpoch 10, Reconstruction Loss: 0.0124\nEpoch 20, Reconstruction Loss: 0.0121\nEpoch 30, Reconstruction Loss: 0.0120\nEpoch 40, Reconstruction Loss: 0.0119\nEpoch 50, Reconstruction Loss: 0.0117\nEpoch 60, Reconstruction Loss: 0.0116\nEpoch 70, Reconstruction Loss: 0.0115\nEpoch 80, Reconstruction Loss: 0.0114\nEpoch 90, Reconstruction Loss: 0.0113\nEpoch 10/200, Loss: 0.4696, Train Accuracy: 0.8714\nEpoch 20/200, Loss: 0.1706, Train Accuracy: 0.9415\nEpoch 30/200, Loss: 0.0802, Train Accuracy: 0.9721\nEpoch 40/200, Loss: 0.0406, Train Accuracy: 0.9873\nEpoch 50/200, Loss: 0.0269, Train Accuracy: 0.9910\nEpoch 60/200, Loss: 0.0199, Train Accuracy: 0.9937\nEpoch 70/200, Loss: 0.0126, Train Accuracy: 0.9968\nEpoch 80/200, Loss: 0.0110, Train Accuracy: 0.9974\nEpoch 90/200, Loss: 0.0105, Train Accuracy: 0.9974\nEpoch 100/200, Loss: 0.0083, Train Accuracy: 0.9974\nEpoch 110/200, Loss: 0.0065, Train Accuracy: 0.9974\nEpoch 120/200, Loss: 0.0091, Train Accuracy: 0.9974\nEpoch 130/200, Loss: 0.0056, Train Accuracy: 0.9979\nEpoch 140/200, Loss: 0.0062, Train Accuracy: 0.9979\nEpoch 150/200, Loss: 0.0062, Train Accuracy: 0.9974\nEpoch 160/200, Loss: 0.0046, Train Accuracy: 0.9984\nEpoch 170/200, Loss: 0.0041, Train Accuracy: 0.9984\nEpoch 180/200, Loss: 0.0040, Train Accuracy: 0.9984\nEpoch 190/200, Loss: 0.0045, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0044, Train Accuracy: 0.9984\nDataset: Cora, Policy: edge_deleted, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 86.69%\n\nGenerating subgraphs using node_deleted policy for Cora.\nTraining baseline ESAN model for dataset Cora (Policy: node_deleted).\nEpoch 10/200, Loss: 0.4594, Train Accuracy: 0.8809\nEpoch 20/200, Loss: 0.1513, Train Accuracy: 0.9457\nEpoch 30/200, Loss: 0.0672, Train Accuracy: 0.9752\nEpoch 40/200, Loss: 0.0366, Train Accuracy: 0.9895\nEpoch 50/200, Loss: 0.0225, Train Accuracy: 0.9921\nEpoch 60/200, Loss: 0.0161, Train Accuracy: 0.9958\nEpoch 70/200, Loss: 0.0134, Train Accuracy: 0.9953\nEpoch 80/200, Loss: 0.0122, Train Accuracy: 0.9974\nEpoch 90/200, Loss: 0.0097, Train Accuracy: 0.9979\nEpoch 100/200, Loss: 0.0077, Train Accuracy: 0.9979\nEpoch 110/200, Loss: 0.0075, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0067, Train Accuracy: 0.9984\nEpoch 130/200, Loss: 0.0067, Train Accuracy: 0.9974\nEpoch 140/200, Loss: 0.0069, Train Accuracy: 0.9974\nEpoch 150/200, Loss: 0.0065, Train Accuracy: 0.9979\nEpoch 160/200, Loss: 0.0043, Train Accuracy: 0.9984\nEpoch 170/200, Loss: 0.0051, Train Accuracy: 0.9984\nEpoch 180/200, Loss: 0.0055, Train Accuracy: 0.9984\nEpoch 190/200, Loss: 0.0045, Train Accuracy: 0.9979\nEpoch 200/200, Loss: 0.0050, Train Accuracy: 0.9984\nTrain Accuracy: 0.9984\nValidation Accuracy: 0.8519\nTest Accuracy: 0.8614\nDataset: Cora, Policy: node_deleted, Baseline Test Accuracy: 86.14%\n\nStarting attack SBA-Samp on dataset: Cora (Policy: node_deleted)\nEpoch 10/200, Loss: 0.3791, Train Accuracy: 0.8998\nEpoch 20/200, Loss: 0.1579, Train Accuracy: 0.9420\nEpoch 30/200, Loss: 0.0687, Train Accuracy: 0.9768\nEpoch 40/200, Loss: 0.0381, Train Accuracy: 0.9884\nEpoch 50/200, Loss: 0.0230, Train Accuracy: 0.9947\nEpoch 60/200, Loss: 0.0167, Train Accuracy: 0.9963\nEpoch 70/200, Loss: 0.0119, Train Accuracy: 0.9968\nEpoch 80/200, Loss: 0.0106, Train Accuracy: 0.9968\nEpoch 90/200, Loss: 0.0094, Train Accuracy: 0.9963\nEpoch 100/200, Loss: 0.0102, Train Accuracy: 0.9968\nEpoch 110/200, Loss: 0.0063, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0058, Train Accuracy: 0.9979\nEpoch 130/200, Loss: 0.0055, Train Accuracy: 0.9979\nEpoch 140/200, Loss: 0.0053, Train Accuracy: 0.9979\nEpoch 150/200, Loss: 0.0045, Train Accuracy: 0.9979\nEpoch 160/200, Loss: 0.0053, Train Accuracy: 0.9979\nEpoch 170/200, Loss: 0.0039, Train Accuracy: 0.9979\nEpoch 180/200, Loss: 0.0041, Train Accuracy: 0.9984\nEpoch 190/200, Loss: 0.0038, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0054, Train Accuracy: 0.9984\nDataset: Cora, Policy: node_deleted, Attack: SBA-Samp - ASR: 0.00%, Clean Accuracy: 86.51%\n\nStarting attack SBA-Gen on dataset: Cora (Policy: node_deleted)\nEpoch 10/200, Loss: 0.4668, Train Accuracy: 0.8856\nEpoch 20/200, Loss: 0.1853, Train Accuracy: 0.9315\nEpoch 30/200, Loss: 0.0867, Train Accuracy: 0.9689\nEpoch 40/200, Loss: 0.0479, Train Accuracy: 0.9852\nEpoch 50/200, Loss: 0.0290, Train Accuracy: 0.9900\nEpoch 60/200, Loss: 0.0207, Train Accuracy: 0.9926\nEpoch 70/200, Loss: 0.0149, Train Accuracy: 0.9953\nEpoch 80/200, Loss: 0.0125, Train Accuracy: 0.9963\nEpoch 90/200, Loss: 0.0097, Train Accuracy: 0.9979\nEpoch 100/200, Loss: 0.0094, Train Accuracy: 0.9968\nEpoch 110/200, Loss: 0.0083, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0076, Train Accuracy: 0.9968\nEpoch 130/200, Loss: 0.0068, Train Accuracy: 0.9979\nEpoch 140/200, Loss: 0.0065, Train Accuracy: 0.9974\nEpoch 150/200, Loss: 0.0048, Train Accuracy: 0.9984\nEpoch 160/200, Loss: 0.0060, Train Accuracy: 0.9974\nEpoch 170/200, Loss: 0.0049, Train Accuracy: 0.9979\nEpoch 180/200, Loss: 0.0045, Train Accuracy: 0.9984\nEpoch 190/200, Loss: 0.0048, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0051, Train Accuracy: 0.9984\nDataset: Cora, Policy: node_deleted, Attack: SBA-Gen - ASR: 0.00%, Clean Accuracy: 87.06%\n\nStarting attack GTA on dataset: Cora (Policy: node_deleted)\nEpoch 10/200, Loss: 0.4804, Train Accuracy: 0.8619\nEpoch 20/200, Loss: 0.1782, Train Accuracy: 0.9352\nEpoch 30/200, Loss: 0.0852, Train Accuracy: 0.9710\nEpoch 40/200, Loss: 0.0449, Train Accuracy: 0.9863\nEpoch 50/200, Loss: 0.0271, Train Accuracy: 0.9921\nEpoch 60/200, Loss: 0.0199, Train Accuracy: 0.9953\nEpoch 70/200, Loss: 0.0161, Train Accuracy: 0.9958\nEpoch 80/200, Loss: 0.0115, Train Accuracy: 0.9968\nEpoch 90/200, Loss: 0.0096, Train Accuracy: 0.9974\nEpoch 100/200, Loss: 0.0092, Train Accuracy: 0.9963\nEpoch 110/200, Loss: 0.0076, Train Accuracy: 0.9974\nEpoch 120/200, Loss: 0.0079, Train Accuracy: 0.9979\nEpoch 130/200, Loss: 0.0066, Train Accuracy: 0.9979\nEpoch 140/200, Loss: 0.0049, Train Accuracy: 0.9984\nEpoch 150/200, Loss: 0.0060, Train Accuracy: 0.9979\nEpoch 160/200, Loss: 0.0055, Train Accuracy: 0.9984\nEpoch 170/200, Loss: 0.0051, Train Accuracy: 0.9984\nEpoch 180/200, Loss: 0.0039, Train Accuracy: 0.9979\nEpoch 190/200, Loss: 0.0039, Train Accuracy: 0.9979\nEpoch 200/200, Loss: 0.0044, Train Accuracy: 0.9984\nDataset: Cora, Policy: node_deleted, Attack: GTA - ASR: 10.00%, Clean Accuracy: 87.06%\n\nStarting attack UGBA on dataset: Cora (Policy: node_deleted)\nEpoch 10/200, Loss: 0.4813, Train Accuracy: 0.8682\nEpoch 20/200, Loss: 0.1795, Train Accuracy: 0.9357\nEpoch 30/200, Loss: 0.0832, Train Accuracy: 0.9715\nEpoch 40/200, Loss: 0.0451, Train Accuracy: 0.9852\nEpoch 50/200, Loss: 0.0277, Train Accuracy: 0.9910\nEpoch 60/200, Loss: 0.0187, Train Accuracy: 0.9947\nEpoch 70/200, Loss: 0.0137, Train Accuracy: 0.9968\nEpoch 80/200, Loss: 0.0118, Train Accuracy: 0.9963\nEpoch 90/200, Loss: 0.0087, Train Accuracy: 0.9979\nEpoch 100/200, Loss: 0.0083, Train Accuracy: 0.9979\nEpoch 110/200, Loss: 0.0063, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0069, Train Accuracy: 0.9979\nEpoch 130/200, Loss: 0.0053, Train Accuracy: 0.9984\nEpoch 140/200, Loss: 0.0058, Train Accuracy: 0.9974\nEpoch 150/200, Loss: 0.0045, Train Accuracy: 0.9979\nEpoch 160/200, Loss: 0.0042, Train Accuracy: 0.9984\nEpoch 170/200, Loss: 0.0045, Train Accuracy: 0.9984\nEpoch 180/200, Loss: 0.0043, Train Accuracy: 0.9979\nEpoch 190/200, Loss: 0.0039, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0043, Train Accuracy: 0.9984\nDataset: Cora, Policy: node_deleted, Attack: UGBA - ASR: 0.00%, Clean Accuracy: 80.22%\n\nStarting attack DPGBA on dataset: Cora (Policy: node_deleted)\nInitializing trigger generator and training OOD Detector for DPGBA on dataset: Cora (Policy: node_deleted)\nEpoch 0, Reconstruction Loss: 0.0128\nEpoch 10, Reconstruction Loss: 0.0124\nEpoch 20, Reconstruction Loss: 0.0122\nEpoch 30, Reconstruction Loss: 0.0120\nEpoch 40, Reconstruction Loss: 0.0119\nEpoch 50, Reconstruction Loss: 0.0118\nEpoch 60, Reconstruction Loss: 0.0117\nEpoch 70, Reconstruction Loss: 0.0116\nEpoch 80, Reconstruction Loss: 0.0114\nEpoch 90, Reconstruction Loss: 0.0113\nEpoch 10/200, Loss: 0.4226, Train Accuracy: 0.8840\nEpoch 20/200, Loss: 0.1519, Train Accuracy: 0.9410\nEpoch 30/200, Loss: 0.0702, Train Accuracy: 0.9763\nEpoch 40/200, Loss: 0.0361, Train Accuracy: 0.9895\nEpoch 50/200, Loss: 0.0233, Train Accuracy: 0.9931\nEpoch 60/200, Loss: 0.0149, Train Accuracy: 0.9958\nEpoch 70/200, Loss: 0.0125, Train Accuracy: 0.9968\nEpoch 80/200, Loss: 0.0109, Train Accuracy: 0.9974\nEpoch 90/200, Loss: 0.0108, Train Accuracy: 0.9968\nEpoch 100/200, Loss: 0.0077, Train Accuracy: 0.9979\nEpoch 110/200, Loss: 0.0074, Train Accuracy: 0.9979\nEpoch 120/200, Loss: 0.0064, Train Accuracy: 0.9979\nEpoch 130/200, Loss: 0.0059, Train Accuracy: 0.9984\nEpoch 140/200, Loss: 0.0059, Train Accuracy: 0.9979\nEpoch 150/200, Loss: 0.0058, Train Accuracy: 0.9979\nEpoch 160/200, Loss: 0.0059, Train Accuracy: 0.9979\nEpoch 170/200, Loss: 0.0061, Train Accuracy: 0.9979\nEpoch 180/200, Loss: 0.0050, Train Accuracy: 0.9979\nEpoch 190/200, Loss: 0.0049, Train Accuracy: 0.9984\nEpoch 200/200, Loss: 0.0050, Train Accuracy: 0.9984\nDataset: Cora, Policy: node_deleted, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 86.51%\n\nStarting process for dataset: CiteSeer\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nGenerating subgraphs using ego policy for CiteSeer.\nTraining baseline ESAN model for dataset CiteSeer (Policy: ego).\nEpoch 10/200, Loss: 1.4658, Train Accuracy: 0.3155\nEpoch 20/200, Loss: 1.3526, Train Accuracy: 0.3270\nEpoch 30/200, Loss: 1.3000, Train Accuracy: 0.3489\nEpoch 40/200, Loss: 1.2818, Train Accuracy: 0.3528\nEpoch 50/200, Loss: 1.2743, Train Accuracy: 0.3588\nEpoch 60/200, Loss: 1.2702, Train Accuracy: 0.3597\nEpoch 70/200, Loss: 1.2670, Train Accuracy: 0.3614\nEpoch 80/200, Loss: 1.2594, Train Accuracy: 0.3635\nEpoch 90/200, Loss: 1.2593, Train Accuracy: 0.3644\nEpoch 100/200, Loss: 1.2557, Train Accuracy: 0.3648\nEpoch 110/200, Loss: 1.2559, Train Accuracy: 0.3631\nEpoch 120/200, Loss: 1.2532, Train Accuracy: 0.3644\nEpoch 130/200, Loss: 1.2543, Train Accuracy: 0.3644\nEpoch 140/200, Loss: 1.2533, Train Accuracy: 0.3652\nEpoch 150/200, Loss: 1.2547, Train Accuracy: 0.3652\nEpoch 160/200, Loss: 1.2526, Train Accuracy: 0.3657\nEpoch 170/200, Loss: 1.2538, Train Accuracy: 0.3652\nEpoch 180/200, Loss: 1.2531, Train Accuracy: 0.3657\nEpoch 190/200, Loss: 1.2518, Train Accuracy: 0.3657\nEpoch 200/200, Loss: 1.2523, Train Accuracy: 0.3657\nTrain Accuracy: 0.3665\nValidation Accuracy: 0.3313\nTest Accuracy: 0.2541\nDataset: CiteSeer, Policy: ego, Baseline Test Accuracy: 25.41%\n\nStarting attack SBA-Samp on dataset: CiteSeer (Policy: ego)\nEpoch 10/200, Loss: 1.4722, Train Accuracy: 0.3129\nEpoch 20/200, Loss: 1.3596, Train Accuracy: 0.3288\nEpoch 30/200, Loss: 1.3081, Train Accuracy: 0.3442\nEpoch 40/200, Loss: 1.2819, Train Accuracy: 0.3541\nEpoch 50/200, Loss: 1.2716, Train Accuracy: 0.3575\nEpoch 60/200, Loss: 1.2685, Train Accuracy: 0.3592\nEpoch 70/200, Loss: 1.2628, Train Accuracy: 0.3622\nEpoch 80/200, Loss: 1.2620, Train Accuracy: 0.3627\nEpoch 90/200, Loss: 1.2617, Train Accuracy: 0.3614\nEpoch 100/200, Loss: 1.2602, Train Accuracy: 0.3631\nEpoch 110/200, Loss: 1.2538, Train Accuracy: 0.3657\nEpoch 120/200, Loss: 1.2550, Train Accuracy: 0.3657\nEpoch 130/200, Loss: 1.2558, Train Accuracy: 0.3635\nEpoch 140/200, Loss: 1.2539, Train Accuracy: 0.3639\nEpoch 150/200, Loss: 1.2547, Train Accuracy: 0.3644\nEpoch 160/200, Loss: 1.2535, Train Accuracy: 0.3652\nEpoch 170/200, Loss: 1.2528, Train Accuracy: 0.3648\nEpoch 180/200, Loss: 1.2518, Train Accuracy: 0.3657\nEpoch 190/200, Loss: 1.2520, Train Accuracy: 0.3652\nEpoch 200/200, Loss: 1.2516, Train Accuracy: 0.3648\nDataset: CiteSeer, Policy: ego, Attack: SBA-Samp - ASR: 45.00%, Clean Accuracy: 25.11%\n\nStarting attack SBA-Gen on dataset: CiteSeer (Policy: ego)\nEpoch 10/200, Loss: 1.4926, Train Accuracy: 0.3146\nEpoch 20/200, Loss: 1.3615, Train Accuracy: 0.3253\nEpoch 30/200, Loss: 1.3096, Train Accuracy: 0.3429\nEpoch 40/200, Loss: 1.2904, Train Accuracy: 0.3502\nEpoch 50/200, Loss: 1.2780, Train Accuracy: 0.3562\nEpoch 60/200, Loss: 1.2711, Train Accuracy: 0.3571\nEpoch 70/200, Loss: 1.2680, Train Accuracy: 0.3592\nEpoch 80/200, Loss: 1.2624, Train Accuracy: 0.3627\nEpoch 90/200, Loss: 1.2609, Train Accuracy: 0.3631\nEpoch 100/200, Loss: 1.2594, Train Accuracy: 0.3635\nEpoch 110/200, Loss: 1.2564, Train Accuracy: 0.3635\nEpoch 120/200, Loss: 1.2567, Train Accuracy: 0.3635\nEpoch 130/200, Loss: 1.2553, Train Accuracy: 0.3652\nEpoch 140/200, Loss: 1.2539, Train Accuracy: 0.3644\nEpoch 150/200, Loss: 1.2554, Train Accuracy: 0.3631\nEpoch 160/200, Loss: 1.2550, Train Accuracy: 0.3648\nEpoch 170/200, Loss: 1.2538, Train Accuracy: 0.3648\nEpoch 180/200, Loss: 1.2515, Train Accuracy: 0.3657\nEpoch 190/200, Loss: 1.2519, Train Accuracy: 0.3665\nEpoch 200/200, Loss: 1.2513, Train Accuracy: 0.3665\nDataset: CiteSeer, Policy: ego, Attack: SBA-Gen - ASR: 45.00%, Clean Accuracy: 25.11%\n\nStarting attack GTA on dataset: CiteSeer (Policy: ego)\nEpoch 10/200, Loss: 1.4896, Train Accuracy: 0.3052\nEpoch 20/200, Loss: 1.3545, Train Accuracy: 0.3292\nEpoch 30/200, Loss: 1.3081, Train Accuracy: 0.3421\nEpoch 40/200, Loss: 1.2870, Train Accuracy: 0.3498\nEpoch 50/200, Loss: 1.2752, Train Accuracy: 0.3554\nEpoch 60/200, Loss: 1.2677, Train Accuracy: 0.3609\nEpoch 70/200, Loss: 1.2640, Train Accuracy: 0.3588\nEpoch 80/200, Loss: 1.2616, Train Accuracy: 0.3627\nEpoch 90/200, Loss: 1.2594, Train Accuracy: 0.3635\nEpoch 100/200, Loss: 1.2585, Train Accuracy: 0.3631\nEpoch 110/200, Loss: 1.2556, Train Accuracy: 0.3648\nEpoch 120/200, Loss: 1.2563, Train Accuracy: 0.3631\nEpoch 130/200, Loss: 1.2551, Train Accuracy: 0.3648\nEpoch 140/200, Loss: 1.2545, Train Accuracy: 0.3652\nEpoch 150/200, Loss: 1.2530, Train Accuracy: 0.3652\nEpoch 160/200, Loss: 1.2540, Train Accuracy: 0.3648\nEpoch 170/200, Loss: 1.2535, Train Accuracy: 0.3644\nEpoch 180/200, Loss: 1.2518, Train Accuracy: 0.3661\nEpoch 190/200, Loss: 1.2538, Train Accuracy: 0.3648\nEpoch 200/200, Loss: 1.2527, Train Accuracy: 0.3652\nDataset: CiteSeer, Policy: ego, Attack: GTA - ASR: 0.00%, Clean Accuracy: 25.56%\n\nStarting attack UGBA on dataset: CiteSeer (Policy: ego)\nEpoch 10/200, Loss: 1.4555, Train Accuracy: 0.3180\nEpoch 20/200, Loss: 1.3483, Train Accuracy: 0.3356\nEpoch 30/200, Loss: 1.3094, Train Accuracy: 0.3416\nEpoch 40/200, Loss: 1.2858, Train Accuracy: 0.3554\nEpoch 50/200, Loss: 1.2746, Train Accuracy: 0.3571\nEpoch 60/200, Loss: 1.2685, Train Accuracy: 0.3592\nEpoch 70/200, Loss: 1.2637, Train Accuracy: 0.3605\nEpoch 80/200, Loss: 1.2626, Train Accuracy: 0.3601\nEpoch 90/200, Loss: 1.2597, Train Accuracy: 0.3622\nEpoch 100/200, Loss: 1.2590, Train Accuracy: 0.3648\nEpoch 110/200, Loss: 1.2583, Train Accuracy: 0.3631\nEpoch 120/200, Loss: 1.2558, Train Accuracy: 0.3648\nEpoch 130/200, Loss: 1.2555, Train Accuracy: 0.3644\nEpoch 140/200, Loss: 1.2542, Train Accuracy: 0.3639\nEpoch 150/200, Loss: 1.2566, Train Accuracy: 0.3631\nEpoch 160/200, Loss: 1.2529, Train Accuracy: 0.3657\nEpoch 170/200, Loss: 1.2535, Train Accuracy: 0.3652\nEpoch 180/200, Loss: 1.2514, Train Accuracy: 0.3657\nEpoch 190/200, Loss: 1.2528, Train Accuracy: 0.3652\nEpoch 200/200, Loss: 1.2512, Train Accuracy: 0.3657\nDataset: CiteSeer, Policy: ego, Attack: UGBA - ASR: 0.00%, Clean Accuracy: 25.26%\n\nStarting attack DPGBA on dataset: CiteSeer (Policy: ego)\nInitializing trigger generator and training OOD Detector for DPGBA on dataset: CiteSeer (Policy: ego)\nEpoch 0, Reconstruction Loss: 0.0086\nEpoch 10, Reconstruction Loss: 0.0083\nEpoch 20, Reconstruction Loss: 0.0082\nEpoch 30, Reconstruction Loss: 0.0081\nEpoch 40, Reconstruction Loss: 0.0081\nEpoch 50, Reconstruction Loss: 0.0080\nEpoch 60, Reconstruction Loss: 0.0079\nEpoch 70, Reconstruction Loss: 0.0078\nEpoch 80, Reconstruction Loss: 0.0077\nEpoch 90, Reconstruction Loss: 0.0076\nEpoch 10/200, Loss: 1.4816, Train Accuracy: 0.3047\nEpoch 20/200, Loss: 1.3623, Train Accuracy: 0.3292\nEpoch 30/200, Loss: 1.3028, Train Accuracy: 0.3468\nEpoch 40/200, Loss: 1.2888, Train Accuracy: 0.3502\nEpoch 50/200, Loss: 1.2784, Train Accuracy: 0.3567\nEpoch 60/200, Loss: 1.2718, Train Accuracy: 0.3579\nEpoch 70/200, Loss: 1.2674, Train Accuracy: 0.3605\nEpoch 80/200, Loss: 1.2628, Train Accuracy: 0.3622\nEpoch 90/200, Loss: 1.2609, Train Accuracy: 0.3631\nEpoch 100/200, Loss: 1.2598, Train Accuracy: 0.3639\nEpoch 110/200, Loss: 1.2598, Train Accuracy: 0.3622\nEpoch 120/200, Loss: 1.2562, Train Accuracy: 0.3652\nEpoch 130/200, Loss: 1.2552, Train Accuracy: 0.3648\nEpoch 140/200, Loss: 1.2534, Train Accuracy: 0.3652\nEpoch 150/200, Loss: 1.2535, Train Accuracy: 0.3657\nEpoch 160/200, Loss: 1.2522, Train Accuracy: 0.3657\nEpoch 170/200, Loss: 1.2535, Train Accuracy: 0.3648\nEpoch 180/200, Loss: 1.2523, Train Accuracy: 0.3652\nEpoch 190/200, Loss: 1.2561, Train Accuracy: 0.3631\nEpoch 200/200, Loss: 1.2524, Train Accuracy: 0.3648\nDataset: CiteSeer, Policy: ego, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 25.56%\n\nGenerating subgraphs using edge_deleted policy for CiteSeer.\nTraining baseline ESAN model for dataset CiteSeer (Policy: edge_deleted).\nEpoch 10/200, Loss: 0.3946, Train Accuracy: 0.8579\nEpoch 20/200, Loss: 0.1729, Train Accuracy: 0.9232\nEpoch 30/200, Loss: 0.1041, Train Accuracy: 0.9554\nEpoch 40/200, Loss: 0.0757, Train Accuracy: 0.9648\nEpoch 50/200, Loss: 0.0632, Train Accuracy: 0.9712\nEpoch 60/200, Loss: 0.0545, Train Accuracy: 0.9730\nEpoch 70/200, Loss: 0.0520, Train Accuracy: 0.9755\nEpoch 80/200, Loss: 0.0519, Train Accuracy: 0.9755\nEpoch 90/200, Loss: 0.0492, Train Accuracy: 0.9773\nEpoch 100/200, Loss: 0.0463, Train Accuracy: 0.9773\nEpoch 110/200, Loss: 0.0484, Train Accuracy: 0.9777\nEpoch 120/200, Loss: 0.0443, Train Accuracy: 0.9768\nEpoch 130/200, Loss: 0.0463, Train Accuracy: 0.9773\nEpoch 140/200, Loss: 0.0458, Train Accuracy: 0.9768\nEpoch 150/200, Loss: 0.0413, Train Accuracy: 0.9773\nEpoch 160/200, Loss: 0.0409, Train Accuracy: 0.9768\nEpoch 170/200, Loss: 0.0412, Train Accuracy: 0.9777\nEpoch 180/200, Loss: 0.0387, Train Accuracy: 0.9773\nEpoch 190/200, Loss: 0.0383, Train Accuracy: 0.9777\nEpoch 200/200, Loss: 0.0393, Train Accuracy: 0.9773\nTrain Accuracy: 0.9785\nValidation Accuracy: 0.7349\nTest Accuracy: 0.7233\nDataset: CiteSeer, Policy: edge_deleted, Baseline Test Accuracy: 72.33%\n\nStarting attack SBA-Samp on dataset: CiteSeer (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.4108, Train Accuracy: 0.8541\nEpoch 20/200, Loss: 0.1783, Train Accuracy: 0.9202\nEpoch 30/200, Loss: 0.1076, Train Accuracy: 0.9489\nEpoch 40/200, Loss: 0.0754, Train Accuracy: 0.9678\nEpoch 50/200, Loss: 0.0607, Train Accuracy: 0.9704\nEpoch 60/200, Loss: 0.0569, Train Accuracy: 0.9738\nEpoch 70/200, Loss: 0.0493, Train Accuracy: 0.9755\nEpoch 80/200, Loss: 0.0511, Train Accuracy: 0.9747\nEpoch 90/200, Loss: 0.0461, Train Accuracy: 0.9768\nEpoch 100/200, Loss: 0.0436, Train Accuracy: 0.9777\nEpoch 110/200, Loss: 0.0438, Train Accuracy: 0.9773\nEpoch 120/200, Loss: 0.0411, Train Accuracy: 0.9777\nEpoch 130/200, Loss: 0.0417, Train Accuracy: 0.9785\nEpoch 140/200, Loss: 0.0411, Train Accuracy: 0.9781\nEpoch 150/200, Loss: 0.0431, Train Accuracy: 0.9773\nEpoch 160/200, Loss: 0.0394, Train Accuracy: 0.9773\nEpoch 170/200, Loss: 0.0414, Train Accuracy: 0.9777\nEpoch 180/200, Loss: 0.0405, Train Accuracy: 0.9781\nEpoch 190/200, Loss: 0.0407, Train Accuracy: 0.9777\nEpoch 200/200, Loss: 0.0401, Train Accuracy: 0.9781\nDataset: CiteSeer, Policy: edge_deleted, Attack: SBA-Samp - ASR: 45.00%, Clean Accuracy: 72.48%\n\nStarting attack SBA-Gen on dataset: CiteSeer (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.4139, Train Accuracy: 0.8494\nEpoch 20/200, Loss: 0.1794, Train Accuracy: 0.9227\nEpoch 30/200, Loss: 0.1067, Train Accuracy: 0.9515\nEpoch 40/200, Loss: 0.0757, Train Accuracy: 0.9652\nEpoch 50/200, Loss: 0.0624, Train Accuracy: 0.9691\nEpoch 60/200, Loss: 0.0552, Train Accuracy: 0.9738\nEpoch 70/200, Loss: 0.0514, Train Accuracy: 0.9734\nEpoch 80/200, Loss: 0.0474, Train Accuracy: 0.9751\nEpoch 90/200, Loss: 0.0471, Train Accuracy: 0.9768\nEpoch 100/200, Loss: 0.0468, Train Accuracy: 0.9781\nEpoch 110/200, Loss: 0.0455, Train Accuracy: 0.9760\nEpoch 120/200, Loss: 0.0429, Train Accuracy: 0.9777\nEpoch 130/200, Loss: 0.0452, Train Accuracy: 0.9755\nEpoch 140/200, Loss: 0.0413, Train Accuracy: 0.9773\nEpoch 150/200, Loss: 0.0415, Train Accuracy: 0.9777\nEpoch 160/200, Loss: 0.0434, Train Accuracy: 0.9777\nEpoch 170/200, Loss: 0.0397, Train Accuracy: 0.9781\nEpoch 180/200, Loss: 0.0406, Train Accuracy: 0.9785\nEpoch 190/200, Loss: 0.0378, Train Accuracy: 0.9781\nEpoch 200/200, Loss: 0.0386, Train Accuracy: 0.9781\nDataset: CiteSeer, Policy: edge_deleted, Attack: SBA-Gen - ASR: 45.00%, Clean Accuracy: 72.48%\n\nStarting attack GTA on dataset: CiteSeer (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.4353, Train Accuracy: 0.8455\nEpoch 20/200, Loss: 0.1878, Train Accuracy: 0.9197\nEpoch 30/200, Loss: 0.1093, Train Accuracy: 0.9532\nEpoch 40/200, Loss: 0.0757, Train Accuracy: 0.9661\nEpoch 50/200, Loss: 0.0605, Train Accuracy: 0.9717\nEpoch 60/200, Loss: 0.0544, Train Accuracy: 0.9751\nEpoch 70/200, Loss: 0.0505, Train Accuracy: 0.9764\nEpoch 80/200, Loss: 0.0523, Train Accuracy: 0.9751\nEpoch 90/200, Loss: 0.0460, Train Accuracy: 0.9773\nEpoch 100/200, Loss: 0.0450, Train Accuracy: 0.9764\nEpoch 110/200, Loss: 0.0484, Train Accuracy: 0.9768\nEpoch 120/200, Loss: 0.0401, Train Accuracy: 0.9764\nEpoch 130/200, Loss: 0.0411, Train Accuracy: 0.9773\nEpoch 140/200, Loss: 0.0423, Train Accuracy: 0.9768\nEpoch 150/200, Loss: 0.0410, Train Accuracy: 0.9781\nEpoch 160/200, Loss: 0.0409, Train Accuracy: 0.9777\nEpoch 170/200, Loss: 0.0405, Train Accuracy: 0.9777\nEpoch 180/200, Loss: 0.0400, Train Accuracy: 0.9781\nEpoch 190/200, Loss: 0.0378, Train Accuracy: 0.9781\nEpoch 200/200, Loss: 0.0378, Train Accuracy: 0.9768\nDataset: CiteSeer, Policy: edge_deleted, Attack: GTA - ASR: 0.00%, Clean Accuracy: 71.88%\n\nStarting attack UGBA on dataset: CiteSeer (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.4220, Train Accuracy: 0.8575\nEpoch 20/200, Loss: 0.1863, Train Accuracy: 0.9193\nEpoch 30/200, Loss: 0.1046, Train Accuracy: 0.9519\nEpoch 40/200, Loss: 0.0735, Train Accuracy: 0.9657\nEpoch 50/200, Loss: 0.0584, Train Accuracy: 0.9712\nEpoch 60/200, Loss: 0.0533, Train Accuracy: 0.9734\nEpoch 70/200, Loss: 0.0542, Train Accuracy: 0.9747\nEpoch 80/200, Loss: 0.0504, Train Accuracy: 0.9760\nEpoch 90/200, Loss: 0.0449, Train Accuracy: 0.9768\nEpoch 100/200, Loss: 0.0464, Train Accuracy: 0.9760\nEpoch 110/200, Loss: 0.0434, Train Accuracy: 0.9773\nEpoch 120/200, Loss: 0.0410, Train Accuracy: 0.9764\nEpoch 130/200, Loss: 0.0423, Train Accuracy: 0.9768\nEpoch 140/200, Loss: 0.0412, Train Accuracy: 0.9781\nEpoch 150/200, Loss: 0.0422, Train Accuracy: 0.9764\nEpoch 160/200, Loss: 0.0399, Train Accuracy: 0.9785\nEpoch 170/200, Loss: 0.0386, Train Accuracy: 0.9781\nEpoch 180/200, Loss: 0.0397, Train Accuracy: 0.9777\nEpoch 190/200, Loss: 0.0406, Train Accuracy: 0.9777\nEpoch 200/200, Loss: 0.0383, Train Accuracy: 0.9785\nDataset: CiteSeer, Policy: edge_deleted, Attack: UGBA - ASR: 0.00%, Clean Accuracy: 71.88%\n\nStarting attack DPGBA on dataset: CiteSeer (Policy: edge_deleted)\nInitializing trigger generator and training OOD Detector for DPGBA on dataset: CiteSeer (Policy: edge_deleted)\nEpoch 0, Reconstruction Loss: 0.0086\nEpoch 10, Reconstruction Loss: 0.0083\nEpoch 20, Reconstruction Loss: 0.0082\nEpoch 30, Reconstruction Loss: 0.0081\nEpoch 40, Reconstruction Loss: 0.0080\nEpoch 50, Reconstruction Loss: 0.0079\nEpoch 60, Reconstruction Loss: 0.0079\nEpoch 70, Reconstruction Loss: 0.0078\nEpoch 80, Reconstruction Loss: 0.0077\nEpoch 90, Reconstruction Loss: 0.0077\nEpoch 10/200, Loss: 0.4280, Train Accuracy: 0.8536\nEpoch 20/200, Loss: 0.1880, Train Accuracy: 0.9185\nEpoch 30/200, Loss: 0.1082, Train Accuracy: 0.9498\nEpoch 40/200, Loss: 0.0738, Train Accuracy: 0.9635\nEpoch 50/200, Loss: 0.0593, Train Accuracy: 0.9717\nEpoch 60/200, Loss: 0.0544, Train Accuracy: 0.9734\nEpoch 70/200, Loss: 0.0503, Train Accuracy: 0.9760\nEpoch 80/200, Loss: 0.0504, Train Accuracy: 0.9751\nEpoch 90/200, Loss: 0.0483, Train Accuracy: 0.9768\nEpoch 100/200, Loss: 0.0465, Train Accuracy: 0.9760\nEpoch 110/200, Loss: 0.0453, Train Accuracy: 0.9773\nEpoch 120/200, Loss: 0.0461, Train Accuracy: 0.9781\nEpoch 130/200, Loss: 0.0445, Train Accuracy: 0.9773\nEpoch 140/200, Loss: 0.0425, Train Accuracy: 0.9777\nEpoch 150/200, Loss: 0.0407, Train Accuracy: 0.9781\nEpoch 160/200, Loss: 0.0406, Train Accuracy: 0.9781\nEpoch 170/200, Loss: 0.0387, Train Accuracy: 0.9777\nEpoch 180/200, Loss: 0.0375, Train Accuracy: 0.9768\nEpoch 190/200, Loss: 0.0381, Train Accuracy: 0.9777\nEpoch 200/200, Loss: 0.0394, Train Accuracy: 0.9781\nDataset: CiteSeer, Policy: edge_deleted, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 73.08%\n\nGenerating subgraphs using node_deleted policy for CiteSeer.\nTraining baseline ESAN model for dataset CiteSeer (Policy: node_deleted).\nEpoch 10/200, Loss: 0.4327, Train Accuracy: 0.8489\nEpoch 20/200, Loss: 0.1825, Train Accuracy: 0.9185\nEpoch 30/200, Loss: 0.1066, Train Accuracy: 0.9536\nEpoch 40/200, Loss: 0.0750, Train Accuracy: 0.9674\nEpoch 50/200, Loss: 0.0605, Train Accuracy: 0.9704\nEpoch 60/200, Loss: 0.0549, Train Accuracy: 0.9738\nEpoch 70/200, Loss: 0.0520, Train Accuracy: 0.9738\nEpoch 80/200, Loss: 0.0522, Train Accuracy: 0.9751\nEpoch 90/200, Loss: 0.0508, Train Accuracy: 0.9755\nEpoch 100/200, Loss: 0.0477, Train Accuracy: 0.9755\nEpoch 110/200, Loss: 0.0474, Train Accuracy: 0.9777\nEpoch 120/200, Loss: 0.0427, Train Accuracy: 0.9764\nEpoch 130/200, Loss: 0.0431, Train Accuracy: 0.9773\nEpoch 140/200, Loss: 0.0425, Train Accuracy: 0.9760\nEpoch 150/200, Loss: 0.0421, Train Accuracy: 0.9773\nEpoch 160/200, Loss: 0.0430, Train Accuracy: 0.9773\nEpoch 170/200, Loss: 0.0408, Train Accuracy: 0.9777\nEpoch 180/200, Loss: 0.0401, Train Accuracy: 0.9768\nEpoch 190/200, Loss: 0.0393, Train Accuracy: 0.9781\nEpoch 200/200, Loss: 0.0385, Train Accuracy: 0.9785\nTrain Accuracy: 0.9773\nValidation Accuracy: 0.7380\nTest Accuracy: 0.7248\nDataset: CiteSeer, Policy: node_deleted, Baseline Test Accuracy: 72.48%\n\nStarting attack SBA-Samp on dataset: CiteSeer (Policy: node_deleted)\nEpoch 10/200, Loss: 0.4232, Train Accuracy: 0.8536\nEpoch 20/200, Loss: 0.1797, Train Accuracy: 0.9240\nEpoch 30/200, Loss: 0.1012, Train Accuracy: 0.9528\nEpoch 40/200, Loss: 0.0751, Train Accuracy: 0.9648\nEpoch 50/200, Loss: 0.0633, Train Accuracy: 0.9734\nEpoch 60/200, Loss: 0.0546, Train Accuracy: 0.9725\nEpoch 70/200, Loss: 0.0535, Train Accuracy: 0.9738\nEpoch 80/200, Loss: 0.0481, Train Accuracy: 0.9742\nEpoch 90/200, Loss: 0.0479, Train Accuracy: 0.9777\nEpoch 100/200, Loss: 0.0463, Train Accuracy: 0.9764\nEpoch 110/200, Loss: 0.0451, Train Accuracy: 0.9773\nEpoch 120/200, Loss: 0.0448, Train Accuracy: 0.9773\nEpoch 130/200, Loss: 0.0422, Train Accuracy: 0.9764\nEpoch 140/200, Loss: 0.0415, Train Accuracy: 0.9773\nEpoch 150/200, Loss: 0.0407, Train Accuracy: 0.9781\nEpoch 160/200, Loss: 0.0416, Train Accuracy: 0.9773\nEpoch 170/200, Loss: 0.0401, Train Accuracy: 0.9768\nEpoch 180/200, Loss: 0.0390, Train Accuracy: 0.9785\nEpoch 190/200, Loss: 0.0404, Train Accuracy: 0.9777\nEpoch 200/200, Loss: 0.0375, Train Accuracy: 0.9785\nDataset: CiteSeer, Policy: node_deleted, Attack: SBA-Samp - ASR: 45.00%, Clean Accuracy: 73.23%\n\nStarting attack SBA-Gen on dataset: CiteSeer (Policy: node_deleted)\nEpoch 10/200, Loss: 0.4179, Train Accuracy: 0.8489\nEpoch 20/200, Loss: 0.1770, Train Accuracy: 0.9202\nEpoch 30/200, Loss: 0.1021, Train Accuracy: 0.9549\nEpoch 40/200, Loss: 0.0734, Train Accuracy: 0.9661\nEpoch 50/200, Loss: 0.0625, Train Accuracy: 0.9730\nEpoch 60/200, Loss: 0.0573, Train Accuracy: 0.9738\nEpoch 70/200, Loss: 0.0507, Train Accuracy: 0.9751\nEpoch 80/200, Loss: 0.0462, Train Accuracy: 0.9755\nEpoch 90/200, Loss: 0.0463, Train Accuracy: 0.9768\nEpoch 100/200, Loss: 0.0451, Train Accuracy: 0.9773\nEpoch 110/200, Loss: 0.0433, Train Accuracy: 0.9764\nEpoch 120/200, Loss: 0.0429, Train Accuracy: 0.9777\nEpoch 130/200, Loss: 0.0428, Train Accuracy: 0.9773\nEpoch 140/200, Loss: 0.0426, Train Accuracy: 0.9777\nEpoch 150/200, Loss: 0.0410, Train Accuracy: 0.9768\nEpoch 160/200, Loss: 0.0402, Train Accuracy: 0.9777\nEpoch 170/200, Loss: 0.0402, Train Accuracy: 0.9773\nEpoch 180/200, Loss: 0.0424, Train Accuracy: 0.9768\nEpoch 190/200, Loss: 0.0383, Train Accuracy: 0.9781\nEpoch 200/200, Loss: 0.0391, Train Accuracy: 0.9773\nDataset: CiteSeer, Policy: node_deleted, Attack: SBA-Gen - ASR: 45.00%, Clean Accuracy: 72.18%\n\nStarting attack GTA on dataset: CiteSeer (Policy: node_deleted)\nEpoch 10/200, Loss: 0.4034, Train Accuracy: 0.8558\nEpoch 20/200, Loss: 0.1775, Train Accuracy: 0.9167\nEpoch 30/200, Loss: 0.1055, Train Accuracy: 0.9519\nEpoch 40/200, Loss: 0.0738, Train Accuracy: 0.9670\nEpoch 50/200, Loss: 0.0599, Train Accuracy: 0.9717\nEpoch 60/200, Loss: 0.0526, Train Accuracy: 0.9764\nEpoch 70/200, Loss: 0.0498, Train Accuracy: 0.9738\nEpoch 80/200, Loss: 0.0468, Train Accuracy: 0.9768\nEpoch 90/200, Loss: 0.0487, Train Accuracy: 0.9764\nEpoch 100/200, Loss: 0.0466, Train Accuracy: 0.9773\nEpoch 110/200, Loss: 0.0436, Train Accuracy: 0.9781\nEpoch 120/200, Loss: 0.0438, Train Accuracy: 0.9768\nEpoch 130/200, Loss: 0.0393, Train Accuracy: 0.9777\nEpoch 140/200, Loss: 0.0416, Train Accuracy: 0.9768\nEpoch 150/200, Loss: 0.0416, Train Accuracy: 0.9777\nEpoch 160/200, Loss: 0.0411, Train Accuracy: 0.9768\nEpoch 170/200, Loss: 0.0399, Train Accuracy: 0.9768\nEpoch 180/200, Loss: 0.0396, Train Accuracy: 0.9781\nEpoch 190/200, Loss: 0.0398, Train Accuracy: 0.9781\nEpoch 200/200, Loss: 0.0378, Train Accuracy: 0.9785\nDataset: CiteSeer, Policy: node_deleted, Attack: GTA - ASR: 0.00%, Clean Accuracy: 72.33%\n\nStarting attack UGBA on dataset: CiteSeer (Policy: node_deleted)\nEpoch 10/200, Loss: 0.4120, Train Accuracy: 0.8579\nEpoch 20/200, Loss: 0.1812, Train Accuracy: 0.9167\nEpoch 30/200, Loss: 0.1058, Train Accuracy: 0.9528\nEpoch 40/200, Loss: 0.0745, Train Accuracy: 0.9665\nEpoch 50/200, Loss: 0.0601, Train Accuracy: 0.9721\nEpoch 60/200, Loss: 0.0535, Train Accuracy: 0.9730\nEpoch 70/200, Loss: 0.0511, Train Accuracy: 0.9755\nEpoch 80/200, Loss: 0.0506, Train Accuracy: 0.9751\nEpoch 90/200, Loss: 0.0479, Train Accuracy: 0.9755\nEpoch 100/200, Loss: 0.0459, Train Accuracy: 0.9760\nEpoch 110/200, Loss: 0.0446, Train Accuracy: 0.9764\nEpoch 120/200, Loss: 0.0432, Train Accuracy: 0.9768\nEpoch 130/200, Loss: 0.0423, Train Accuracy: 0.9764\nEpoch 140/200, Loss: 0.0428, Train Accuracy: 0.9773\nEpoch 150/200, Loss: 0.0414, Train Accuracy: 0.9781\nEpoch 160/200, Loss: 0.0407, Train Accuracy: 0.9768\nEpoch 170/200, Loss: 0.0403, Train Accuracy: 0.9777\nEpoch 180/200, Loss: 0.0399, Train Accuracy: 0.9777\nEpoch 190/200, Loss: 0.0388, Train Accuracy: 0.9777\nEpoch 200/200, Loss: 0.0387, Train Accuracy: 0.9781\nDataset: CiteSeer, Policy: node_deleted, Attack: UGBA - ASR: 0.00%, Clean Accuracy: 70.83%\n\nStarting attack DPGBA on dataset: CiteSeer (Policy: node_deleted)\nInitializing trigger generator and training OOD Detector for DPGBA on dataset: CiteSeer (Policy: node_deleted)\nEpoch 0, Reconstruction Loss: 0.0086\nEpoch 10, Reconstruction Loss: 0.0083\nEpoch 20, Reconstruction Loss: 0.0082\nEpoch 30, Reconstruction Loss: 0.0081\nEpoch 40, Reconstruction Loss: 0.0080\nEpoch 50, Reconstruction Loss: 0.0079\nEpoch 60, Reconstruction Loss: 0.0078\nEpoch 70, Reconstruction Loss: 0.0077\nEpoch 80, Reconstruction Loss: 0.0077\nEpoch 90, Reconstruction Loss: 0.0076\nEpoch 10/200, Loss: 0.4215, Train Accuracy: 0.8502\nEpoch 20/200, Loss: 0.1841, Train Accuracy: 0.9215\nEpoch 30/200, Loss: 0.1079, Train Accuracy: 0.9536\nEpoch 40/200, Loss: 0.0758, Train Accuracy: 0.9648\nEpoch 50/200, Loss: 0.0606, Train Accuracy: 0.9717\nEpoch 60/200, Loss: 0.0558, Train Accuracy: 0.9755\nEpoch 70/200, Loss: 0.0528, Train Accuracy: 0.9760\nEpoch 80/200, Loss: 0.0514, Train Accuracy: 0.9760\nEpoch 90/200, Loss: 0.0462, Train Accuracy: 0.9768\nEpoch 100/200, Loss: 0.0434, Train Accuracy: 0.9781\nEpoch 110/200, Loss: 0.0421, Train Accuracy: 0.9755\nEpoch 120/200, Loss: 0.0442, Train Accuracy: 0.9773\nEpoch 130/200, Loss: 0.0433, Train Accuracy: 0.9773\nEpoch 140/200, Loss: 0.0422, Train Accuracy: 0.9764\nEpoch 150/200, Loss: 0.0406, Train Accuracy: 0.9785\nEpoch 160/200, Loss: 0.0420, Train Accuracy: 0.9773\nEpoch 170/200, Loss: 0.0411, Train Accuracy: 0.9768\nEpoch 180/200, Loss: 0.0422, Train Accuracy: 0.9777\nEpoch 190/200, Loss: 0.0388, Train Accuracy: 0.9777\nEpoch 200/200, Loss: 0.0402, Train Accuracy: 0.9768\nDataset: CiteSeer, Policy: node_deleted, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 72.18%\n\nStarting process for dataset: PubMed\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nGenerating subgraphs using ego policy for PubMed.\nTraining baseline ESAN model for dataset PubMed (Policy: ego).\nEpoch 10/200, Loss: 0.9681, Train Accuracy: 0.3405\nEpoch 20/200, Loss: 0.9450, Train Accuracy: 0.3493\nEpoch 30/200, Loss: 0.9303, Train Accuracy: 0.3531\nEpoch 40/200, Loss: 0.9202, Train Accuracy: 0.3576\nEpoch 50/200, Loss: 0.9121, Train Accuracy: 0.3604\nEpoch 60/200, Loss: 0.9063, Train Accuracy: 0.3617\nEpoch 70/200, Loss: 0.9000, Train Accuracy: 0.3651\nEpoch 80/200, Loss: 0.8929, Train Accuracy: 0.3680\nEpoch 90/200, Loss: 0.8878, Train Accuracy: 0.3698\nEpoch 100/200, Loss: 0.8840, Train Accuracy: 0.3717\nEpoch 110/200, Loss: 0.8820, Train Accuracy: 0.3715\nEpoch 120/200, Loss: 0.8789, Train Accuracy: 0.3730\nEpoch 130/200, Loss: 0.8766, Train Accuracy: 0.3743\nEpoch 140/200, Loss: 0.8747, Train Accuracy: 0.3745\nEpoch 150/200, Loss: 0.8734, Train Accuracy: 0.3752\nEpoch 160/200, Loss: 0.8718, Train Accuracy: 0.3756\nEpoch 170/200, Loss: 0.8721, Train Accuracy: 0.3750\nEpoch 180/200, Loss: 0.8705, Train Accuracy: 0.3759\nEpoch 190/200, Loss: 0.8696, Train Accuracy: 0.3767\nEpoch 200/200, Loss: 0.8696, Train Accuracy: 0.3762\nTrain Accuracy: 0.3767\nValidation Accuracy: 0.3460\nTest Accuracy: 0.3434\nDataset: PubMed, Policy: ego, Baseline Test Accuracy: 34.34%\n\nStarting attack SBA-Samp on dataset: PubMed (Policy: ego)\nEpoch 10/200, Loss: 0.9671, Train Accuracy: 0.3404\nEpoch 20/200, Loss: 0.9439, Train Accuracy: 0.3497\nEpoch 30/200, Loss: 0.9303, Train Accuracy: 0.3533\nEpoch 40/200, Loss: 0.9233, Train Accuracy: 0.3562\nEpoch 50/200, Loss: 0.9141, Train Accuracy: 0.3600\nEpoch 60/200, Loss: 0.9083, Train Accuracy: 0.3617\nEpoch 70/200, Loss: 0.9018, Train Accuracy: 0.3644\nEpoch 80/200, Loss: 0.8947, Train Accuracy: 0.3678\nEpoch 90/200, Loss: 0.8892, Train Accuracy: 0.3696\nEpoch 100/200, Loss: 0.8843, Train Accuracy: 0.3709\nEpoch 110/200, Loss: 0.8809, Train Accuracy: 0.3719\nEpoch 120/200, Loss: 0.8781, Train Accuracy: 0.3738\nEpoch 130/200, Loss: 0.8756, Train Accuracy: 0.3740\nEpoch 140/200, Loss: 0.8739, Train Accuracy: 0.3751\nEpoch 150/200, Loss: 0.8726, Train Accuracy: 0.3756\nEpoch 160/200, Loss: 0.8715, Train Accuracy: 0.3755\nEpoch 170/200, Loss: 0.8718, Train Accuracy: 0.3756\nEpoch 180/200, Loss: 0.8698, Train Accuracy: 0.3762\nEpoch 190/200, Loss: 0.8696, Train Accuracy: 0.3765\nEpoch 200/200, Loss: 0.8690, Train Accuracy: 0.3768\nDataset: PubMed, Policy: ego, Attack: SBA-Samp - ASR: 63.33%, Clean Accuracy: 34.26%\n\nStarting attack SBA-Gen on dataset: PubMed (Policy: ego)\nEpoch 10/200, Loss: 0.9688, Train Accuracy: 0.3404\nEpoch 20/200, Loss: 0.9427, Train Accuracy: 0.3501\nEpoch 30/200, Loss: 0.9296, Train Accuracy: 0.3535\nEpoch 40/200, Loss: 0.9216, Train Accuracy: 0.3574\nEpoch 50/200, Loss: 0.9153, Train Accuracy: 0.3585\nEpoch 60/200, Loss: 0.9065, Train Accuracy: 0.3627\nEpoch 70/200, Loss: 0.9010, Train Accuracy: 0.3649\nEpoch 80/200, Loss: 0.8959, Train Accuracy: 0.3671\nEpoch 90/200, Loss: 0.8900, Train Accuracy: 0.3693\nEpoch 100/200, Loss: 0.8854, Train Accuracy: 0.3712\nEpoch 110/200, Loss: 0.8824, Train Accuracy: 0.3720\nEpoch 120/200, Loss: 0.8785, Train Accuracy: 0.3733\nEpoch 130/200, Loss: 0.8771, Train Accuracy: 0.3735\nEpoch 140/200, Loss: 0.8755, Train Accuracy: 0.3743\nEpoch 150/200, Loss: 0.8734, Train Accuracy: 0.3751\nEpoch 160/200, Loss: 0.8720, Train Accuracy: 0.3755\nEpoch 170/200, Loss: 0.8725, Train Accuracy: 0.3759\nEpoch 180/200, Loss: 0.8706, Train Accuracy: 0.3761\nEpoch 190/200, Loss: 0.8713, Train Accuracy: 0.3755\nEpoch 200/200, Loss: 0.8702, Train Accuracy: 0.3759\nDataset: PubMed, Policy: ego, Attack: SBA-Gen - ASR: 60.00%, Clean Accuracy: 34.19%\n\nStarting attack GTA on dataset: PubMed (Policy: ego)\nEpoch 10/200, Loss: 0.9634, Train Accuracy: 0.3428\nEpoch 20/200, Loss: 0.9448, Train Accuracy: 0.3504\nEpoch 30/200, Loss: 0.9310, Train Accuracy: 0.3530\nEpoch 40/200, Loss: 0.9217, Train Accuracy: 0.3563\nEpoch 50/200, Loss: 0.9161, Train Accuracy: 0.3583\nEpoch 60/200, Loss: 0.9097, Train Accuracy: 0.3621\nEpoch 70/200, Loss: 0.9040, Train Accuracy: 0.3633\nEpoch 80/200, Loss: 0.8984, Train Accuracy: 0.3664\nEpoch 90/200, Loss: 0.8923, Train Accuracy: 0.3686\nEpoch 100/200, Loss: 0.8873, Train Accuracy: 0.3704\nEpoch 110/200, Loss: 0.8842, Train Accuracy: 0.3717\nEpoch 120/200, Loss: 0.8821, Train Accuracy: 0.3711\nEpoch 130/200, Loss: 0.8782, Train Accuracy: 0.3739\nEpoch 140/200, Loss: 0.8756, Train Accuracy: 0.3746\nEpoch 150/200, Loss: 0.8761, Train Accuracy: 0.3743\nEpoch 160/200, Loss: 0.8742, Train Accuracy: 0.3741\nEpoch 170/200, Loss: 0.8728, Train Accuracy: 0.3759\nEpoch 180/200, Loss: 0.8712, Train Accuracy: 0.3757\nEpoch 190/200, Loss: 0.8710, Train Accuracy: 0.3761\nEpoch 200/200, Loss: 0.8706, Train Accuracy: 0.3757\nDataset: PubMed, Policy: ego, Attack: GTA - ASR: 10.00%, Clean Accuracy: 34.14%\n\nStarting attack UGBA on dataset: PubMed (Policy: ego)\nEpoch 10/200, Loss: 0.9687, Train Accuracy: 0.3418\nEpoch 20/200, Loss: 0.9441, Train Accuracy: 0.3497\nEpoch 30/200, Loss: 0.9300, Train Accuracy: 0.3533\nEpoch 40/200, Loss: 0.9224, Train Accuracy: 0.3559\nEpoch 50/200, Loss: 0.9163, Train Accuracy: 0.3591\nEpoch 60/200, Loss: 0.9098, Train Accuracy: 0.3621\nEpoch 70/200, Loss: 0.9029, Train Accuracy: 0.3642\nEpoch 80/200, Loss: 0.8976, Train Accuracy: 0.3652\nEpoch 90/200, Loss: 0.8916, Train Accuracy: 0.3677\nEpoch 100/200, Loss: 0.8869, Train Accuracy: 0.3696\nEpoch 110/200, Loss: 0.8843, Train Accuracy: 0.3709\nEpoch 120/200, Loss: 0.8810, Train Accuracy: 0.3722\nEpoch 130/200, Loss: 0.8786, Train Accuracy: 0.3729\nEpoch 140/200, Loss: 0.8763, Train Accuracy: 0.3746\nEpoch 150/200, Loss: 0.8746, Train Accuracy: 0.3749\nEpoch 160/200, Loss: 0.8733, Train Accuracy: 0.3748\nEpoch 170/200, Loss: 0.8739, Train Accuracy: 0.3751\nEpoch 180/200, Loss: 0.8713, Train Accuracy: 0.3756\nEpoch 190/200, Loss: 0.8712, Train Accuracy: 0.3752\nEpoch 200/200, Loss: 0.8705, Train Accuracy: 0.3762\nDataset: PubMed, Policy: ego, Attack: UGBA - ASR: 10.00%, Clean Accuracy: 34.09%\n\nStarting attack DPGBA on dataset: PubMed (Policy: ego)\nInitializing trigger generator and training OOD Detector for DPGBA on dataset: PubMed (Policy: ego)\nEpoch 0, Reconstruction Loss: 0.0003\nEpoch 10, Reconstruction Loss: 0.0003\nEpoch 20, Reconstruction Loss: 0.0003\nEpoch 30, Reconstruction Loss: 0.0003\nEpoch 40, Reconstruction Loss: 0.0003\nEpoch 50, Reconstruction Loss: 0.0003\nEpoch 60, Reconstruction Loss: 0.0003\nEpoch 70, Reconstruction Loss: 0.0003\nEpoch 80, Reconstruction Loss: 0.0003\nEpoch 90, Reconstruction Loss: 0.0003\nEpoch 10/200, Loss: 0.9705, Train Accuracy: 0.3400\nEpoch 20/200, Loss: 0.9440, Train Accuracy: 0.3506\nEpoch 30/200, Loss: 0.9294, Train Accuracy: 0.3533\nEpoch 40/200, Loss: 0.9221, Train Accuracy: 0.3559\nEpoch 50/200, Loss: 0.9137, Train Accuracy: 0.3610\nEpoch 60/200, Loss: 0.9075, Train Accuracy: 0.3622\nEpoch 70/200, Loss: 0.8999, Train Accuracy: 0.3653\nEpoch 80/200, Loss: 0.8926, Train Accuracy: 0.3683\nEpoch 90/200, Loss: 0.8893, Train Accuracy: 0.3695\nEpoch 100/200, Loss: 0.8835, Train Accuracy: 0.3717\nEpoch 110/200, Loss: 0.8796, Train Accuracy: 0.3731\nEpoch 120/200, Loss: 0.8778, Train Accuracy: 0.3735\nEpoch 130/200, Loss: 0.8759, Train Accuracy: 0.3744\nEpoch 140/200, Loss: 0.8741, Train Accuracy: 0.3750\nEpoch 150/200, Loss: 0.8721, Train Accuracy: 0.3759\nEpoch 160/200, Loss: 0.8719, Train Accuracy: 0.3749\nEpoch 170/200, Loss: 0.8708, Train Accuracy: 0.3759\nEpoch 180/200, Loss: 0.8698, Train Accuracy: 0.3764\nEpoch 190/200, Loss: 0.8704, Train Accuracy: 0.3760\nEpoch 200/200, Loss: 0.8688, Train Accuracy: 0.3764\nDataset: PubMed, Policy: ego, Attack: DPGBA - ASR: 3.33%, Clean Accuracy: 34.16%\n\nGenerating subgraphs using edge_deleted policy for PubMed.\nTraining baseline ESAN model for dataset PubMed (Policy: edge_deleted).\nEpoch 10/200, Loss: 0.6817, Train Accuracy: 0.7924\nEpoch 20/200, Loss: 0.4155, Train Accuracy: 0.8478\nEpoch 30/200, Loss: 0.3499, Train Accuracy: 0.8734\nEpoch 40/200, Loss: 0.3239, Train Accuracy: 0.8826\nEpoch 50/200, Loss: 0.3028, Train Accuracy: 0.8902\nEpoch 60/200, Loss: 0.2919, Train Accuracy: 0.8946\nEpoch 70/200, Loss: 0.2791, Train Accuracy: 0.8997\nEpoch 80/200, Loss: 0.2668, Train Accuracy: 0.9024\nEpoch 90/200, Loss: 0.2545, Train Accuracy: 0.9073\nEpoch 100/200, Loss: 0.2430, Train Accuracy: 0.9134\nEpoch 110/200, Loss: 0.2358, Train Accuracy: 0.9144\nEpoch 120/200, Loss: 0.2239, Train Accuracy: 0.9202\nEpoch 130/200, Loss: 0.2171, Train Accuracy: 0.9223\nEpoch 140/200, Loss: 0.2055, Train Accuracy: 0.9283\nEpoch 150/200, Loss: 0.1964, Train Accuracy: 0.9304\nEpoch 160/200, Loss: 0.2088, Train Accuracy: 0.9241\nEpoch 170/200, Loss: 0.1868, Train Accuracy: 0.9337\nEpoch 180/200, Loss: 0.1767, Train Accuracy: 0.9399\nEpoch 190/200, Loss: 0.1705, Train Accuracy: 0.9425\nEpoch 200/200, Loss: 0.1654, Train Accuracy: 0.9430\nTrain Accuracy: 0.9470\nValidation Accuracy: 0.8777\nTest Accuracy: 0.8663\nDataset: PubMed, Policy: edge_deleted, Baseline Test Accuracy: 86.63%\n\nStarting attack SBA-Samp on dataset: PubMed (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.7433, Train Accuracy: 0.7852\nEpoch 20/200, Loss: 0.4189, Train Accuracy: 0.8458\nEpoch 30/200, Loss: 0.3601, Train Accuracy: 0.8710\nEpoch 40/200, Loss: 0.3281, Train Accuracy: 0.8824\nEpoch 50/200, Loss: 0.3058, Train Accuracy: 0.8901\nEpoch 60/200, Loss: 0.2920, Train Accuracy: 0.8948\nEpoch 70/200, Loss: 0.2820, Train Accuracy: 0.8994\nEpoch 80/200, Loss: 0.2687, Train Accuracy: 0.9019\nEpoch 90/200, Loss: 0.2562, Train Accuracy: 0.9065\nEpoch 100/200, Loss: 0.2455, Train Accuracy: 0.9105\nEpoch 110/200, Loss: 0.2344, Train Accuracy: 0.9163\nEpoch 120/200, Loss: 0.2223, Train Accuracy: 0.9202\nEpoch 130/200, Loss: 0.2122, Train Accuracy: 0.9238\nEpoch 140/200, Loss: 0.1999, Train Accuracy: 0.9291\nEpoch 150/200, Loss: 0.1893, Train Accuracy: 0.9330\nEpoch 160/200, Loss: 0.1857, Train Accuracy: 0.9348\nEpoch 170/200, Loss: 0.1725, Train Accuracy: 0.9390\nEpoch 180/200, Loss: 0.1645, Train Accuracy: 0.9433\nEpoch 190/200, Loss: 0.1582, Train Accuracy: 0.9473\nEpoch 200/200, Loss: 0.1524, Train Accuracy: 0.9486\nDataset: PubMed, Policy: edge_deleted, Attack: SBA-Samp - ASR: 63.33%, Clean Accuracy: 86.53%\n\nStarting attack SBA-Gen on dataset: PubMed (Policy: edge_deleted)\nEpoch 10/200, Loss: 0.6819, Train Accuracy: 0.7984\nEpoch 20/200, Loss: 0.4140, Train Accuracy: 0.8492\nEpoch 30/200, Loss: 0.3484, Train Accuracy: 0.8730\nEpoch 40/200, Loss: 0.3230, Train Accuracy: 0.8849\nEpoch 50/200, Loss: 0.3013, Train Accuracy: 0.8907\nEpoch 60/200, Loss: 0.2865, Train Accuracy: 0.8965\nEpoch 70/200, Loss: 0.2779, Train Accuracy: 0.8984\nEpoch 80/200, Loss: 0.2626, Train Accuracy: 0.9043\nEpoch 90/200, Loss: 0.2539, Train Accuracy: 0.9077\nEpoch 100/200, Loss: 0.2426, Train Accuracy: 0.9112\nEpoch 110/200, Loss: 0.2333, Train Accuracy: 0.9151\nEpoch 120/200, Loss: 0.2233, Train Accuracy: 0.9206\nEpoch 130/200, Loss: 0.2205, Train Accuracy: 0.9207\nEpoch 140/200, Loss: 0.2100, Train Accuracy: 0.9252\nEpoch 150/200, Loss: 0.1998, Train Accuracy: 0.9293\nEpoch 160/200, Loss: 0.1962, Train Accuracy: 0.9307\nEpoch 170/200, Loss: 0.1917, Train Accuracy: 0.9312\nEpoch 180/200, Loss: 0.1807, Train Accuracy: 0.9365\nEpoch 190/200, Loss: 0.1752, Train Accuracy: 0.9373\nEpoch 200/200, Loss: 0.1826, Train Accuracy: 0.9345\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}