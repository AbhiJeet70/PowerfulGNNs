{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ESAN_New",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/PowerfulGNNs/blob/main/ESAN_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torch-geometric\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx, from_networkx\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def load_dataset(name):\n",
        "    dataset = Planetoid(root=f\"./data/{name}\", name=name)\n",
        "    data = dataset[0]\n",
        "\n",
        "    # Ensure node features are initialized\n",
        "    if data.x is None:\n",
        "        num_nodes = data.num_nodes\n",
        "        data.x = torch.eye(num_nodes)  # One-hot encoding for nodes\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Define the ESAN Model\n",
        "class ESAN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ESAN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.shared_aggregator = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, subgraphs, num_nodes, batch_size=50):\n",
        "        # Initialize tensors to store node predictions and counts\n",
        "        device = next(self.parameters()).device\n",
        "        node_predictions = torch.zeros((num_nodes, self.shared_aggregator.out_features), device=device)\n",
        "        node_counts = torch.zeros(num_nodes, device=device)\n",
        "\n",
        "        # Process subgraphs in batches\n",
        "        for i in range(0, len(subgraphs), batch_size):\n",
        "            batch = subgraphs[i:i + batch_size]\n",
        "            for subgraph in batch:\n",
        "                x, edge_index = subgraph.x.to(device), subgraph.edge_index.to(device)\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "                x = self.conv2(x, edge_index)\n",
        "\n",
        "                # Map to output dimension (num_classes)\n",
        "                x = self.shared_aggregator(x)\n",
        "\n",
        "                # Aggregate features for nodes in the subgraph\n",
        "                node_predictions[subgraph.n_id] += x\n",
        "                node_counts[subgraph.n_id] += 1\n",
        "\n",
        "        # Average predictions for nodes that appear in multiple subgraphs\n",
        "        node_predictions = node_predictions / node_counts.unsqueeze(1).clamp(min=1)\n",
        "        return F.log_softmax(node_predictions, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# Train the ESAN model\n",
        "def train_model(model, subgraphs, data, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(subgraphs, data.num_nodes)  # Process subgraphs through the model\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
        "            train_acc = (out[data.train_mask].argmax(dim=1) == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Test the ESAN model\n",
        "def test_model(model, subgraphs, data):\n",
        "    model.eval()\n",
        "    logits = model(subgraphs, data.num_nodes)  # Process subgraphs through the model\n",
        "    accs = []\n",
        "    for mask_name, mask in zip([\"Train\", \"Validation\", \"Test\"], [data.train_mask, data.val_mask, data.test_mask]):\n",
        "        pred = logits[mask].argmax(dim=1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "        print(f\"{mask_name} Accuracy: {acc:.4f}\")\n",
        "    return accs\n",
        "\n",
        "def generate_subgraphs(data, policy=\"edge_deleted\", max_subgraphs=300):\n",
        "    graph = to_networkx(data, to_undirected=True)\n",
        "    subgraphs = []\n",
        "\n",
        "    if policy == \"edge_deleted\":\n",
        "        for i, edge in enumerate(graph.edges):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = graph.copy()\n",
        "            subgraph.remove_edge(*edge)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes))  # Map subgraph nodes to original graph nodes\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[pyg_subgraph.n_id]  # Use features from the original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"node_deleted\":\n",
        "        for i, node in enumerate(graph.nodes):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = graph.copy()\n",
        "            subgraph.remove_node(node)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes))  # Map subgraph nodes to original graph nodes\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[pyg_subgraph.n_id]  # Use features from the original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"ego\":\n",
        "        radius = 2\n",
        "        for i, node in enumerate(graph.nodes):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            # Generate ego graph for the node with the specified radius\n",
        "            subgraph = nx.ego_graph(graph, node, radius=radius)\n",
        "\n",
        "            # Convert the subgraph to PyTorch Geometric format\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "\n",
        "            # Add mapping of subgraph nodes to original graph nodes\n",
        "            pyg_subgraph.n_id = torch.tensor(list(subgraph.nodes))  # Map subgraph nodes\n",
        "\n",
        "            # Add central node feature\n",
        "            central_node_feature = torch.zeros(len(subgraph.nodes), 1)\n",
        "            central_node_idx = list(subgraph.nodes).index(node)  # Index of the central node\n",
        "            central_node_feature[central_node_idx] = 1\n",
        "\n",
        "            # Combine central node feature with original features\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[pyg_subgraph.n_id]  # Use original features from the graph\n",
        "            pyg_subgraph.x = torch.cat([pyg_subgraph.x, central_node_feature], dim=1)  # Add central node feature\n",
        "\n",
        "            # Normalize the features (optional)\n",
        "            pyg_subgraph.x = F.normalize(pyg_subgraph.x, p=2, dim=1)\n",
        "\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "\n",
        "    return subgraphs\n",
        "\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    datasets = [\"Cora\", \"CiteSeer\", \"PubMed\"]\n",
        "    policies = [\"edge_deleted\", \"node_deleted\", \"ego\"]\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        data = dataset[0]\n",
        "\n",
        "        for policy in policies:\n",
        "            print(f\"Processing policy: {policy} on dataset: {dataset_name}\")\n",
        "            subgraphs = generate_subgraphs(data, policy=policy, max_subgraphs=300)\n",
        "            print(f\"Generated {len(subgraphs)} subgraphs using {policy} policy\")\n",
        "\n",
        "\n",
        "            # Model parameters\n",
        "            input_dim = dataset.num_node_features + (1 if policy == \"ego\" else 0)\n",
        "            hidden_dim = 16\n",
        "            output_dim = dataset.num_classes\n",
        "\n",
        "            # Initialize model, optimizer\n",
        "            model = ESAN(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "            # Train and test the model\n",
        "            print(f\"Training on {dataset_name} with policy {policy}...\")\n",
        "            model = train_model(model, subgraphs, data, optimizer, epochs=50)\n",
        "\n",
        "            print(f\"Testing on {dataset_name} with policy {policy}...\")\n",
        "            test_model(model, subgraphs, data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-09T07:05:29.913104Z",
          "iopub.execute_input": "2025-01-09T07:05:29.913682Z",
          "iopub.status.idle": "2025-01-09T08:21:54.016118Z",
          "shell.execute_reply.started": "2025-01-09T07:05:29.913637Z",
          "shell.execute_reply": "2025-01-09T08:21:54.014477Z"
        },
        "id": "XH9VvKgCN6RN",
        "outputId": "947a1059-41c7-40ff-89a7-2361300f60b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: torch-geometric in /opt/conda/lib/python3.10/site-packages (2.6.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nProcessing policy: edge_deleted on dataset: Cora\nGenerated 300 subgraphs using edge_deleted policy\nTraining on Cora with policy edge_deleted...\nEpoch 10/50, Loss: 0.8448, Train Accuracy: 0.8357\nEpoch 20/50, Loss: 0.0806, Train Accuracy: 1.0000\nEpoch 30/50, Loss: 0.0058, Train Accuracy: 1.0000\nEpoch 40/50, Loss: 0.0020, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0023, Train Accuracy: 1.0000\nTesting on Cora with policy edge_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7540\nTest Accuracy: 0.7810\nProcessing policy: node_deleted on dataset: Cora\nGenerated 300 subgraphs using node_deleted policy\nTraining on Cora with policy node_deleted...\nEpoch 10/50, Loss: 0.8650, Train Accuracy: 0.9429\nEpoch 20/50, Loss: 0.0778, Train Accuracy: 1.0000\nEpoch 30/50, Loss: 0.0064, Train Accuracy: 1.0000\nEpoch 40/50, Loss: 0.0024, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0025, Train Accuracy: 1.0000\nTesting on Cora with policy node_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7520\nTest Accuracy: 0.7680\nProcessing policy: ego on dataset: Cora\nGenerated 300 subgraphs using ego policy\nTraining on Cora with policy ego...\nEpoch 10/50, Loss: 1.6776, Train Accuracy: 0.7429\nEpoch 20/50, Loss: 0.8921, Train Accuracy: 0.9214\nEpoch 30/50, Loss: 0.3112, Train Accuracy: 0.9571\nEpoch 40/50, Loss: 0.0990, Train Accuracy: 0.9786\nEpoch 50/50, Loss: 0.0915, Train Accuracy: 0.9714\nTesting on Cora with policy ego...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6280\nTest Accuracy: 0.6150\nProcessing policy: edge_deleted on dataset: CiteSeer\nGenerated 300 subgraphs using edge_deleted policy\nTraining on CiteSeer with policy edge_deleted...\nEpoch 10/50, Loss: 0.3370, Train Accuracy: 0.9667\nEpoch 20/50, Loss: 0.0153, Train Accuracy: 1.0000\nEpoch 30/50, Loss: 0.0026, Train Accuracy: 1.0000\nEpoch 40/50, Loss: 0.0021, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0035, Train Accuracy: 1.0000\nTesting on CiteSeer with policy edge_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6240\nTest Accuracy: 0.6200\nProcessing policy: node_deleted on dataset: CiteSeer\nGenerated 300 subgraphs using node_deleted policy\nTraining on CiteSeer with policy node_deleted...\nEpoch 10/50, Loss: 0.2770, Train Accuracy: 0.9833\nEpoch 20/50, Loss: 0.0091, Train Accuracy: 1.0000\nEpoch 30/50, Loss: 0.0016, Train Accuracy: 1.0000\nEpoch 40/50, Loss: 0.0013, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0025, Train Accuracy: 1.0000\nTesting on CiteSeer with policy node_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6940\nTest Accuracy: 0.6650\nProcessing policy: ego on dataset: CiteSeer\nGenerated 300 subgraphs using ego policy\nTraining on CiteSeer with policy ego...\nEpoch 10/50, Loss: 1.3097, Train Accuracy: 0.7500\nEpoch 20/50, Loss: 0.5185, Train Accuracy: 0.9500\nEpoch 30/50, Loss: 0.1337, Train Accuracy: 0.9667\nEpoch 40/50, Loss: 0.0519, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0356, Train Accuracy: 1.0000\nTesting on CiteSeer with policy ego...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.4760\nTest Accuracy: 0.3900\nProcessing policy: edge_deleted on dataset: PubMed\nGenerated 300 subgraphs using edge_deleted policy\nTraining on PubMed with policy edge_deleted...\nEpoch 10/50, Loss: 0.9300, Train Accuracy: 0.9333\nEpoch 20/50, Loss: 0.4947, Train Accuracy: 0.9667\nEpoch 30/50, Loss: 0.1694, Train Accuracy: 1.0000\nEpoch 40/50, Loss: 0.0366, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0147, Train Accuracy: 1.0000\nTesting on PubMed with policy edge_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7780\nTest Accuracy: 0.7510\nProcessing policy: node_deleted on dataset: PubMed\nGenerated 300 subgraphs using node_deleted policy\nTraining on PubMed with policy node_deleted...\nEpoch 10/50, Loss: 0.9181, Train Accuracy: 0.6500\nEpoch 20/50, Loss: 0.5489, Train Accuracy: 0.8000\nEpoch 30/50, Loss: 0.2532, Train Accuracy: 0.9833\nEpoch 40/50, Loss: 0.0666, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0171, Train Accuracy: 1.0000\nTesting on PubMed with policy node_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7860\nTest Accuracy: 0.7700\nProcessing policy: ego on dataset: PubMed\nGenerated 300 subgraphs using ego policy\nTraining on PubMed with policy ego...\nEpoch 10/50, Loss: 0.8307, Train Accuracy: 0.8000\nEpoch 20/50, Loss: 0.3162, Train Accuracy: 0.9167\nEpoch 30/50, Loss: 0.1022, Train Accuracy: 0.9667\nEpoch 40/50, Loss: 0.0232, Train Accuracy: 1.0000\nEpoch 50/50, Loss: 0.0130, Train Accuracy: 1.0000\nTesting on PubMed with policy ego...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6280\nTest Accuracy: 0.4720\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}