{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "ESAN_New",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/PowerfulGNNs/blob/main/ESAN_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx, from_networkx\n",
        "\n",
        "# Necessary installs instructions\n",
        "# To run this script, ensure the following packages are installed:\n",
        "# pip install torch torchvision torchaudio\n",
        "# pip install torch-geometric\n",
        "# pip install torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
        "# pip install networkx\n",
        "\n",
        "# Load Planetoid datasets (Cora, PubMed, Citeseer)\n",
        "def load_dataset(name):\n",
        "    dataset = Planetoid(root=f\"./data/{name}\", name=name)\n",
        "    data = dataset[0]\n",
        "\n",
        "    # Ensure node features are initialized\n",
        "    if data.x is None:\n",
        "        num_nodes = data.num_nodes\n",
        "        data.x = torch.eye(num_nodes)  # One-hot encoding for nodes\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Define the ESAN Model\n",
        "class ESAN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ESAN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.shared_aggregator = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Node-level prediction\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Subgraph Selection Policies\n",
        "def generate_subgraphs(data, policy=\"edge_deleted\", max_subgraphs=500):\n",
        "    graph = to_networkx(data, to_undirected=True)\n",
        "    subgraphs = []\n",
        "\n",
        "    if policy == \"edge_deleted\":\n",
        "        for i, edge in enumerate(graph.edges):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = graph.copy()\n",
        "            subgraph.remove_edge(*edge)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[:pyg_subgraph.num_nodes]  # Copy features from original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"node_deleted\":\n",
        "        for i, node in enumerate(graph.nodes):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = graph.copy()\n",
        "            subgraph.remove_node(node)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[:pyg_subgraph.num_nodes]  # Copy features from original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    elif policy == \"ego\":\n",
        "        for i, node in enumerate(graph.nodes):\n",
        "            if len(subgraphs) >= max_subgraphs:\n",
        "                break\n",
        "            subgraph = nx.ego_graph(graph, node, radius=1)\n",
        "            pyg_subgraph = from_networkx(subgraph)\n",
        "            if pyg_subgraph.x is None:\n",
        "                pyg_subgraph.x = data.x[:pyg_subgraph.num_nodes]  # Copy features from original graph\n",
        "            subgraphs.append(pyg_subgraph)\n",
        "\n",
        "    return subgraphs\n",
        "\n",
        "# Train the ESAN model\n",
        "def train_model(model, data, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
        "            train_acc = (out[data.train_mask].argmax(dim=1) == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Test the ESAN model\n",
        "def test_model(model, data):\n",
        "    model.eval()\n",
        "    logits = model(data)\n",
        "    accs = []\n",
        "    for mask_name, mask in zip([\"Train\", \"Validation\", \"Test\"], [data.train_mask, data.val_mask, data.test_mask]):\n",
        "        pred = logits[mask].argmax(dim=1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "        print(f\"{mask_name} Accuracy: {acc:.4f}\")\n",
        "    return accs\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    datasets = [\"Cora\", \"CiteSeer\", \"PubMed\"]\n",
        "    policies = [\"edge_deleted\", \"node_deleted\", \"ego\"]\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset = load_dataset(dataset_name)\n",
        "        data = dataset[0]\n",
        "\n",
        "        for policy in policies:\n",
        "            print(f\"Processing policy: {policy} on dataset: {dataset_name}\")\n",
        "            subgraphs = generate_subgraphs(data, policy=policy, max_subgraphs=500)\n",
        "            print(f\"Generated {len(subgraphs)} subgraphs using {policy} policy\")\n",
        "\n",
        "            # Model parameters\n",
        "            input_dim = dataset.num_node_features\n",
        "            hidden_dim = 16\n",
        "            output_dim = dataset.num_classes\n",
        "\n",
        "            # Initialize model, optimizer\n",
        "            model = ESAN(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "            # Train and test the model\n",
        "            print(f\"Training on {dataset_name} with policy {policy}...\")\n",
        "            model = train_model(model, data, optimizer, epochs=50)\n",
        "\n",
        "            print(f\"Testing on {dataset_name} with policy {policy}...\")\n",
        "            train_acc, val_acc, test_acc = test_model(model, data)\n",
        "\n",
        "            # Print results immediately\n",
        "            print(f\"Results for {dataset_name} with policy {policy}:\")\n",
        "            print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "            print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "            print(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-12T17:27:12.608375Z",
          "iopub.execute_input": "2024-12-12T17:27:12.608726Z",
          "iopub.status.idle": "2024-12-12T18:02:19.974737Z",
          "shell.execute_reply.started": "2024-12-12T17:27:12.608698Z",
          "shell.execute_reply": "2024-12-12T18:02:19.97379Z"
        },
        "id": "w8l-a_r7jeez",
        "outputId": "2d6f6753-5e75-4fda-c7d8-f434434acdab"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Processing policy: edge_deleted on dataset: Cora\nGenerated 500 subgraphs using edge_deleted policy\nTraining on Cora with policy edge_deleted...\nEpoch 10/50, Loss: 1.2796, Train Accuracy: 0.8000\nEpoch 20/50, Loss: 0.4616, Train Accuracy: 0.9286\nEpoch 30/50, Loss: 0.1704, Train Accuracy: 0.9857\nEpoch 40/50, Loss: 0.1003, Train Accuracy: 0.9857\nEpoch 50/50, Loss: 0.0893, Train Accuracy: 1.0000\nTesting on Cora with policy edge_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7660\nTest Accuracy: 0.7850\nResults for Cora with policy edge_deleted:\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7660\nTest Accuracy: 0.7850\n\nProcessing policy: node_deleted on dataset: Cora\nGenerated 500 subgraphs using node_deleted policy\nTraining on Cora with policy node_deleted...\nEpoch 10/50, Loss: 1.1993, Train Accuracy: 0.7214\nEpoch 20/50, Loss: 0.4902, Train Accuracy: 0.9214\nEpoch 30/50, Loss: 0.1949, Train Accuracy: 0.9571\nEpoch 40/50, Loss: 0.0877, Train Accuracy: 0.9929\nEpoch 50/50, Loss: 0.0702, Train Accuracy: 1.0000\nTesting on Cora with policy node_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7760\nTest Accuracy: 0.7880\nResults for Cora with policy node_deleted:\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7760\nTest Accuracy: 0.7880\n\nProcessing policy: ego on dataset: Cora\nGenerated 500 subgraphs using ego policy\nTraining on Cora with policy ego...\nEpoch 10/50, Loss: 1.1906, Train Accuracy: 0.7714\nEpoch 20/50, Loss: 0.3900, Train Accuracy: 0.9714\nEpoch 30/50, Loss: 0.1617, Train Accuracy: 0.9857\nEpoch 40/50, Loss: 0.0741, Train Accuracy: 0.9929\nEpoch 50/50, Loss: 0.0627, Train Accuracy: 0.9929\nTesting on Cora with policy ego...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7880\nTest Accuracy: 0.8000\nResults for Cora with policy ego:\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.7880\nTest Accuracy: 0.8000\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Processing policy: edge_deleted on dataset: CiteSeer\nGenerated 500 subgraphs using edge_deleted policy\nTraining on CiteSeer with policy edge_deleted...\nEpoch 10/50, Loss: 0.6200, Train Accuracy: 0.8917\nEpoch 20/50, Loss: 0.1684, Train Accuracy: 0.9500\nEpoch 30/50, Loss: 0.0948, Train Accuracy: 0.9667\nEpoch 40/50, Loss: 0.0586, Train Accuracy: 0.9833\nEpoch 50/50, Loss: 0.0459, Train Accuracy: 1.0000\nTesting on CiteSeer with policy edge_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6800\nTest Accuracy: 0.6720\nResults for CiteSeer with policy edge_deleted:\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6800\nTest Accuracy: 0.6720\n\nProcessing policy: node_deleted on dataset: CiteSeer\nGenerated 500 subgraphs using node_deleted policy\nTraining on CiteSeer with policy node_deleted...\nEpoch 10/50, Loss: 0.8433, Train Accuracy: 0.7917\nEpoch 20/50, Loss: 0.3272, Train Accuracy: 0.9417\nEpoch 30/50, Loss: 0.1383, Train Accuracy: 0.9750\nEpoch 40/50, Loss: 0.0762, Train Accuracy: 0.9833\nEpoch 50/50, Loss: 0.0539, Train Accuracy: 0.9917\nTesting on CiteSeer with policy node_deleted...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6560\nTest Accuracy: 0.6570\nResults for CiteSeer with policy node_deleted:\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6560\nTest Accuracy: 0.6570\n\nProcessing policy: ego on dataset: CiteSeer\nGenerated 500 subgraphs using ego policy\nTraining on CiteSeer with policy ego...\nEpoch 10/50, Loss: 0.9036, Train Accuracy: 0.7583\nEpoch 20/50, Loss: 0.2590, Train Accuracy: 0.9417\nEpoch 30/50, Loss: 0.1163, Train Accuracy: 0.9667\nEpoch 40/50, Loss: 0.0584, Train Accuracy: 0.9833\nEpoch 50/50, Loss: 0.0796, Train Accuracy: 0.9917\nTesting on CiteSeer with policy ego...\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6640\nTest Accuracy: 0.6630\nResults for CiteSeer with policy ego:\nTrain Accuracy: 1.0000\nValidation Accuracy: 0.6640\nTest Accuracy: 0.6630\n\nProcessing policy: edge_deleted on dataset: PubMed\nGenerated 500 subgraphs using edge_deleted policy\nTraining on PubMed with policy edge_deleted...\nEpoch 10/50, Loss: 2.2975, Train Accuracy: 0.5667\nEpoch 20/50, Loss: 1.4689, Train Accuracy: 0.5333\nEpoch 30/50, Loss: 1.0922, Train Accuracy: 0.6167\nEpoch 40/50, Loss: 0.8637, Train Accuracy: 0.7667\nEpoch 50/50, Loss: 0.7149, Train Accuracy: 0.8167\nTesting on PubMed with policy edge_deleted...\nTrain Accuracy: 0.9333\nValidation Accuracy: 0.7380\nTest Accuracy: 0.7260\nResults for PubMed with policy edge_deleted:\nTrain Accuracy: 0.9333\nValidation Accuracy: 0.7380\nTest Accuracy: 0.7260\n\nProcessing policy: node_deleted on dataset: PubMed\nGenerated 500 subgraphs using node_deleted policy\nTraining on PubMed with policy node_deleted...\nEpoch 10/50, Loss: 2.1260, Train Accuracy: 0.4833\nEpoch 20/50, Loss: 1.3109, Train Accuracy: 0.4833\nEpoch 30/50, Loss: 0.9622, Train Accuracy: 0.7000\nEpoch 40/50, Loss: 0.7567, Train Accuracy: 0.7667\nEpoch 50/50, Loss: 0.6366, Train Accuracy: 0.9167\nTesting on PubMed with policy node_deleted...\nTrain Accuracy: 0.9667\nValidation Accuracy: 0.7460\nTest Accuracy: 0.7000\nResults for PubMed with policy node_deleted:\nTrain Accuracy: 0.9667\nValidation Accuracy: 0.7460\nTest Accuracy: 0.7000\n\nProcessing policy: ego on dataset: PubMed\nGenerated 500 subgraphs using ego policy\nTraining on PubMed with policy ego...\nEpoch 10/50, Loss: 2.1020, Train Accuracy: 0.5833\nEpoch 20/50, Loss: 1.2893, Train Accuracy: 0.5833\nEpoch 30/50, Loss: 0.9190, Train Accuracy: 0.6833\nEpoch 40/50, Loss: 0.7498, Train Accuracy: 0.7667\nEpoch 50/50, Loss: 0.5675, Train Accuracy: 0.8167\nTesting on PubMed with policy ego...\nTrain Accuracy: 0.9667\nValidation Accuracy: 0.7520\nTest Accuracy: 0.7140\nResults for PubMed with policy ego:\nTrain Accuracy: 0.9667\nValidation Accuracy: 0.7520\nTest Accuracy: 0.7140\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}