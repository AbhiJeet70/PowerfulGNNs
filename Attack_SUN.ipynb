{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30823,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Attack_SUN",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiJeet70/PowerfulGNNs/blob/main/Attack_SUN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import subgraph\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import networkx as nx\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, Flickr\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import networkx as nx\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.nn.models.autoencoder import GAE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name in [\"Cora\", \"PubMed\", \"CiteSeer\"]:\n",
        "        dataset = Planetoid(root=f\"./data/{dataset_name}\", name=dataset_name)\n",
        "    elif dataset_name == \"Flickr\":\n",
        "        dataset = Flickr(root=\"./data/Flickr\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Split dataset into train/validation/test\n",
        "# Updated to randomly mask out 20% of nodes, use 10% for labeled nodes, and 10% for validation\n",
        "def split_dataset(data, test_size=0.2, val_size=0.1):\n",
        "    num_nodes = data.num_nodes\n",
        "    indices = np.arange(num_nodes)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    num_test = int(test_size * num_nodes)\n",
        "    num_val = int(val_size * num_nodes)\n",
        "    num_train = num_nodes - num_test - num_val\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    val_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train + num_val]] = True\n",
        "    test_mask[indices[num_train + num_val:]] = True\n",
        "\n",
        "    data.train_mask = train_mask\n",
        "    data.val_mask = val_mask\n",
        "    data.test_mask = test_mask\n",
        "\n",
        "    # Mask out 20% nodes for attack performance evaluation (half target, half clean test)\n",
        "    num_target = int(0.1 * num_nodes)  # Half of 20%\n",
        "    target_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    clean_test_mask = torch.zeros(num_nodes, dtype=torch.bool).to(device)\n",
        "    target_mask[indices[num_train + num_val:num_train + num_val + num_target]] = True\n",
        "    clean_test_mask[indices[num_train + num_val + num_target:]] = True\n",
        "\n",
        "    data.target_mask = target_mask\n",
        "    data.clean_test_mask = clean_test_mask\n",
        "\n",
        "    return data\n",
        "\n",
        "# Define GNN Model with multiple architectures (GCN, GraphSAGE, GAT)\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, model_type='GCN'):\n",
        "        super(GNN, self).__init__()\n",
        "        if model_type == 'GCN':\n",
        "            self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "            self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "        elif model_type == 'GraphSage':\n",
        "            self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "            self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "        elif model_type == 'GAT':\n",
        "            self.conv1 = GATConv(input_dim, hidden_dim, heads=8, concat=True)\n",
        "            self.conv2 = GATConv(hidden_dim * 8, output_dim, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Select nodes to poison based on high-centrality (degree centrality) for a stronger impact\n",
        "def select_high_centrality_nodes(data, num_nodes_to_select):\n",
        "    graph = nx.Graph()\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    graph.add_edges_from(edge_index.T)\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    sorted_nodes = sorted(centrality, key=centrality.get, reverse=True)\n",
        "    return torch.tensor(sorted_nodes[:num_nodes_to_select], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "class TriggerGenerator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(TriggerGenerator, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),  # Reduce dimensions\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, input_dim)  # Restore to input_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class GAE(torch.nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        return self.encoder(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "class OODDetector(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(OODDetector, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.gae = GAE(self.encoder, self.decoder)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        return z\n",
        "\n",
        "    def reconstruct(self, z, edge_index):\n",
        "        return self.decoder(z, edge_index)\n",
        "\n",
        "    def reconstruction_loss(self, x, edge_index):\n",
        "        z = self.gae.encode(x, edge_index)\n",
        "        reconstructed = self.reconstruct(z, edge_index)\n",
        "        return F.mse_loss(reconstructed, x)\n",
        "\n",
        "    def detect_ood(self, x, edge_index, threshold):\n",
        "        loss = self.reconstruction_loss(x, edge_index)\n",
        "        return loss > threshold\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        z = F.relu(self.conv1(x, edge_index))\n",
        "        z = self.conv2(z, edge_index)\n",
        "        return z\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = GCNConv(latent_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        x = F.relu(self.conv1(z, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def train_ood_detector(ood_detector, data, optimizer, epochs=50):\n",
        "    ood_detector.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        z = ood_detector(data.x, data.edge_index)\n",
        "\n",
        "        # Reconstruct data using the latent embedding\n",
        "        reconstructed_x = ood_detector.reconstruct(z, data.edge_index)\n",
        "\n",
        "        # Use only the training mask to compute reconstruction loss\n",
        "        loss = F.mse_loss(reconstructed_x[data.train_mask], data.x[data.train_mask])\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Print loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Reconstruction Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_with_poisoned_data(model, data, optimizer, poisoned_nodes, trigger_gen, attack, ood_detector=None, alpha=0.7, early_stopping=False):\n",
        "    # Apply trigger injection\n",
        "    data_poisoned = inject_trigger(data, poisoned_nodes, attack, trigger_gen, ood_detector, alpha)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data_poisoned.x, data_poisoned.edge_index)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.cross_entropy(out[data_poisoned.train_mask], data_poisoned.y[data_poisoned.train_mask])\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()  # No retain_graph=True unless explicitly required\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional: Print loss during training for insight\n",
        "        if early_stopping and epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, data_poisoned\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "def select_diverse_nodes(data, num_nodes_to_select, num_clusters=None):\n",
        "    if num_clusters is None:\n",
        "        num_clusters = len(torch.unique(data.y))\n",
        "\n",
        "    device = data.x.device  # Get the current device (CPU or GPU)\n",
        "\n",
        "    encoder = GCNEncoder(data.num_features, out_channels=16).to(device)  # Ensure model is on the correct device\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = encoder(data.x.to(device), data.edge_index.to(device)).cpu().numpy()\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(embeddings)\n",
        "    labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    selected_nodes = []\n",
        "    for i in range(num_clusters):\n",
        "        cluster_indices = np.where(labels == i)[0]\n",
        "        center = cluster_centers[i]\n",
        "        distances = np.linalg.norm(embeddings[cluster_indices] - center, axis=1)\n",
        "        closest_node = cluster_indices[np.argmin(distances)]\n",
        "        selected_nodes.append(closest_node)\n",
        "\n",
        "    degree = torch.bincount(data.edge_index[0]).to(device)  # Ensure degree is on the correct device\n",
        "    high_degree_nodes = torch.topk(degree, len(selected_nodes) // 2).indices.to(device)\n",
        "\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    betweenness_centrality = nx.betweenness_centrality(G)\n",
        "    central_nodes = sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)\n",
        "    central_nodes_tensor = torch.tensor(central_nodes[:len(selected_nodes) // 2], dtype=torch.long).to(device)\n",
        "\n",
        "    combined_nodes = torch.cat([torch.tensor(selected_nodes, device=device), high_degree_nodes, central_nodes_tensor])\n",
        "    unique_nodes = torch.unique(combined_nodes)[:num_nodes_to_select]\n",
        "\n",
        "    return unique_nodes.to(device)\n",
        "\n",
        "\n",
        "def inject_trigger(data, poisoned_nodes, attack_type, model, trigger_gen=None, ood_detector=None,\n",
        "                   alpha=0.7, trigger_size=5, trigger_density=0.5, input_dim=None):\n",
        "    # Clone data to avoid overwriting the original graph\n",
        "    data_poisoned = data.clone()\n",
        "    device = data_poisoned.x.device\n",
        "\n",
        "    if len(poisoned_nodes) == 0:\n",
        "        raise ValueError(\"No poisoned nodes selected. Ensure 'poisoned_nodes' is populated and non-empty.\")\n",
        "\n",
        "    # Adjust trigger_size if it exceeds the number of poisoned nodes\n",
        "    trigger_size = min(trigger_size, len(poisoned_nodes))\n",
        "\n",
        "    # Initialize target labels as a copy of the original labels for the poisoned nodes.\n",
        "    target_labels = data.y[poisoned_nodes].clone()\n",
        "\n",
        "    if attack_type == 'SBA-Samp':\n",
        "        # Subgraph-Based Attack - Random Sampling\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes[:trigger_size]]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        natural_features = avg_features + torch.randn_like(avg_features) * 0.02  # Small randomness\n",
        "\n",
        "        # Generate subgraph with realistic density\n",
        "        G = nx.erdos_renyi_graph(trigger_size, trigger_density)\n",
        "        trigger_edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
        "\n",
        "        # Connect poisoned nodes to the subgraph\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        # Update graph structure and features\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index.to(device),\n",
        "                                              poisoned_edges.to(device)], dim=1)\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = natural_features[:trigger_size]\n",
        "\n",
        "        # Modify target labels - Random misclassification\n",
        "        num_classes = data.y.max().item() + 1\n",
        "        for i in range(len(poisoned_nodes)):\n",
        "            original_label = data.y[poisoned_nodes[i]].item()\n",
        "            possible_labels = list(set(range(num_classes)) - {original_label})\n",
        "            target_labels[i] = random.choice(possible_labels)\n",
        "\n",
        "    elif attack_type == 'SBA-Gen':\n",
        "        # Subgraph-Based Attack - Gaussian\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes[:trigger_size]]\n",
        "\n",
        "        # Calculate mean and standard deviation of features\n",
        "        feature_mean = data.x.mean(dim=0)\n",
        "        feature_std = data.x.std(dim=0)\n",
        "\n",
        "        # Generate natural features with Gaussian noise\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else feature_mean for nodes in connected_nodes\n",
        "        ])\n",
        "        natural_features = avg_features + torch.normal(mean=0.0, std=0.03, size=avg_features.shape).to(data.x.device)\n",
        "\n",
        "        # Generate subgraph edges based on Gaussian similarity\n",
        "        trigger_edge_index = []\n",
        "        for i in range(trigger_size):\n",
        "            for j in range(i + 1, trigger_size):\n",
        "                similarity = torch.exp(-torch.norm((natural_features[i] - natural_features[j]) / feature_std)**2)\n",
        "                if similarity > torch.rand(1).item():\n",
        "                    trigger_edge_index.append([i, j])\n",
        "        if trigger_edge_index:\n",
        "            trigger_edge_index = torch.tensor(trigger_edge_index, dtype=torch.long).t().contiguous()\n",
        "            trigger_edge_index += poisoned_nodes[:trigger_size].unsqueeze(0)\n",
        "        else:\n",
        "            trigger_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
        "\n",
        "        poisoned_edges = torch.stack([\n",
        "            poisoned_nodes[:trigger_size],\n",
        "            torch.randint(0, data.num_nodes, (trigger_size,), device=device)\n",
        "        ])\n",
        "\n",
        "        data_poisoned.edge_index = torch.cat([data.edge_index, trigger_edge_index.to(device),\n",
        "                                              poisoned_edges.to(device)], dim=1)\n",
        "        data_poisoned.x[poisoned_nodes[:trigger_size]] = natural_features[:trigger_size]\n",
        "\n",
        "        num_classes = data.y.max().item() + 1\n",
        "        for i in range(len(poisoned_nodes)):\n",
        "            original_label = data.y[poisoned_nodes[i]].item()\n",
        "            possible_labels = list(set(range(num_classes)) - {original_label})\n",
        "            target_labels[i] = random.choice(possible_labels)\n",
        "\n",
        "    elif attack_type == 'DPGBA':\n",
        "        # DPGBA Attack: Distribution-Preserving Graph Backdoor Attack\n",
        "        connected_nodes = [data.edge_index[0][data.edge_index[1] == node] for node in poisoned_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data.x[nodes].mean(dim=0) if len(nodes) > 0 else data.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ]).to(device)\n",
        "\n",
        "        if trigger_gen is None:\n",
        "            raise ValueError(\"Trigger generator is required for the DPGBA attack.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            trigger_features = trigger_gen(avg_features)\n",
        "\n",
        "        if trigger_features.shape[1] != data.x.shape[1]:\n",
        "            raise ValueError(f\"Trigger feature dimension mismatch: {trigger_features.shape[1]} vs {data.x.shape[1]}\")\n",
        "\n",
        "        node_alphas = torch.rand(len(poisoned_nodes)).to(device) * 0.3 + 0.5\n",
        "        distribution_preserved_features = (\n",
        "            node_alphas.unsqueeze(1) * data.x[poisoned_nodes] +\n",
        "            (1 - node_alphas.unsqueeze(1)) * trigger_features\n",
        "        )\n",
        "\n",
        "        num_classes = data.y.max().item() + 1\n",
        "        target_labels = (data.y[poisoned_nodes] + 1) % num_classes\n",
        "        data_poisoned.x[poisoned_nodes] = distribution_preserved_features\n",
        "\n",
        "    elif attack_type == 'GTA':\n",
        "        # Graph Trojan Attack with Bi-Level Optimization\n",
        "        target_class = 0  # Set desired target class\n",
        "        num_poisoned = len(poisoned_nodes)\n",
        "        # Initialize learnable trigger parameters (per poisoned node)\n",
        "        trigger_params = torch.zeros((num_poisoned, data.x.size(1)), device=device, requires_grad=True)\n",
        "        trigger_lr = 0.01\n",
        "        optimizer_trigger = torch.optim.Adam([trigger_params], lr=trigger_lr)\n",
        "        inner_steps = 50  # Number of optimization steps\n",
        "        alpha_fixed = 0.7\n",
        "\n",
        "        # Use the passed model for bi-level optimization\n",
        "        model.eval()  # Ensure the model is in evaluation mode.\n",
        "        for step in range(inner_steps):\n",
        "            optimizer_trigger.zero_grad()\n",
        "            blended_features = (1 - alpha_fixed) * data.x[poisoned_nodes] + alpha_fixed * trigger_params\n",
        "            x_modified = data.x.clone()\n",
        "            x_modified[poisoned_nodes] = blended_features\n",
        "            out_modified = model(x_modified, data.edge_index)\n",
        "            target_labels_fixed = torch.full((num_poisoned,), target_class, dtype=torch.long, device=device)\n",
        "            loss_backdoor = F.cross_entropy(out_modified[poisoned_nodes], target_labels_fixed)\n",
        "            loss_reg = 0.001 * torch.norm(trigger_params, p=2)\n",
        "            loss_total = loss_backdoor + loss_reg\n",
        "            loss_total.backward()\n",
        "            optimizer_trigger.step()\n",
        "            if step % 10 == 0:\n",
        "                print(f\"GTA Inner Loop Step {step}: Loss = {loss_total.item():.4f}\")\n",
        "        optimized_trigger = trigger_params.detach()\n",
        "        data_poisoned = data.clone()\n",
        "        data_poisoned.x[poisoned_nodes] = (1 - alpha_fixed) * data.x[poisoned_nodes] + alpha_fixed * optimized_trigger\n",
        "        target_labels = torch.full((num_poisoned,), target_class, dtype=torch.long, device=device)\n",
        "\n",
        "    elif attack_type == 'UGBA':\n",
        "        # Unnoticeable Graph Backdoor Attack\n",
        "        diverse_nodes = select_diverse_nodes(data_poisoned, len(poisoned_nodes)).to(device)\n",
        "        connected_nodes = [data_poisoned.edge_index[0][data_poisoned.edge_index[1] == node] for node in diverse_nodes]\n",
        "        avg_features = torch.stack([\n",
        "            data_poisoned.x[nodes].mean(dim=0) if len(nodes) > 0 else data_poisoned.x.mean(dim=0) for nodes in connected_nodes\n",
        "        ])\n",
        "        refined_trigger_features = avg_features + torch.normal(mean=2.0, std=0.5, size=avg_features.shape).to(data_poisoned.x.device)\n",
        "        data_poisoned.x[diverse_nodes] = refined_trigger_features.to(data_poisoned.x.device)\n",
        "        new_edges = []\n",
        "        for i in range(len(diverse_nodes)):\n",
        "            node = diverse_nodes[i]\n",
        "            neighbor = connected_nodes[i][0] if len(connected_nodes[i]) > 0 else diverse_nodes[(i + 1) % len(diverse_nodes)]\n",
        "            new_edges.append([node, neighbor])\n",
        "        new_edges = torch.tensor(new_edges, dtype=torch.long).t().contiguous().to(data_poisoned.x.device)\n",
        "        data_poisoned.edge_index = torch.cat([data_poisoned.edge_index, new_edges], dim=1)\n",
        "        target_labels[:] = 0\n",
        "\n",
        "    return data_poisoned, target_labels\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "\n",
        "def dominant_set_clustering(data, threshold=0.7, use_pca=True, pca_components=10):\n",
        "    \"\"\"\n",
        "    Applies a simplified outlier detection framework using a combination of K-Means clustering and distance-based heuristics.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - threshold: Quantile threshold for identifying outliers based on cluster distances.\n",
        "    - use_pca: Whether to use PCA for dimensionality reduction.\n",
        "    - pca_components: Number of PCA components to use if PCA is applied.\n",
        "\n",
        "    Returns:\n",
        "    - pruned_nodes: Set of nodes identified as outliers.\n",
        "    - data: Updated PyG data object with modified features and labels for outliers.\n",
        "    \"\"\"\n",
        "    # Step 1: Determine the number of clusters based on the number of classes\n",
        "    n_clusters = len(data.y.unique())  # Number of unique classes in the dataset\n",
        "\n",
        "    # Step 2: Dimensionality reduction using PCA (optional)\n",
        "    node_features = data.x.detach().cpu().numpy()\n",
        "    if use_pca and node_features.shape[1] > pca_components:\n",
        "        pca = PCA(n_components=pca_components)\n",
        "        node_features = pca.fit_transform(node_features)\n",
        "\n",
        "    # Step 3: K-Means Clustering to identify clusters and potential outliers\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(node_features)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculate distances to cluster centers\n",
        "    distances = np.linalg.norm(node_features - cluster_centers[cluster_labels], axis=1)\n",
        "\n",
        "    # Identify outlier candidates based on distance threshold\n",
        "    distance_threshold = np.percentile(distances, 100 * threshold)\n",
        "    outlier_candidates = np.where(distances > distance_threshold)[0]\n",
        "\n",
        "    # Step 4: Update data to reflect removal of outlier influence\n",
        "    pruned_nodes = set(outlier_candidates)\n",
        "    if len(pruned_nodes) > 0:\n",
        "        outliers = torch.tensor(list(pruned_nodes), dtype=torch.long, device=data.x.device)\n",
        "\n",
        "        # Assign an invalid label (-1) to outlier nodes to discard them during training\n",
        "        data.y[outliers] = -1\n",
        "\n",
        "        # Replace the features of outliers with the average feature value to reduce their impact\n",
        "        data.x[outliers] = data.x.mean(dim=0).to(data.x.device)\n",
        "\n",
        "    return pruned_nodes, data\n",
        "\n",
        "def defense_prune_edges(data, quantile_threshold=0.9):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity between node features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile to determine pruning threshold (e.g., 0.9 means pruning edges in the top 10% dissimilar).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Adaptive threshold based on quantile of similarity distribution\n",
        "    similarity_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Keep edges with cosine similarity above the threshold\n",
        "    pruned_mask = cosine_similarities >= similarity_threshold\n",
        "    pruned_edges = edge_index[:, pruned_mask]\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def defense_prune_and_discard_labels(data, quantile_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Prunes edges based on adaptive cosine similarity and discards labels of nodes connected by pruned edges selectively.\n",
        "\n",
        "    Parameters:\n",
        "    - data: PyG data object representing the graph.\n",
        "    - quantile_threshold: Quantile threshold for cosine similarity pruning (e.g., 0.2 means pruning edges in the bottom 20%).\n",
        "\n",
        "    Returns:\n",
        "    - data: Updated PyG data object with pruned edges and selectively discarded labels.\n",
        "    \"\"\"\n",
        "    features = data.x\n",
        "    norm_features = F.normalize(features, p=2, dim=1)  # Normalize features using PyTorch\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Calculate cosine similarity for each edge\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    cosine_similarities = torch.sum(norm_features[src] * norm_features[dst], dim=1)\n",
        "\n",
        "    # Use quantile to determine adaptive threshold for pruning\n",
        "    adaptive_threshold = torch.quantile(cosine_similarities, quantile_threshold).item()\n",
        "\n",
        "    # Mask edges with similarity below the adaptive threshold\n",
        "    pruned_mask = cosine_similarities < adaptive_threshold\n",
        "    pruned_edges = edge_index[:, ~pruned_mask]  # Retain edges that are above the threshold\n",
        "\n",
        "    # Update edge index with pruned edges\n",
        "    data.edge_index = pruned_edges\n",
        "\n",
        "    # Selectively discard labels of nodes connected by many pruned edges\n",
        "    pruned_src, pruned_dst = edge_index[:, pruned_mask]\n",
        "    pruned_nodes_count = torch.bincount(torch.cat([pruned_src, pruned_dst]), minlength=data.num_nodes)\n",
        "\n",
        "    # Only discard labels if the node has a high count of pruned edges\n",
        "    threshold_count = int(torch.median(pruned_nodes_count).item())  # Use median count as a threshold\n",
        "    nodes_to_discard = torch.where(pruned_nodes_count > threshold_count)[0]\n",
        "\n",
        "    data.y[nodes_to_discard] = -1  # Use -1 to represent discarded labels\n",
        "\n",
        "    return data\n",
        "\n",
        "# Compute ASR and Clean Accuracy (using .detach() to avoid retaining computation graph)\n",
        "def compute_metrics(model, data, poisoned_nodes):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index).detach()\n",
        "        _, pred = out.max(dim=1)\n",
        "        asr = (pred[poisoned_nodes] == data.y[poisoned_nodes]).sum().item() / len(poisoned_nodes) * 100\n",
        "        clean_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu()) * 100\n",
        "    return asr, clean_acc\n",
        "\n",
        "\n",
        "\n",
        "# Visualization Function\n",
        "# Visualize PCA for Attacks\n",
        "# Added function to visualize PCA projections of node embeddings for different attacks\n",
        "def visualize_pca_for_attacks(attack_embeddings_dict):\n",
        "    pca = PCA(n_components=2)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    for i, (attack, attack_data) in enumerate(attack_embeddings_dict.items(), 1):\n",
        "        embeddings = attack_data['data'].detach().cpu().numpy()\n",
        "        poisoned_nodes = attack_data['poisoned_nodes'].detach().cpu().numpy()\n",
        "\n",
        "        # Apply PCA to the node embeddings\n",
        "        pca_result = pca.fit_transform(embeddings)\n",
        "\n",
        "        # Create masks for clean and poisoned nodes\n",
        "        clean_mask = np.ones(embeddings.shape[0], dtype=bool)\n",
        "        clean_mask[poisoned_nodes] = False\n",
        "\n",
        "        # Extract clean and poisoned node embeddings after PCA\n",
        "        clean_embeddings = pca_result[clean_mask]\n",
        "        poisoned_embeddings = pca_result[~clean_mask]\n",
        "\n",
        "        # Plotting clean and poisoned nodes\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.scatter(clean_embeddings[:, 0], clean_embeddings[:, 1], s=10, alpha=0.5, label='Clean Nodes', c='b')\n",
        "        plt.scatter(poisoned_embeddings[:, 0], poisoned_embeddings[:, 1], s=10, alpha=0.8, label='Poisoned Nodes', c='r')\n",
        "        plt.title(f'PCA Visualization for {attack}')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import to_dense_adj, k_hop_subgraph\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SUNLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SUNLayer, self).__init__()\n",
        "        # Separate transformations for root and non-root nodes\n",
        "        self.root_mlp = nn.Linear(in_channels, out_channels)\n",
        "        self.non_root_mlp = nn.Linear(in_channels, out_channels)\n",
        "        self.global_mlp = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, subgraph_masks):\n",
        "        # Convert edge_index to dense adjacency matrix\n",
        "        adjacency_matrix = to_dense_adj(edge_index, max_num_nodes=x.size(0))[0]\n",
        "\n",
        "        # Local message passing within subgraphs\n",
        "        local_features = torch.matmul(adjacency_matrix, x)\n",
        "\n",
        "        # Global aggregation across subgraphs\n",
        "        global_features = self.global_mlp(torch.mean(x, dim=0, keepdim=True))\n",
        "        global_features = global_features.expand(x.size(0), global_features.size(1))  # Broadcast to match x's shape\n",
        "\n",
        "        # Initialize root and non-root features with correct shape\n",
        "        root_features = torch.zeros((x.size(0), global_features.size(1)), device=x.device)\n",
        "        non_root_features = torch.zeros((x.size(0), global_features.size(1)), device=x.device)\n",
        "\n",
        "        # Apply transformations to root nodes\n",
        "        root_features[subgraph_masks] = self.root_mlp(x[subgraph_masks])\n",
        "\n",
        "        # Apply transformations to non-root nodes\n",
        "        non_root_features = self.non_root_mlp(local_features)\n",
        "\n",
        "        # Combine root, non-root, and global updates\n",
        "        updated_features = root_features + non_root_features + global_features\n",
        "\n",
        "        return updated_features\n",
        "\n",
        "class SUN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, hidden_channels):\n",
        "        super(SUN, self).__init__()\n",
        "        self.layer1 = SUNLayer(num_features, hidden_channels)\n",
        "        self.layer2 = SUNLayer(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        num_nodes = x.size(0)\n",
        "        subgraph_masks = torch.zeros(num_nodes, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        # Example subgraph extraction: mark every node as root for simplicity\n",
        "        for i in range(num_nodes):\n",
        "            _, _, _, node_mask = k_hop_subgraph(i, 1, edge_index, relabel_nodes=False, num_nodes=num_nodes)\n",
        "            subgraph_masks[node_mask[:num_nodes]] = True\n",
        "\n",
        "        # Pass through SUN layers\n",
        "        x = self.layer1(x, edge_index, subgraph_masks)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x, edge_index, subgraph_masks)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def compute_metrics(model, data, poisoned_nodes, target_labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index).detach()\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "        # ASR: Percentage of poisoned nodes classified into their respective attack-determined target labels\n",
        "        asr = (pred[poisoned_nodes] == target_labels).sum().item() / len(poisoned_nodes) * 100\n",
        "\n",
        "        # Clean accuracy: Standard accuracy on clean test nodes\n",
        "        clean_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu()) * 100\n",
        "\n",
        "    return asr, clean_acc\n",
        "\n",
        "\n",
        "def run_all_attacks_sun():\n",
        "    datasets = [\"Cora\", \"CiteSeer\", \"PubMed\"]\n",
        "    results_summary = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        try:\n",
        "            print(f\"Starting process for dataset: {dataset_name}\")\n",
        "\n",
        "            # Load the dataset\n",
        "            dataset = load_dataset(dataset_name)\n",
        "            print(f\"Loaded dataset: {dataset_name}\")\n",
        "            data = dataset[0].to(device)\n",
        "            print(f\"Dataset {dataset_name} has {data.num_nodes} nodes and {data.num_edges} edges.\")\n",
        "\n",
        "            input_dim = data.num_features\n",
        "            output_dim = dataset.num_classes if isinstance(dataset.num_classes, int) else dataset.num_classes[0]\n",
        "            data = split_dataset(data)\n",
        "            print(f\"Dataset {dataset_name} split into train/val/test.\")\n",
        "\n",
        "            # Dataset-specific poisoning budgets\n",
        "            dataset_budgets = {'Cora': 10, 'PubMed': 40, 'CiteSeer': 30}\n",
        "            poisoned_node_budget = dataset_budgets.get(dataset_name, 10)\n",
        "\n",
        "            # Initialize SUN model and optimizer\n",
        "            print(f\"Initializing SUN for dataset: {dataset_name}\")\n",
        "            model = SUN(num_features=input_dim, num_classes=output_dim, hidden_channels=64).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "            # Train baseline model\n",
        "            print(f\"Training baseline model for dataset: {dataset_name}\")\n",
        "            model.train()\n",
        "            for epoch in range(200):\n",
        "                optimizer.zero_grad()\n",
        "                out = model(data.x, data.edge_index)\n",
        "                loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Dataset: {dataset_name}, Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "            # Evaluate baseline accuracy\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out = model(data.x, data.edge_index)\n",
        "                predictions = out.argmax(dim=1)\n",
        "                baseline_acc = (predictions[data.test_mask] == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
        "                print(f\"Dataset: {dataset_name}, Baseline Accuracy: {baseline_acc * 100:.2f}%\")\n",
        "                results_summary.append({\n",
        "                    \"Dataset\": dataset_name,\n",
        "                    \"Model\": \"SUN\",\n",
        "                    \"Attack\": \"None\",\n",
        "                    \"Defense\": \"None\",\n",
        "                    \"ASR\": \"N/A\",\n",
        "                    \"Clean Accuracy\": baseline_acc * 100\n",
        "                })\n",
        "\n",
        "            # Initialize Trigger Generator (needed for DPGBA)\n",
        "            print(f\"Initializing Trigger Generator for dataset: {dataset_name}\")\n",
        "            trigger_gen = TriggerGenerator(input_dim=input_dim, hidden_dim=64).to(device)\n",
        "\n",
        "            # Select nodes to poison\n",
        "            print(f\"Selecting poisoned nodes for dataset: {dataset_name}\")\n",
        "            poisoned_nodes = select_high_centrality_nodes(data, poisoned_node_budget).to(device)\n",
        "            print(f\"Selected {len(poisoned_nodes)} poisoned nodes.\")\n",
        "\n",
        "            # Define attack methods\n",
        "            attack_methods = ['SBA-Samp', 'SBA-Gen', 'GTA', 'UGBA', 'DPGBA']\n",
        "\n",
        "            # Loop over attack methods\n",
        "            for attack in attack_methods:\n",
        "                try:\n",
        "                    print(f\"Starting attack {attack} on dataset: {dataset_name}\")\n",
        "\n",
        "                    # For DPGBA, initialize and train the OOD detector immediately before the attack.\n",
        "                    if attack == 'DPGBA':\n",
        "                        print(\"Initializing OOD Detector for DPGBA\")\n",
        "                        ood_detector = OODDetector(input_dim=input_dim, hidden_dim=64, latent_dim=16).to(device)\n",
        "                        ood_optimizer = torch.optim.Adam(ood_detector.parameters(), lr=0.001)\n",
        "                        train_ood_detector(ood_detector, data, ood_optimizer, epochs=100)\n",
        "                        current_trigger_gen = trigger_gen  # Use the trigger generator for DPGBA\n",
        "                    else:\n",
        "                        ood_detector = None\n",
        "                        current_trigger_gen = None\n",
        "\n",
        "                    data_poisoned, target_labels = inject_trigger(\n",
        "                        data=data,\n",
        "                        poisoned_nodes=poisoned_nodes,\n",
        "                        attack_type=attack,\n",
        "                        model=model,  # Pass the current model here\n",
        "                        trigger_gen=current_trigger_gen,  # Only for DPGBA\n",
        "                        ood_detector=ood_detector,         # Only for DPGBA\n",
        "                        alpha=0.7,\n",
        "                        trigger_size=10,\n",
        "                        trigger_density=0.5\n",
        "                    )\n",
        "\n",
        "\n",
        "                    # Train the model on poisoned data\n",
        "                    model.train()\n",
        "                    for epoch in range(200):\n",
        "                        optimizer.zero_grad()\n",
        "                        out = model(data_poisoned.x, data_poisoned.edge_index)\n",
        "                        loss = F.cross_entropy(out[data_poisoned.train_mask], data_poisoned.y[data_poisoned.train_mask])\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        if epoch % 10 == 0:\n",
        "                            print(f\"Dataset: {dataset_name}, Attack: {attack}, Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                    # Evaluate ASR and clean accuracy using correct target labels\n",
        "                    asr, clean_acc = compute_metrics(model, data_poisoned, poisoned_nodes, target_labels)\n",
        "                    results_summary.append({\n",
        "                        \"Dataset\": dataset_name,\n",
        "                        \"Model\": \"SUN\",\n",
        "                        \"Attack\": attack,\n",
        "                        \"Defense\": \"None\",\n",
        "                        \"ASR\": asr,\n",
        "                        \"Clean Accuracy\": clean_acc\n",
        "                    })\n",
        "                    print(f\"Dataset: {dataset_name}, Attack: {attack} - ASR: {asr:.2f}%, Clean Accuracy: {clean_acc:.2f}%\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during attack {attack} on dataset {dataset_name}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with dataset {dataset_name}: {e}\")\n",
        "\n",
        "    # Save and display results\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nSummary of Results:\")\n",
        "    print(results_df)\n",
        "    results_df.to_csv(\"sun_backdoor_attack_results_summary.csv\", index=False)\n",
        "\n",
        "run_all_attacks_sun()\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-02T20:15:44.278897Z",
          "iopub.execute_input": "2025-02-02T20:15:44.279229Z",
          "iopub.status.idle": "2025-02-03T02:03:54.703425Z",
          "shell.execute_reply.started": "2025-02-02T20:15:44.279203Z",
          "shell.execute_reply": "2025-02-03T02:03:54.702234Z"
        },
        "id": "VA8Z80hFp-sF",
        "outputId": "28b054ba-9708-4e04-a45d-03f6a6feecc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nUsing device: cuda\nStarting process for dataset: Cora\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loaded dataset: Cora\nDataset Cora has 2708 nodes and 10556 edges.\nDataset Cora split into train/val/test.\nInitializing SUN for dataset: Cora\nTraining baseline model for dataset: Cora\nDataset: Cora, Epoch 0, Loss: 2.0277\nDataset: Cora, Epoch 10, Loss: 0.5944\nDataset: Cora, Epoch 20, Loss: 0.3298\nDataset: Cora, Epoch 30, Loss: 0.1908\nDataset: Cora, Epoch 40, Loss: 0.1105\nDataset: Cora, Epoch 50, Loss: 0.0634\nDataset: Cora, Epoch 60, Loss: 0.0378\nDataset: Cora, Epoch 70, Loss: 0.0241\nDataset: Cora, Epoch 80, Loss: 0.0165\nDataset: Cora, Epoch 90, Loss: 0.0121\nDataset: Cora, Epoch 100, Loss: 0.0093\nDataset: Cora, Epoch 110, Loss: 0.0075\nDataset: Cora, Epoch 120, Loss: 0.0061\nDataset: Cora, Epoch 130, Loss: 0.0052\nDataset: Cora, Epoch 140, Loss: 0.0044\nDataset: Cora, Epoch 150, Loss: 0.0038\nDataset: Cora, Epoch 160, Loss: 0.0034\nDataset: Cora, Epoch 170, Loss: 0.0030\nDataset: Cora, Epoch 180, Loss: 0.0027\nDataset: Cora, Epoch 190, Loss: 0.0024\nDataset: Cora, Baseline Accuracy: 87.62%\nInitializing Trigger Generator for dataset: Cora\nSelecting poisoned nodes for dataset: Cora\nSelected 10 poisoned nodes.\nStarting attack SBA-Samp on dataset: Cora\nDataset: Cora, Attack: SBA-Samp, Epoch 0, Loss: 0.0166\nDataset: Cora, Attack: SBA-Samp, Epoch 10, Loss: 0.0037\nDataset: Cora, Attack: SBA-Samp, Epoch 20, Loss: 0.0038\nDataset: Cora, Attack: SBA-Samp, Epoch 30, Loss: 0.0022\nDataset: Cora, Attack: SBA-Samp, Epoch 40, Loss: 0.0018\nDataset: Cora, Attack: SBA-Samp, Epoch 50, Loss: 0.0015\nDataset: Cora, Attack: SBA-Samp, Epoch 60, Loss: 0.0014\nDataset: Cora, Attack: SBA-Samp, Epoch 70, Loss: 0.0013\nDataset: Cora, Attack: SBA-Samp, Epoch 80, Loss: 0.0012\nDataset: Cora, Attack: SBA-Samp, Epoch 90, Loss: 0.0011\nDataset: Cora, Attack: SBA-Samp, Epoch 100, Loss: 0.0010\nDataset: Cora, Attack: SBA-Samp, Epoch 110, Loss: 0.0009\nDataset: Cora, Attack: SBA-Samp, Epoch 120, Loss: 0.0009\nDataset: Cora, Attack: SBA-Samp, Epoch 130, Loss: 0.0008\nDataset: Cora, Attack: SBA-Samp, Epoch 140, Loss: 0.0008\nDataset: Cora, Attack: SBA-Samp, Epoch 150, Loss: 0.0007\nDataset: Cora, Attack: SBA-Samp, Epoch 160, Loss: 0.0007\nDataset: Cora, Attack: SBA-Samp, Epoch 170, Loss: 0.0007\nDataset: Cora, Attack: SBA-Samp, Epoch 180, Loss: 0.0006\nDataset: Cora, Attack: SBA-Samp, Epoch 190, Loss: 0.0006\nDataset: Cora, Attack: SBA-Samp - ASR: 0.00%, Clean Accuracy: 87.62%\nStarting attack SBA-Gen on dataset: Cora\nDataset: Cora, Attack: SBA-Gen, Epoch 0, Loss: 0.0006\nDataset: Cora, Attack: SBA-Gen, Epoch 10, Loss: 0.0006\nDataset: Cora, Attack: SBA-Gen, Epoch 20, Loss: 0.0005\nDataset: Cora, Attack: SBA-Gen, Epoch 30, Loss: 0.0005\nDataset: Cora, Attack: SBA-Gen, Epoch 40, Loss: 0.0005\nDataset: Cora, Attack: SBA-Gen, Epoch 50, Loss: 0.0005\nDataset: Cora, Attack: SBA-Gen, Epoch 60, Loss: 0.0004\nDataset: Cora, Attack: SBA-Gen, Epoch 70, Loss: 0.0004\nDataset: Cora, Attack: SBA-Gen, Epoch 80, Loss: 0.0004\nDataset: Cora, Attack: SBA-Gen, Epoch 90, Loss: 0.0004\nDataset: Cora, Attack: SBA-Gen, Epoch 100, Loss: 0.0004\nDataset: Cora, Attack: SBA-Gen, Epoch 110, Loss: 0.0004\nDataset: Cora, Attack: SBA-Gen, Epoch 120, Loss: 0.0004\nDataset: Cora, Attack: SBA-Gen, Epoch 130, Loss: 0.0003\nDataset: Cora, Attack: SBA-Gen, Epoch 140, Loss: 0.0003\nDataset: Cora, Attack: SBA-Gen, Epoch 150, Loss: 0.0003\nDataset: Cora, Attack: SBA-Gen, Epoch 160, Loss: 0.0003\nDataset: Cora, Attack: SBA-Gen, Epoch 170, Loss: 0.0003\nDataset: Cora, Attack: SBA-Gen, Epoch 180, Loss: 0.0003\nDataset: Cora, Attack: SBA-Gen, Epoch 190, Loss: 0.0003\nDataset: Cora, Attack: SBA-Gen - ASR: 0.00%, Clean Accuracy: 87.43%\nStarting attack GTA on dataset: Cora\nGTA Inner Loop Step 0: Loss = 160.2761\nGTA Inner Loop Step 10: Loss = 0.0080\nGTA Inner Loop Step 20: Loss = 0.0106\nGTA Inner Loop Step 30: Loss = 0.0116\nGTA Inner Loop Step 40: Loss = 0.0120\nDataset: Cora, Attack: GTA, Epoch 0, Loss: 2.2973\nDataset: Cora, Attack: GTA, Epoch 10, Loss: 0.2621\nDataset: Cora, Attack: GTA, Epoch 20, Loss: 0.0939\nDataset: Cora, Attack: GTA, Epoch 30, Loss: 0.0232\nDataset: Cora, Attack: GTA, Epoch 40, Loss: 0.0058\nDataset: Cora, Attack: GTA, Epoch 50, Loss: 0.0031\nDataset: Cora, Attack: GTA, Epoch 60, Loss: 0.0022\nDataset: Cora, Attack: GTA, Epoch 70, Loss: 0.0017\nDataset: Cora, Attack: GTA, Epoch 80, Loss: 0.0014\nDataset: Cora, Attack: GTA, Epoch 90, Loss: 0.0012\nDataset: Cora, Attack: GTA, Epoch 100, Loss: 0.0011\nDataset: Cora, Attack: GTA, Epoch 110, Loss: 0.0010\nDataset: Cora, Attack: GTA, Epoch 120, Loss: 0.0009\nDataset: Cora, Attack: GTA, Epoch 130, Loss: 0.0008\nDataset: Cora, Attack: GTA, Epoch 140, Loss: 0.0008\nDataset: Cora, Attack: GTA, Epoch 150, Loss: 0.0007\nDataset: Cora, Attack: GTA, Epoch 160, Loss: 0.0007\nDataset: Cora, Attack: GTA, Epoch 170, Loss: 0.0007\nDataset: Cora, Attack: GTA, Epoch 180, Loss: 0.0006\nDataset: Cora, Attack: GTA, Epoch 190, Loss: 0.0006\nDataset: Cora, Attack: GTA - ASR: 20.00%, Clean Accuracy: 86.88%\nStarting attack UGBA on dataset: Cora\nDataset: Cora, Attack: UGBA, Epoch 0, Loss: 6.0280\nDataset: Cora, Attack: UGBA, Epoch 10, Loss: 0.6703\nDataset: Cora, Attack: UGBA, Epoch 20, Loss: 0.3210\nDataset: Cora, Attack: UGBA, Epoch 30, Loss: 0.1039\nDataset: Cora, Attack: UGBA, Epoch 40, Loss: 0.0370\nDataset: Cora, Attack: UGBA, Epoch 50, Loss: 0.0245\nDataset: Cora, Attack: UGBA, Epoch 60, Loss: 0.0168\nDataset: Cora, Attack: UGBA, Epoch 70, Loss: 0.0158\nDataset: Cora, Attack: UGBA, Epoch 80, Loss: 0.0135\nDataset: Cora, Attack: UGBA, Epoch 90, Loss: 0.0118\nDataset: Cora, Attack: UGBA, Epoch 100, Loss: 0.0109\nDataset: Cora, Attack: UGBA, Epoch 110, Loss: 0.0102\nDataset: Cora, Attack: UGBA, Epoch 120, Loss: 0.0096\nDataset: Cora, Attack: UGBA, Epoch 130, Loss: 0.0092\nDataset: Cora, Attack: UGBA, Epoch 140, Loss: 0.0087\nDataset: Cora, Attack: UGBA, Epoch 150, Loss: 0.0084\nDataset: Cora, Attack: UGBA, Epoch 160, Loss: 0.0080\nDataset: Cora, Attack: UGBA, Epoch 170, Loss: 0.0077\nDataset: Cora, Attack: UGBA, Epoch 180, Loss: 0.0074\nDataset: Cora, Attack: UGBA, Epoch 190, Loss: 0.0072\nDataset: Cora, Attack: UGBA - ASR: 20.00%, Clean Accuracy: 87.25%\nStarting attack DPGBA on dataset: Cora\nInitializing OOD Detector for DPGBA\nEpoch 0, Reconstruction Loss: 0.0128\nEpoch 10, Reconstruction Loss: 0.0124\nEpoch 20, Reconstruction Loss: 0.0121\nEpoch 30, Reconstruction Loss: 0.0120\nEpoch 40, Reconstruction Loss: 0.0119\nEpoch 50, Reconstruction Loss: 0.0118\nEpoch 60, Reconstruction Loss: 0.0117\nEpoch 70, Reconstruction Loss: 0.0115\nEpoch 80, Reconstruction Loss: 0.0114\nEpoch 90, Reconstruction Loss: 0.0113\nDataset: Cora, Attack: DPGBA, Epoch 0, Loss: 0.0146\nDataset: Cora, Attack: DPGBA, Epoch 10, Loss: 0.0041\nDataset: Cora, Attack: DPGBA, Epoch 20, Loss: 0.0031\nDataset: Cora, Attack: DPGBA, Epoch 30, Loss: 0.0023\nDataset: Cora, Attack: DPGBA, Epoch 40, Loss: 0.0018\nDataset: Cora, Attack: DPGBA, Epoch 50, Loss: 0.0016\nDataset: Cora, Attack: DPGBA, Epoch 60, Loss: 0.0014\nDataset: Cora, Attack: DPGBA, Epoch 70, Loss: 0.0013\nDataset: Cora, Attack: DPGBA, Epoch 80, Loss: 0.0012\nDataset: Cora, Attack: DPGBA, Epoch 90, Loss: 0.0011\nDataset: Cora, Attack: DPGBA, Epoch 100, Loss: 0.0010\nDataset: Cora, Attack: DPGBA, Epoch 110, Loss: 0.0010\nDataset: Cora, Attack: DPGBA, Epoch 120, Loss: 0.0009\nDataset: Cora, Attack: DPGBA, Epoch 130, Loss: 0.0009\nDataset: Cora, Attack: DPGBA, Epoch 140, Loss: 0.0008\nDataset: Cora, Attack: DPGBA, Epoch 150, Loss: 0.0008\nDataset: Cora, Attack: DPGBA, Epoch 160, Loss: 0.0008\nDataset: Cora, Attack: DPGBA, Epoch 170, Loss: 0.0007\nDataset: Cora, Attack: DPGBA, Epoch 180, Loss: 0.0007\nDataset: Cora, Attack: DPGBA, Epoch 190, Loss: 0.0007\nDataset: Cora, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 87.25%\nStarting process for dataset: CiteSeer\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loaded dataset: CiteSeer\nDataset CiteSeer has 3327 nodes and 9104 edges.\nDataset CiteSeer split into train/val/test.\nInitializing SUN for dataset: CiteSeer\nTraining baseline model for dataset: CiteSeer\nDataset: CiteSeer, Epoch 0, Loss: 1.8461\nDataset: CiteSeer, Epoch 10, Loss: 0.7579\nDataset: CiteSeer, Epoch 20, Loss: 0.3771\nDataset: CiteSeer, Epoch 30, Loss: 0.1736\nDataset: CiteSeer, Epoch 40, Loss: 0.0765\nDataset: CiteSeer, Epoch 50, Loss: 0.0374\nDataset: CiteSeer, Epoch 60, Loss: 0.0215\nDataset: CiteSeer, Epoch 70, Loss: 0.0142\nDataset: CiteSeer, Epoch 80, Loss: 0.0105\nDataset: CiteSeer, Epoch 90, Loss: 0.0083\nDataset: CiteSeer, Epoch 100, Loss: 0.0068\nDataset: CiteSeer, Epoch 110, Loss: 0.0058\nDataset: CiteSeer, Epoch 120, Loss: 0.0050\nDataset: CiteSeer, Epoch 130, Loss: 0.0044\nDataset: CiteSeer, Epoch 140, Loss: 0.0039\nDataset: CiteSeer, Epoch 150, Loss: 0.0035\nDataset: CiteSeer, Epoch 160, Loss: 0.0032\nDataset: CiteSeer, Epoch 170, Loss: 0.0029\nDataset: CiteSeer, Epoch 180, Loss: 0.0026\nDataset: CiteSeer, Epoch 190, Loss: 0.0024\nDataset: CiteSeer, Baseline Accuracy: 75.04%\nInitializing Trigger Generator for dataset: CiteSeer\nSelecting poisoned nodes for dataset: CiteSeer\nSelected 30 poisoned nodes.\nStarting attack SBA-Samp on dataset: CiteSeer\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 0, Loss: 0.0042\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 10, Loss: 0.0023\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 20, Loss: 0.0021\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 30, Loss: 0.0019\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 40, Loss: 0.0018\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 50, Loss: 0.0017\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 60, Loss: 0.0016\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 70, Loss: 0.0015\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 80, Loss: 0.0015\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 90, Loss: 0.0014\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 100, Loss: 0.0013\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 110, Loss: 0.0013\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 120, Loss: 0.0012\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 130, Loss: 0.0012\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 140, Loss: 0.0012\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 150, Loss: 0.0011\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 160, Loss: 0.0011\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 170, Loss: 0.0011\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 180, Loss: 0.0011\nDataset: CiteSeer, Attack: SBA-Samp, Epoch 190, Loss: 0.0010\nDataset: CiteSeer, Attack: SBA-Samp - ASR: 0.00%, Clean Accuracy: 74.59%\nStarting attack SBA-Gen on dataset: CiteSeer\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 0, Loss: 0.0010\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 10, Loss: 0.0010\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 20, Loss: 0.0010\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 30, Loss: 0.0010\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 40, Loss: 0.0009\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 50, Loss: 0.0009\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 60, Loss: 0.0009\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 70, Loss: 0.0009\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 80, Loss: 0.0009\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 90, Loss: 0.0009\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 100, Loss: 0.0009\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 110, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 120, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 130, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 140, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 150, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 160, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 170, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 180, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen, Epoch 190, Loss: 0.0008\nDataset: CiteSeer, Attack: SBA-Gen - ASR: 0.00%, Clean Accuracy: 73.98%\nStarting attack GTA on dataset: CiteSeer\nGTA Inner Loop Step 0: Loss = 113.3833\nGTA Inner Loop Step 10: Loss = 0.0178\nGTA Inner Loop Step 20: Loss = 0.0230\nGTA Inner Loop Step 30: Loss = 0.0250\nGTA Inner Loop Step 40: Loss = 0.0257\nDataset: CiteSeer, Attack: GTA, Epoch 0, Loss: 10.9374\nDataset: CiteSeer, Attack: GTA, Epoch 10, Loss: 0.3933\nDataset: CiteSeer, Attack: GTA, Epoch 20, Loss: 0.1287\nDataset: CiteSeer, Attack: GTA, Epoch 30, Loss: 0.0220\nDataset: CiteSeer, Attack: GTA, Epoch 40, Loss: 0.0063\nDataset: CiteSeer, Attack: GTA, Epoch 50, Loss: 0.0037\nDataset: CiteSeer, Attack: GTA, Epoch 60, Loss: 0.0027\nDataset: CiteSeer, Attack: GTA, Epoch 70, Loss: 0.0023\nDataset: CiteSeer, Attack: GTA, Epoch 80, Loss: 0.0020\nDataset: CiteSeer, Attack: GTA, Epoch 90, Loss: 0.0019\nDataset: CiteSeer, Attack: GTA, Epoch 100, Loss: 0.0018\nDataset: CiteSeer, Attack: GTA, Epoch 110, Loss: 0.0017\nDataset: CiteSeer, Attack: GTA, Epoch 120, Loss: 0.0016\nDataset: CiteSeer, Attack: GTA, Epoch 130, Loss: 0.0015\nDataset: CiteSeer, Attack: GTA, Epoch 140, Loss: 0.0015\nDataset: CiteSeer, Attack: GTA, Epoch 150, Loss: 0.0014\nDataset: CiteSeer, Attack: GTA, Epoch 160, Loss: 0.0014\nDataset: CiteSeer, Attack: GTA, Epoch 170, Loss: 0.0013\nDataset: CiteSeer, Attack: GTA, Epoch 180, Loss: 0.0013\nDataset: CiteSeer, Attack: GTA, Epoch 190, Loss: 0.0013\nDataset: CiteSeer, Attack: GTA - ASR: 3.33%, Clean Accuracy: 72.63%\nStarting attack UGBA on dataset: CiteSeer\nDataset: CiteSeer, Attack: UGBA, Epoch 0, Loss: 2.1191\nDataset: CiteSeer, Attack: UGBA, Epoch 10, Loss: 2.8370\nDataset: CiteSeer, Attack: UGBA, Epoch 20, Loss: 1.6911\nDataset: CiteSeer, Attack: UGBA, Epoch 30, Loss: 1.8141\nDataset: CiteSeer, Attack: UGBA, Epoch 40, Loss: 0.6577\nDataset: CiteSeer, Attack: UGBA, Epoch 50, Loss: 0.3914\nDataset: CiteSeer, Attack: UGBA, Epoch 60, Loss: 0.1765\nDataset: CiteSeer, Attack: UGBA, Epoch 70, Loss: 0.0738\nDataset: CiteSeer, Attack: UGBA, Epoch 80, Loss: 0.0430\nDataset: CiteSeer, Attack: UGBA, Epoch 90, Loss: 0.0102\nDataset: CiteSeer, Attack: UGBA, Epoch 100, Loss: 0.0020\nDataset: CiteSeer, Attack: UGBA, Epoch 110, Loss: 0.0012\nDataset: CiteSeer, Attack: UGBA, Epoch 120, Loss: 0.0010\nDataset: CiteSeer, Attack: UGBA, Epoch 130, Loss: 0.0009\nDataset: CiteSeer, Attack: UGBA, Epoch 140, Loss: 0.0009\nDataset: CiteSeer, Attack: UGBA, Epoch 150, Loss: 0.0008\nDataset: CiteSeer, Attack: UGBA, Epoch 160, Loss: 0.0008\nDataset: CiteSeer, Attack: UGBA, Epoch 170, Loss: 0.0008\nDataset: CiteSeer, Attack: UGBA, Epoch 180, Loss: 0.0008\nDataset: CiteSeer, Attack: UGBA, Epoch 190, Loss: 0.0008\nDataset: CiteSeer, Attack: UGBA - ASR: 0.00%, Clean Accuracy: 75.19%\nStarting attack DPGBA on dataset: CiteSeer\nInitializing OOD Detector for DPGBA\nEpoch 0, Reconstruction Loss: 0.0086\nEpoch 10, Reconstruction Loss: 0.0083\nEpoch 20, Reconstruction Loss: 0.0082\nEpoch 30, Reconstruction Loss: 0.0081\nEpoch 40, Reconstruction Loss: 0.0080\nEpoch 50, Reconstruction Loss: 0.0080\nEpoch 60, Reconstruction Loss: 0.0079\nEpoch 70, Reconstruction Loss: 0.0078\nEpoch 80, Reconstruction Loss: 0.0077\nEpoch 90, Reconstruction Loss: 0.0076\nDataset: CiteSeer, Attack: DPGBA, Epoch 0, Loss: 0.0177\nDataset: CiteSeer, Attack: DPGBA, Epoch 10, Loss: 0.0021\nDataset: CiteSeer, Attack: DPGBA, Epoch 20, Loss: 0.0015\nDataset: CiteSeer, Attack: DPGBA, Epoch 30, Loss: 0.0013\nDataset: CiteSeer, Attack: DPGBA, Epoch 40, Loss: 0.0012\nDataset: CiteSeer, Attack: DPGBA, Epoch 50, Loss: 0.0012\nDataset: CiteSeer, Attack: DPGBA, Epoch 60, Loss: 0.0011\nDataset: CiteSeer, Attack: DPGBA, Epoch 70, Loss: 0.0011\nDataset: CiteSeer, Attack: DPGBA, Epoch 80, Loss: 0.0011\nDataset: CiteSeer, Attack: DPGBA, Epoch 90, Loss: 0.0010\nDataset: CiteSeer, Attack: DPGBA, Epoch 100, Loss: 0.0010\nDataset: CiteSeer, Attack: DPGBA, Epoch 110, Loss: 0.0010\nDataset: CiteSeer, Attack: DPGBA, Epoch 120, Loss: 0.0010\nDataset: CiteSeer, Attack: DPGBA, Epoch 130, Loss: 0.0010\nDataset: CiteSeer, Attack: DPGBA, Epoch 140, Loss: 0.0010\nDataset: CiteSeer, Attack: DPGBA, Epoch 150, Loss: 0.0009\nDataset: CiteSeer, Attack: DPGBA, Epoch 160, Loss: 0.0009\nDataset: CiteSeer, Attack: DPGBA, Epoch 170, Loss: 0.0009\nDataset: CiteSeer, Attack: DPGBA, Epoch 180, Loss: 0.0009\nDataset: CiteSeer, Attack: DPGBA, Epoch 190, Loss: 0.0009\nDataset: CiteSeer, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 74.44%\nStarting process for dataset: PubMed\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\nProcessing...\nDone!\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loaded dataset: PubMed\nDataset PubMed has 19717 nodes and 88648 edges.\nDataset PubMed split into train/val/test.\nInitializing SUN for dataset: PubMed\nTraining baseline model for dataset: PubMed\nDataset: PubMed, Epoch 0, Loss: 1.1695\nDataset: PubMed, Epoch 10, Loss: 0.7211\nDataset: PubMed, Epoch 20, Loss: 0.6085\nDataset: PubMed, Epoch 30, Loss: 0.5607\nDataset: PubMed, Epoch 40, Loss: 0.5219\nDataset: PubMed, Epoch 50, Loss: 0.4897\nDataset: PubMed, Epoch 60, Loss: 0.4611\nDataset: PubMed, Epoch 70, Loss: 0.4344\nDataset: PubMed, Epoch 80, Loss: 0.4093\nDataset: PubMed, Epoch 90, Loss: 0.3854\nDataset: PubMed, Epoch 100, Loss: 0.3629\nDataset: PubMed, Epoch 110, Loss: 0.3420\nDataset: PubMed, Epoch 120, Loss: 0.3227\nDataset: PubMed, Epoch 130, Loss: 0.3053\nDataset: PubMed, Epoch 140, Loss: 0.2895\nDataset: PubMed, Epoch 150, Loss: 0.2755\nDataset: PubMed, Epoch 160, Loss: 0.2630\nDataset: PubMed, Epoch 170, Loss: 0.2518\nDataset: PubMed, Epoch 180, Loss: 0.2416\nDataset: PubMed, Epoch 190, Loss: 0.2323\nDataset: PubMed, Baseline Accuracy: 88.18%\nInitializing Trigger Generator for dataset: PubMed\nSelecting poisoned nodes for dataset: PubMed\nSelected 40 poisoned nodes.\nStarting attack SBA-Samp on dataset: PubMed\nDataset: PubMed, Attack: SBA-Samp, Epoch 0, Loss: 0.2247\nDataset: PubMed, Attack: SBA-Samp, Epoch 10, Loss: 0.2171\nDataset: PubMed, Attack: SBA-Samp, Epoch 20, Loss: 0.2096\nDataset: PubMed, Attack: SBA-Samp, Epoch 30, Loss: 0.2031\nDataset: PubMed, Attack: SBA-Samp, Epoch 40, Loss: 0.1972\nDataset: PubMed, Attack: SBA-Samp, Epoch 50, Loss: 0.1915\nDataset: PubMed, Attack: SBA-Samp, Epoch 60, Loss: 0.1862\nDataset: PubMed, Attack: SBA-Samp, Epoch 70, Loss: 0.1811\nDataset: PubMed, Attack: SBA-Samp, Epoch 80, Loss: 0.1763\nDataset: PubMed, Attack: SBA-Samp, Epoch 90, Loss: 0.1718\nDataset: PubMed, Attack: SBA-Samp, Epoch 100, Loss: 0.1673\nDataset: PubMed, Attack: SBA-Samp, Epoch 110, Loss: 0.1630\nDataset: PubMed, Attack: SBA-Samp, Epoch 120, Loss: 0.1592\nDataset: PubMed, Attack: SBA-Samp, Epoch 130, Loss: 0.1552\nDataset: PubMed, Attack: SBA-Samp, Epoch 140, Loss: 0.1514\nDataset: PubMed, Attack: SBA-Samp, Epoch 150, Loss: 0.1479\nDataset: PubMed, Attack: SBA-Samp, Epoch 160, Loss: 0.1443\nDataset: PubMed, Attack: SBA-Samp, Epoch 170, Loss: 0.1409\nDataset: PubMed, Attack: SBA-Samp, Epoch 180, Loss: 0.1379\nDataset: PubMed, Attack: SBA-Samp, Epoch 190, Loss: 0.1345\nDataset: PubMed, Attack: SBA-Samp - ASR: 0.00%, Clean Accuracy: 88.36%\nStarting attack SBA-Gen on dataset: PubMed\nDataset: PubMed, Attack: SBA-Gen, Epoch 0, Loss: 0.1347\nDataset: PubMed, Attack: SBA-Gen, Epoch 10, Loss: 0.1302\nDataset: PubMed, Attack: SBA-Gen, Epoch 20, Loss: 0.1267\nDataset: PubMed, Attack: SBA-Gen, Epoch 30, Loss: 0.1235\nDataset: PubMed, Attack: SBA-Gen, Epoch 40, Loss: 0.1206\nDataset: PubMed, Attack: SBA-Gen, Epoch 50, Loss: 0.1194\nDataset: PubMed, Attack: SBA-Gen, Epoch 60, Loss: 0.1153\nDataset: PubMed, Attack: SBA-Gen, Epoch 70, Loss: 0.1128\nDataset: PubMed, Attack: SBA-Gen, Epoch 80, Loss: 0.1100\nDataset: PubMed, Attack: SBA-Gen, Epoch 90, Loss: 0.1075\nDataset: PubMed, Attack: SBA-Gen, Epoch 100, Loss: 0.1051\nDataset: PubMed, Attack: SBA-Gen, Epoch 110, Loss: 0.1027\nDataset: PubMed, Attack: SBA-Gen, Epoch 120, Loss: 0.1004\nDataset: PubMed, Attack: SBA-Gen, Epoch 130, Loss: 0.0980\nDataset: PubMed, Attack: SBA-Gen, Epoch 140, Loss: 0.0958\nDataset: PubMed, Attack: SBA-Gen, Epoch 150, Loss: 0.0936\nDataset: PubMed, Attack: SBA-Gen, Epoch 160, Loss: 0.0915\nDataset: PubMed, Attack: SBA-Gen, Epoch 170, Loss: 0.0894\nDataset: PubMed, Attack: SBA-Gen, Epoch 180, Loss: 0.0871\nDataset: PubMed, Attack: SBA-Gen, Epoch 190, Loss: 0.0851\nDataset: PubMed, Attack: SBA-Gen - ASR: 0.00%, Clean Accuracy: 88.18%\nStarting attack GTA on dataset: PubMed\nGTA Inner Loop Step 0: Loss = 86.3074\nGTA Inner Loop Step 10: Loss = 0.0059\nGTA Inner Loop Step 20: Loss = 0.0075\nGTA Inner Loop Step 30: Loss = 0.0081\nGTA Inner Loop Step 40: Loss = 0.0083\nDataset: PubMed, Attack: GTA, Epoch 0, Loss: 11.7410\nDataset: PubMed, Attack: GTA, Epoch 10, Loss: 0.8188\nDataset: PubMed, Attack: GTA, Epoch 20, Loss: 0.5859\nDataset: PubMed, Attack: GTA, Epoch 30, Loss: 0.3805\nDataset: PubMed, Attack: GTA, Epoch 40, Loss: 0.2968\nDataset: PubMed, Attack: GTA, Epoch 50, Loss: 0.2502\nDataset: PubMed, Attack: GTA, Epoch 60, Loss: 0.2202\nDataset: PubMed, Attack: GTA, Epoch 70, Loss: 0.2019\nDataset: PubMed, Attack: GTA, Epoch 80, Loss: 0.1880\nDataset: PubMed, Attack: GTA, Epoch 90, Loss: 0.1782\nDataset: PubMed, Attack: GTA, Epoch 100, Loss: 0.1704\nDataset: PubMed, Attack: GTA, Epoch 110, Loss: 0.1639\nDataset: PubMed, Attack: GTA, Epoch 120, Loss: 0.1583\nDataset: PubMed, Attack: GTA, Epoch 130, Loss: 0.1531\nDataset: PubMed, Attack: GTA, Epoch 140, Loss: 0.1485\nDataset: PubMed, Attack: GTA, Epoch 150, Loss: 0.1443\nDataset: PubMed, Attack: GTA, Epoch 160, Loss: 0.1404\nDataset: PubMed, Attack: GTA, Epoch 170, Loss: 0.1368\nDataset: PubMed, Attack: GTA, Epoch 180, Loss: 0.1335\nDataset: PubMed, Attack: GTA, Epoch 190, Loss: 0.1303\nDataset: PubMed, Attack: GTA - ASR: 5.00%, Clean Accuracy: 88.38%\nStarting attack UGBA on dataset: PubMed\nDataset: PubMed, Attack: UGBA, Epoch 0, Loss: 1.3597\nDataset: PubMed, Attack: UGBA, Epoch 10, Loss: 0.6259\nDataset: PubMed, Attack: UGBA, Epoch 20, Loss: 0.2629\nDataset: PubMed, Attack: UGBA, Epoch 30, Loss: 0.1953\nDataset: PubMed, Attack: UGBA, Epoch 40, Loss: 0.1536\nDataset: PubMed, Attack: UGBA, Epoch 50, Loss: 0.1424\nDataset: PubMed, Attack: UGBA, Epoch 60, Loss: 0.1351\nDataset: PubMed, Attack: UGBA, Epoch 70, Loss: 0.1290\nDataset: PubMed, Attack: UGBA, Epoch 80, Loss: 0.1243\nDataset: PubMed, Attack: UGBA, Epoch 90, Loss: 0.1220\nDataset: PubMed, Attack: UGBA, Epoch 100, Loss: 0.1197\nDataset: PubMed, Attack: UGBA, Epoch 110, Loss: 0.1179\nDataset: PubMed, Attack: UGBA, Epoch 120, Loss: 0.1161\nDataset: PubMed, Attack: UGBA, Epoch 130, Loss: 0.1144\nDataset: PubMed, Attack: UGBA, Epoch 140, Loss: 0.1128\nDataset: PubMed, Attack: UGBA, Epoch 150, Loss: 0.1112\nDataset: PubMed, Attack: UGBA, Epoch 160, Loss: 0.1098\nDataset: PubMed, Attack: UGBA, Epoch 170, Loss: 0.1084\nDataset: PubMed, Attack: UGBA, Epoch 180, Loss: 0.1070\nDataset: PubMed, Attack: UGBA, Epoch 190, Loss: 0.1057\nDataset: PubMed, Attack: UGBA - ASR: 2.50%, Clean Accuracy: 88.66%\nStarting attack DPGBA on dataset: PubMed\nInitializing OOD Detector for DPGBA\nEpoch 0, Reconstruction Loss: 0.0003\nEpoch 10, Reconstruction Loss: 0.0003\nEpoch 20, Reconstruction Loss: 0.0003\nEpoch 30, Reconstruction Loss: 0.0003\nEpoch 40, Reconstruction Loss: 0.0003\nEpoch 50, Reconstruction Loss: 0.0003\nEpoch 60, Reconstruction Loss: 0.0003\nEpoch 70, Reconstruction Loss: 0.0003\nEpoch 80, Reconstruction Loss: 0.0003\nEpoch 90, Reconstruction Loss: 0.0003\nDataset: PubMed, Attack: DPGBA, Epoch 0, Loss: 0.1157\nDataset: PubMed, Attack: DPGBA, Epoch 10, Loss: 0.1068\nDataset: PubMed, Attack: DPGBA, Epoch 20, Loss: 0.1032\nDataset: PubMed, Attack: DPGBA, Epoch 30, Loss: 0.1010\nDataset: PubMed, Attack: DPGBA, Epoch 40, Loss: 0.0991\nDataset: PubMed, Attack: DPGBA, Epoch 50, Loss: 0.0976\nDataset: PubMed, Attack: DPGBA, Epoch 60, Loss: 0.0962\nDataset: PubMed, Attack: DPGBA, Epoch 70, Loss: 0.0949\nDataset: PubMed, Attack: DPGBA, Epoch 80, Loss: 0.0936\nDataset: PubMed, Attack: DPGBA, Epoch 90, Loss: 0.0924\nDataset: PubMed, Attack: DPGBA, Epoch 100, Loss: 0.0913\nDataset: PubMed, Attack: DPGBA, Epoch 110, Loss: 0.0902\nDataset: PubMed, Attack: DPGBA, Epoch 120, Loss: 0.0891\nDataset: PubMed, Attack: DPGBA, Epoch 130, Loss: 0.0880\nDataset: PubMed, Attack: DPGBA, Epoch 140, Loss: 0.0870\nDataset: PubMed, Attack: DPGBA, Epoch 150, Loss: 0.0860\nDataset: PubMed, Attack: DPGBA, Epoch 160, Loss: 0.0850\nDataset: PubMed, Attack: DPGBA, Epoch 170, Loss: 0.0841\nDataset: PubMed, Attack: DPGBA, Epoch 180, Loss: 0.0832\nDataset: PubMed, Attack: DPGBA, Epoch 190, Loss: 0.0823\nDataset: PubMed, Attack: DPGBA - ASR: 0.00%, Clean Accuracy: 89.04%\n\nSummary of Results:\n     Dataset Model    Attack Defense       ASR  Clean Accuracy\n0       Cora   SUN      None    None       N/A       87.615527\n1       Cora   SUN  SBA-Samp    None       0.0       87.615527\n2       Cora   SUN   SBA-Gen    None       0.0       87.430684\n3       Cora   SUN       GTA    None      20.0       86.876155\n4       Cora   SUN      UGBA    None      20.0       87.245841\n5       Cora   SUN     DPGBA    None       0.0       87.245841\n6   CiteSeer   SUN      None    None       N/A       75.037594\n7   CiteSeer   SUN  SBA-Samp    None       0.0       74.586466\n8   CiteSeer   SUN   SBA-Gen    None       0.0       73.984962\n9   CiteSeer   SUN       GTA    None  3.333333       72.631579\n10  CiteSeer   SUN      UGBA    None       0.0       75.187970\n11  CiteSeer   SUN     DPGBA    None       0.0       74.436090\n12    PubMed   SUN      None    None       N/A       88.181588\n13    PubMed   SUN  SBA-Samp    None       0.0       88.359117\n14    PubMed   SUN   SBA-Gen    None       0.0       88.181588\n15    PubMed   SUN       GTA    None       5.0       88.384479\n16    PubMed   SUN      UGBA    None       2.5       88.663454\n17    PubMed   SUN     DPGBA    None       0.0       89.043875\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}